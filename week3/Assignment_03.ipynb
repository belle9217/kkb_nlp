{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QbeC34Ocjivy"
   },
   "source": [
    "## Assignment-03 First Step of Machine Learning: Model and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ZQJ5isKjiv4"
   },
   "source": [
    "同学们，今天我们的学习了基本的机器学习概念，相比你已经对机器学习的这些方法有一个基本的认识了。值得说明的是，机器学习不仅仅是一系列方法，更重要的是一种思维体系，即：依据以往的、现有的数据，构建某种方法来解决未见过的问题。而且决策树，贝叶斯只是实现这个目标的一个方法，包括之后的神经网络。很有可能有一天，神经网络也会被淘汰，但是重要的是我们要理解机器学习的目标，就是尽可能的自动化解决未知的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yc2QgEx0jiv6"
   },
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1571556399207&di=4a97dc15ad08dd49d3748d1edf6109b3&imgtype=0&src=http%3A%2F%2Fc.hiphotos.baidu.com%2Fzhidao%2Fwh%3D450%2C600%2Fsign%3Dae742c6aedcd7b89e93932873a146e91%2F5d6034a85edf8db1b16050c40223dd54574e74c7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9l7uyRBujiv8"
   },
   "source": [
    "## Part-1 Programming Review 编程回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8mXil2bwjiv9"
   },
   "source": [
    "#### 1. Re-code the Linear-Regression Model using scikit-learning(10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s00FXCDtjiv_"
   },
   "source": [
    "<评阅点>： \n",
    "> + 是否完成线性回归模型 (4')\n",
    "+ 能够进行预测新数据(3')\n",
    "+ 能够进行可视化操作(3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "JPrLpJXljiwB",
    "outputId": "ba7a3390-b8cb-4b3b-bda4-785415fd8b42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = np.random.random(100)\n",
    "y = 13.14 * x + 5 + np.random.randint(-5,5,100)\n",
    "x_train, y_train = x.reshape(-1,1), y.reshape(-1,1)\n",
    "reg = LinearRegression()\n",
    "reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "44rBo8mWqWpX",
    "outputId": "d709cc95-b657-4174-fb95-f107d4be17bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6191397309308876\n"
     ]
    }
   ],
   "source": [
    "#模型的预测准确度得分\n",
    "print(reg.score(x_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "uJlkLmQ-qaoz",
    "outputId": "3e7857ea-84f1-4ac4-b123-622e89957fd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcfceffb438>]"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvYhIxf1/koTpHvlRZuUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#可视化操作\n",
    "y_pred = reg.predict(x_train)\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.plot(x_train, y_pred, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "N1l1L9PWq1qD",
    "outputId": "89f720f0-02eb-43aa-d8da-b4474a43d016"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21.66382293]])"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#预测新的数据点\n",
    "reg.predict([[1.3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aMQiMB1cjiwK"
   },
   "source": [
    "<评阅点>:\n",
    "> + 是否完成了KNN模型 (4')\n",
    "+ 是否能够预测新的数据 (4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EDQnorZcjiwI"
   },
   "source": [
    "#### 2. Complete the unfinished KNN Model using pure python to solve the previous Line-Regression problem. (8 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ydaj09gmjiwL"
   },
   "outputs": [],
   "source": [
    "def knn(x, y):\n",
    "  return [(xi, yi) for xi, yi in zip(x,y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KfMDe5xRsIRk"
   },
   "outputs": [],
   "source": [
    "def distance(x1, x2):\n",
    "  return abs(x1-x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TP1ziGdzsBcl"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def predict(x, k=5):\n",
    "  most_similars = sorted(knn(x_train, y_train), key=lambda xi:distance(x,xi[0]))[:k]\n",
    "  y_hat = [y_ for x_, y_ in most_similars]\n",
    "  return np.mean(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 917
    },
    "colab_type": "code",
    "id": "MAc5PucF_P_j",
    "outputId": "4f33e05c-9bb1-41e8-cc38-d080c8fa1288"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Predict 0.42 using LinearRegression:10.12\n",
      "Predict 0.42 using KNN:10.16\n",
      "\n",
      "\n",
      "1\n",
      "Predict 0.89 using LinearRegression:16.29\n",
      "Predict 0.89 using KNN:15.75\n",
      "\n",
      "\n",
      "2\n",
      "Predict 0.54 using LinearRegression:11.70\n",
      "Predict 0.54 using KNN:12.32\n",
      "\n",
      "\n",
      "3\n",
      "Predict 0.88 using LinearRegression:16.19\n",
      "Predict 0.88 using KNN:17.95\n",
      "\n",
      "\n",
      "4\n",
      "Predict 0.23 using LinearRegression:7.70\n",
      "Predict 0.23 using KNN:7.93\n",
      "\n",
      "\n",
      "5\n",
      "Predict 0.93 using LinearRegression:16.79\n",
      "Predict 0.93 using KNN:17.34\n",
      "\n",
      "\n",
      "6\n",
      "Predict 0.45 using LinearRegression:10.47\n",
      "Predict 0.45 using KNN:10.85\n",
      "\n",
      "\n",
      "7\n",
      "Predict 0.89 using LinearRegression:16.29\n",
      "Predict 0.89 using KNN:15.75\n",
      "\n",
      "\n",
      "8\n",
      "Predict 0.33 using LinearRegression:9.00\n",
      "Predict 0.33 using KNN:8.49\n",
      "\n",
      "\n",
      "9\n",
      "Predict 0.39 using LinearRegression:9.70\n",
      "Predict 0.39 using KNN:12.01\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#使用LinearRegression/knn预测新数据\n",
    "for i in range(10):\n",
    "  x_test = np.random.random()\n",
    "  print(i)\n",
    "  print(\"Predict {:.2f} using LinearRegression:{:.2f}\".format(x_test, reg.predict([[x_test]])[0][0]))\n",
    "  print(\"Predict {:.2f} using KNN:{:.2f}\".format(x_test, predict(x_test)))\n",
    "  print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kYH9vINGjiwS"
   },
   "source": [
    "\n",
    "#### 3. Re-code the Decision Tree, which could sort the features by salience. (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-OJeMR-LjiwX"
   },
   "source": [
    "<评阅点>\n",
    "> + 是否实现了信息熵 (1' )\n",
    "+ 是否实现了最优先特征点的选择(5')\n",
    "+ 是否实现了持续的特征选则(6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QXn56RtnjiwZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "mock_data = {\n",
    "    'gender':['F', 'F', 'F', 'F', 'M', 'M', 'M'],\n",
    "    'income': ['+10', '-10', '+10', '+10', '+10', '+10', '-10'],\n",
    "    'family_number': [1, 1, 2, 1, 1, 1, 2],\n",
    "    #'pet': [1, 1, 1, 0, 0, 0, 1],\n",
    "    'bought': [1, 1, 1, 0, 0, 0, 1],\n",
    "}\n",
    "data = pd.DataFrame(mock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A1rD8qfqCFhQ"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def entropy(elements):\n",
    "  counter = Counter(elements)\n",
    "  probs = []\n",
    "  for e in set(elements):\n",
    "    probs.append(counter[e] / len(elements))\n",
    "  return -sum(p*np.log(p) for p in probs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "OH-1LFg9Cu5j",
    "outputId": "f21a89cd-6b66-451a-e56a-4cf25de3d3d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0397207708399179"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy([1,2,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "colab_type": "code",
    "id": "T0LQk2lRCxx4",
    "outputId": "646729bd-430d-4682-df0f-247f9dd9544e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M</td>\n",
       "      <td>-10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "0      F    +10              1       1\n",
       "1      F    -10              1       1\n",
       "2      F    +10              2       1\n",
       "3      F    +10              1       0\n",
       "4      M    +10              1       0\n",
       "5      M    +10              1       0\n",
       "6      M    -10              2       1"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "23y9WBfgD5be"
   },
   "outputs": [],
   "source": [
    "def optimal_split(dataset, target):\n",
    "  x_fields = set(dataset.columns.tolist()) - {target}\n",
    "  spliter = None\n",
    "  min_entropy = float('inf')\n",
    "\n",
    "  for f in x_fields:\n",
    "    values = set(dataset[f])\n",
    "    if len(values) == 1:\n",
    "      continue\n",
    "    for v in values:\n",
    "      sub_spliter1 = dataset[dataset[f] == v][target].tolist()\n",
    "      sub_spliter2 = dataset[dataset[f] != v][target].tolist()\n",
    "      entropy_v = entropy(sub_spliter1) + entropy(sub_spliter2)\n",
    "      if entropy_v < min_entropy:\n",
    "        min_entropy = entropy_v\n",
    "        spliter = (f,v)\n",
    "  # print(\"spliter:{0}, entropy:{1:.2f}\".format(spliter, min_entropy))\n",
    "  return min_entropy, spliter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "DdP1pYAJMdkN",
    "outputId": "30e69011-3ac4-4d1c-bb57-011237fc2403"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.0, ('gender', 'M'))"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_split(data[data['income']=='-10'], 'bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KRbd80hyNusQ"
   },
   "outputs": [],
   "source": [
    "def split_all(dataset,target):\n",
    "  datas = [dataset]\n",
    "  spliters = []\n",
    "  while datas:\n",
    "    d = datas.pop(0)\n",
    "    min_entropy, spliter = optimal_split(d, target)\n",
    "    if min_entropy < 0.001 or min_entropy is float('inf'):\n",
    "      continue\n",
    "    if spliter is None:\n",
    "      continue\n",
    "    f, v = spliter\n",
    "    spliters.append(spliter)\n",
    "    dataset1 = d[d[f] == v]\n",
    "    dataset2 = d[d[f] != v]\n",
    "    print(d, '\\n', 'Spliter:',spliter)\n",
    "    if not dataset1.empty:\n",
    "\n",
    "      datas.append(dataset1)\n",
    "    if not dataset2.empty:\n",
    "\n",
    "      datas.append(dataset2)\n",
    "  return spliters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "colab_type": "code",
    "id": "XmOiv3uYPnPH",
    "outputId": "a4f08560-8e32-4e87-b481-4d8f648c513f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  gender income  family_number  bought\n",
      "0      F    +10              1       1\n",
      "1      F    -10              1       1\n",
      "2      F    +10              2       1\n",
      "3      F    +10              1       0\n",
      "4      M    +10              1       0\n",
      "5      M    +10              1       0\n",
      "6      M    -10              2       1 \n",
      " Spliter: ('income', '-10')\n",
      "  gender income  family_number  bought\n",
      "0      F    +10              1       1\n",
      "2      F    +10              2       1\n",
      "3      F    +10              1       0\n",
      "4      M    +10              1       0\n",
      "5      M    +10              1       0 \n",
      " Spliter: ('family_number', 1)\n",
      "  gender income  family_number  bought\n",
      "0      F    +10              1       1\n",
      "3      F    +10              1       0\n",
      "4      M    +10              1       0\n",
      "5      M    +10              1       0 \n",
      " Spliter: ('gender', 'M')\n"
     ]
    }
   ],
   "source": [
    "spliters = split_all(data, 'bought')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WBxPItymjiwe"
   },
   "source": [
    "#### 4. Finish the K-Means using 2-D matplotlib (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4E10D8mXjiwi"
   },
   "source": [
    "<评阅点>\n",
    "> + 是否完成了KMeans模型，基于scikit-learning (3')\n",
    "+ 是否完成了可视化任务（5'）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x2QiXLwha5ga"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "5cA9NyS0a9et",
    "outputId": "9460f520-27a6-4e32-98ad-dbe1c224c4e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fcfca3e1208>"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "x1 = [random.randint(0,100) for _ in range(100)]\n",
    "x2 = [random.randint(0,100) for _ in range(100)]\n",
    "plt.scatter(x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "UIm2mJMWbpzY",
    "outputId": "192edcc0-3de8-4fc2-d479-d18bcf4106c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=500,\n",
       "       n_clusters=5, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = list(zip(x1,x2))\n",
    "cluster = KMeans(n_clusters=5, max_iter=500)\n",
    "cluster.fit(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "9F8-OpFscIiT",
    "outputId": "0276ffdf-3413-45bd-eb4b-f93d041c81f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18.52380952, 41.66666667],\n",
       "       [79.69230769, 31.30769231],\n",
       "       [77.3       , 83.35      ],\n",
       "       [37.41176471, 16.05882353],\n",
       "       [31.5625    , 77.75      ]])"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "Cgf4ONVQcN6p",
    "outputId": "c6656f3b-39b7-4b57-9323-c10bfbfa9424"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 3, 1, 4, 1, 4, 1, 4, 0, 0, 3, 1, 0, 1, 2, 3, 2, 2, 1, 4, 1,\n",
       "       4, 2, 1, 4, 3, 4, 1, 0, 0, 1, 3, 0, 4, 0, 3, 2, 3, 1, 2, 1, 4, 0,\n",
       "       4, 3, 4, 4, 0, 0, 2, 0, 0, 2, 0, 3, 3, 1, 2, 3, 0, 3, 0, 3, 3, 0,\n",
       "       1, 0, 1, 0, 2, 4, 4, 2, 1, 1, 3, 2, 1, 2, 2, 0, 0, 3, 1, 1, 2, 1,\n",
       "       2, 2, 2, 1, 1, 4, 2, 1, 3, 0, 4, 1], dtype=int32)"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "FB91AigBcUgW",
    "outputId": "e4473665-fb3e-4b85-e7ef-ed2234709f63"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = ['red','green','black','blue','purple']\n",
    "for i, (x, y) in enumerate(data_train):\n",
    "  plt.scatter(x, y, c=colors[cluster.labels_[i]])\n",
    "\n",
    "for (x, y) in cluster.cluster_centers_:\n",
    "  plt.scatter(x, y, s=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BiIrj4bHjiwj"
   },
   "source": [
    "## Part-2 Question and Answer 问答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "naL-lq1qjiwl"
   },
   "source": [
    "#### 1. What's the *model*? why  all the models are wrong, but some are useful? (5 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PwVvHb0jjiwm"
   },
   "source": [
    "Ans:因为所有的模型都是对真实数据值的预测，无法完全反应真实世界的数据情况。但是，我们可以不断对模型进行优化，使得模型的预测不断接近真实世界的数据。在一定条件下，这些模型是有用的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b5eQRBTqjiwn"
   },
   "source": [
    "<评阅点>\n",
    "> + 对模型的理解是否正确,对模型的抽象性是否正确(5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e3FKCk6wjiwp"
   },
   "source": [
    "#### 2. What's the underfitting and overfitting? List the reasons that could make model overfitting or underfitting. (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Atljn1zDjiwq"
   },
   "source": [
    "Ans:欠拟合，是指模型无法准确预测训练集和测试集的数据。原因包括，训练集的数据量偏少，模型过于简单等。\n",
    "过拟合，是指模型可以准确预测训练集的数据，但无法准确预测测试集的数据。原因包括：模型过于复杂，使得模型学习了训练集的所有特征（包括异常值的特征和数据集的误差）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XUKzDSCLjiwt"
   },
   "source": [
    "<评阅点>\n",
    "> + 对过拟合和欠拟合的理解是否正确 (3')\n",
    "+ 对欠拟合产生的原因是否理解正确(2')\n",
    "+ 对过拟合产生的原因是否理解正确(5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jglX7zBUjiw1"
   },
   "source": [
    "#### 3. What's the precision, recall, AUC, F1, F2score. What are they mainly target on? (12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MOACmEXkjiw3"
   },
   "source": [
    "Ans:\n",
    "\n",
    "$ precision = \\frac{TP}{TP + FP} $\n",
    "\n",
    "$ recall = \\frac{TP}{TP + FN} $\n",
    "\n",
    "$ f \\space score = (1 + \\beta ^2）*\\frac{precision * recall}{\\beta * precision + recall} $\n",
    "\n",
    "$ f1 \\space score = 2*\\frac{precision * recall}{precision + recall} $\n",
    "\n",
    "$ f2 \\space score = 5*\\frac{precision * recall}{4 * precision + recall} $\n",
    "\n",
    "AUC，即Area Under Curve，是指受试者工作曲线（ROC）下的面积。\n",
    "\n",
    "精确率和召回率都是评判模型预测不平衡数据的指标。其中，用召回率评判是需要把数据中的正样本都尽可能准确预测出来；用精确率评判是预测为正样本的数据中，实际为正样本的数目要尽可能多。\n",
    "\n",
    "F-Score同时考虑了精确率和召回率的影响。F1-Score中，精确率和召回率的影响因素一样；F2-Score中，精确率的权重更大。\n",
    "\n",
    "AUC是为了解决在分类问题中，阈值设置对预测结果的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hc4sKqlsjiw5"
   },
   "source": [
    "<评阅点>\n",
    "> + 对precision, recall, AUC, F1, F2 理解是否正确(6‘)\n",
    "+ 对precision, recall, AUC, F1, F2的使用侧重点是否理解正确 (6’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A7EkViRxjiw6"
   },
   "source": [
    "#### 4. Based on our course and yourself mind, what's the machine learning?  (8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GAt4Sk8Mjiw-"
   },
   "source": [
    "Ans:机器学习是基于已知数据和模型，对未知的数据进行预测。其中，数据的多少、好坏直接影响了模型预测的准确度。传统的编程，是基于设置好的规则进行预测，不依赖于数据，也无法根据数据对预测结果进行修正。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VuSN6vZtjiw-"
   },
   "source": [
    "<评阅点> 开放式问题，是否能说出来机器学习这种思维方式和传统的分析式编程的区别（8'）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j39TppyZjiw-"
   },
   "source": [
    "#### 5. \"正确定义了机器学习模型的评价标准(evaluation)， 问题基本上就已经解决一半\". 这句话是否正确？你是怎么看待的？ (8‘)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7bjanPFEoYH0"
   },
   "source": [
    "Ans:这句话是正确的。因为在机器学习中，不同的任务对评价指标的要求有着很大的不同。而能否选择正确的评价指标，对问题的研究和模型的评估至关重要。例如，在不平衡数据的分类问题中中，使用准确率（accuracy）就无法正确衡量模型的好坏。此时，使用precision或者recall才能对模型进行评估。而在医学检测肿瘤的问题中，为了使得所有病人都尽可能被检测出来，对于recall的要求就很高，而不会过多关注precision。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GhqvHX21jiw-"
   },
   "source": [
    "<评阅点> 开放式问题，主要看能理解评价指标对机器学习模型的重要性."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tZyoJMsFjiw-"
   },
   "source": [
    "## Part-03 Programming Practice 编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qRHHWsrTjiw-"
   },
   "source": [
    "1. In our course and previous practice, we complete some importance components of Decision Tree. In this problem, you need to build a **completed** Decision Tree Model. You show finish a `predicate()` function, which accepts three parameters **<gender, income, family_number>**, and outputs the predicated 'bought': 1 or 0.  (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "colab_type": "code",
    "id": "JxyntGK7jiw-",
    "outputId": "4c10ba82-781f-4a60-e096-e7f66b6ac21a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('income', '-10'), ('family_number', 1), ('gender', 'M')]\n",
      "  gender income  family_number  bought\n",
      "0      F    +10              1       1\n",
      "1      F    -10              1       1\n",
      "2      F    +10              2       1\n",
      "3      F    +10              1       0\n",
      "4      M    +10              1       0\n",
      "5      M    +10              1       0\n",
      "6      M    -10              2       1\n"
     ]
    }
   ],
   "source": [
    "print(spliters)\n",
    "print(pd.DataFrame(mock_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4zBv9sHHSEUK"
   },
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "  def __init__(self, feature = None, value = None, left=None, right=None, result=None):\n",
    "    self.feature = feature\n",
    "    self.value = value\n",
    "    self.true = left\n",
    "    self.false = right\n",
    "    self.result = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fGWrDch_TEZU"
   },
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (<ipython-input-1-69f395edfb8e>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-69f395edfb8e>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    min_entropy, spliter = optimal_split(dataset, target)\u001b[0m\n\u001b[0m                                                         ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "#build dicision tree based on optimal split\n",
    "from collections import Counter\n",
    "def build_tree(dataset,target):\n",
    "    tree = DecisionTree(value = dataset)\n",
    "    # features = set(dataset.columns.tolist()) - {target}\n",
    "\tmin_entropy, spliter = optimal_split(dataset, target)\n",
    "\tif min_entropy <= 1e-3 or spliter is None :\n",
    "\t\ttree.result = Counter(tree.value[target]).most_common(1)[0][0]\n",
    "\t\treturn tree\n",
    "\tf, v = spliter\n",
    "\ttree.feature = spliter\n",
    "\tdataset1 = dataset[dataset[f] == v]\n",
    "\tdataset2 = dataset[dataset[f] != v]\n",
    "\ttree.true = build_tree(dataset1, target)\n",
    "\ttree.false = build_tree(dataset2, target)\n",
    "\treturn tree\n",
    "\n",
    "#predict function based on decision tree\n",
    "def predict(x_test, tree):\n",
    "\tif tree.result != None:\n",
    "\t\treturn tree.result\n",
    "\telse:\n",
    "\t\tv = x_test[tree.feature[0]]\n",
    "\t\tbranch = None\n",
    "\t\tif tree.feature[1] == v:\n",
    "\t\t\tbranch = tree.true\n",
    "\t\telse: \n",
    "\t\t\tbranch = tree.false\n",
    "\treturn predict(x_test, branch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "colab_type": "code",
    "id": "uOCFGxo55c2t",
    "outputId": "12904738-8c61-4a9d-cb54-3c0e5fbb64cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  gender income  family_number  bought\n",
      "0      F    +10              1       1\n",
      "1      F    -10              1       1\n",
      "2      F    +10              2       1\n",
      "3      F    +10              1       0\n",
      "4      M    +10              1       0\n",
      "5      M    +10              1       0\n",
      "6      M    -10              2       1 \n",
      " ('income', '-10') None\n",
      "  gender income  family_number  bought\n",
      "1      F    -10              1       1\n",
      "6      M    -10              2       1 \n",
      " None 1\n",
      "  gender income  family_number  bought\n",
      "0      F    +10              1       1\n",
      "2      F    +10              2       1\n",
      "3      F    +10              1       0\n",
      "4      M    +10              1       0\n",
      "5      M    +10              1       0 \n",
      " ('family_number', 1) None\n",
      "  gender income  family_number  bought\n",
      "0      F    +10              1       1\n",
      "3      F    +10              1       0\n",
      "4      M    +10              1       0\n",
      "5      M    +10              1       0 \n",
      " ('gender', 'M') None\n",
      "  gender income  family_number  bought\n",
      "2      F    +10              2       1 \n",
      " None 1\n",
      "  gender income  family_number  bought\n",
      "4      M    +10              1       0\n",
      "5      M    +10              1       0 \n",
      " None 0\n",
      "  gender income  family_number  bought\n",
      "0      F    +10              1       1\n",
      "3      F    +10              1       0 \n",
      " None 1\n"
     ]
    }
   ],
   "source": [
    "#print training dacision tree\n",
    "stack = [tree]\n",
    "while stack:\n",
    "  t = stack.pop(0)\n",
    "  print(t.value, '\\n', t.feature, t.result)\n",
    "  if t.true:\n",
    "    stack.append(t.true)\n",
    "  if t.false:\n",
    "    stack.append(t.false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "colab_type": "code",
    "id": "qi7L72WnpH51",
    "outputId": "475b6519-b756-4075-cf5d-fc7a790b7f55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict training data:\n",
      "Gender:F, income:+10, family number:1, bought:1, bought_predict:1\n",
      "Gender:F, income:-10, family number:1, bought:1, bought_predict:1\n",
      "Gender:F, income:+10, family number:2, bought:1, bought_predict:1\n",
      "Gender:F, income:+10, family number:1, bought:0, bought_predict:1\n",
      "Gender:M, income:+10, family number:1, bought:0, bought_predict:0\n",
      "Gender:M, income:+10, family number:1, bought:0, bought_predict:0\n",
      "Gender:M, income:-10, family number:2, bought:1, bought_predict:1\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "dataset = pd.DataFrame(mock_data)\n",
    "tree = build_tree(dataset,'bought')\n",
    "\n",
    "print(\"Predict training data:\")\n",
    "for i in range(len(dataset)):\n",
    "  data = dataset.iloc[i,:]\n",
    "  test_data = {'gender':data.gender, 'income':data.income, 'family_number':data.family_number}\n",
    "  pred = predict(test_data,tree)\n",
    "  print(\"Gender:{}, income:{}, family number:{}, bought:{}, bought_predict:{}\".format(data.gender, data.income, data.family_number, data.bought, pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "Ro0oBMT4ebQT",
    "outputId": "6589be10-9ce5-4d96-867e-d7c354b5f09d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict testing data:\n",
      "Gender:F, income:-10, family number:2, bought_predict:1\n",
      "Gender:F, income:+10, family number:1, bought_predict:1\n",
      "Gender:M, income:+10, family number:1, bought_predict:0\n",
      "Gender:F, income:+10, family number:2, bought_predict:1\n",
      "Gender:F, income:-10, family number:1, bought_predict:1\n",
      "Gender:F, income:-10, family number:1, bought_predict:1\n",
      "Gender:F, income:+10, family number:1, bought_predict:1\n",
      "Gender:F, income:+10, family number:2, bought_predict:1\n",
      "Gender:F, income:+10, family number:2, bought_predict:1\n",
      "Gender:M, income:+10, family number:1, bought_predict:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Predict testing data:\")\n",
    "for i in range(10):\n",
    "  gender = random.choice(['M','F'])\n",
    "  income = random.choice(['+10','-10'])\n",
    "  family_number = random.choice((1,2))\n",
    "  test_data = {'gender':gender, 'income':income, 'family_number':family_number}\n",
    "  pred = predict(test_data,tree)\n",
    "  print(\"Gender:{}, income:{}, family number:{}, bought_predict:{}\".format(gender, income, family_number, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aj7Vgas2jiw-"
   },
   "source": [
    "<评阅点>\n",
    "> + 是否将之前的决策树模型的部分进行合并组装， predicate函数能够顺利运行(8')\n",
    "+ 是够能够输入未曾见过的X变量，例如gender, income, family_number 分别是： <M, -10, 1>, 模型能够预测出结果 (12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9yuaPwX5jixN"
   },
   "source": [
    "2. 将上一节课(第二节课)的线性回归问题中的Loss函数改成\"绝对值\"，并且改变其偏导的求值方式，观察其结果的变化。(19 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VgEkhCcKjixN"
   },
   "outputs": [],
   "source": [
    "def loss(y,y_hat):\n",
    "  l = 0\n",
    "  for y_i, y_hat_i in zip(list(y),list(y_hat)):\n",
    "    l += abs(y_i - y_hat_i)\n",
    "  return l / len(list(y))\n",
    "\n",
    "def price(rm, k, b):\n",
    "  return k * rm + b\n",
    "\n",
    "def derivative_k(x, y, y_hat):\n",
    "  grad = 0\n",
    "  for x_i, y_i, y_hat_i in zip(list(x),list(y),list(y_hat)):\n",
    "    if y_i > y_hat_i:\n",
    "      grad += -x_i\n",
    "    else:\n",
    "      grad += x_i\n",
    "  return grad / len(list(y))\n",
    "\n",
    "def derivative_b(x, y, y_hat):\n",
    "  grad = 0\n",
    "  for x_i, y_i, y_hat_i in zip(list(x),list(y),list(y_hat)):\n",
    "    if y_i > y_hat_i:\n",
    "      grad += -1\n",
    "    else:\n",
    "      grad += 1\n",
    "    return grad / len(list(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zY64dlpLmTkK"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "dataset = load_boston()\n",
    "x, y = dataset['data'], dataset['target']\n",
    "x_rm = x[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZhYEta6Gmp9b",
    "outputId": "818da74a-be22-41bf-e7d6-244037ea4585"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, the loss is 370.81502576941864, parameters k is 73.72707229337652 and b is -69.99986172019558\n",
      "Iteration 1, the loss is 370.4200397127456, parameters k is 73.664225949503 and b is -69.99988148304143\n",
      "Iteration 2, the loss is 370.0250536560727, parameters k is 73.60137960562949 and b is -69.99990124588729\n",
      "Iteration 3, the loss is 369.63006759940043, parameters k is 73.53853326175597 and b is -69.99992100873314\n",
      "Iteration 4, the loss is 369.23508154272764, parameters k is 73.47568691788246 and b is -69.99994077157899\n",
      "Iteration 5, the loss is 368.8400954860552, parameters k is 73.41284057400894 and b is -69.99996053442484\n",
      "Iteration 6, the loss is 368.4451094293824, parameters k is 73.34999423013542 and b is -69.9999802972707\n",
      "Iteration 7, the loss is 368.05012337270955, parameters k is 73.2871478862619 and b is -70.00000006011655\n",
      "Iteration 8, the loss is 367.6551373160368, parameters k is 73.22430154238839 and b is -70.0000198229624\n",
      "Iteration 9, the loss is 367.26015125936436, parameters k is 73.16145519851487 and b is -70.00003958580825\n",
      "Iteration 10, the loss is 366.8651652026912, parameters k is 73.09860885464136 and b is -70.0000593486541\n",
      "Iteration 11, the loss is 366.47017914601884, parameters k is 73.03576251076784 and b is -70.00007911149996\n",
      "Iteration 12, the loss is 366.0751930893461, parameters k is 72.97291616689432 and b is -70.00009887434581\n",
      "Iteration 13, the loss is 365.6802070326737, parameters k is 72.9100698230208 and b is -70.00011863719166\n",
      "Iteration 14, the loss is 365.2852209760007, parameters k is 72.84722347914729 and b is -70.00013840003751\n",
      "Iteration 15, the loss is 364.8902349193282, parameters k is 72.78437713527377 and b is -70.00015816288337\n",
      "Iteration 16, the loss is 364.49524886265505, parameters k is 72.72153079140026 and b is -70.00017792572922\n",
      "Iteration 17, the loss is 364.1002628059823, parameters k is 72.65868444752674 and b is -70.00019768857507\n",
      "Iteration 18, the loss is 363.7052767493097, parameters k is 72.59583810365322 and b is -70.00021745142092\n",
      "Iteration 19, the loss is 363.3102906926368, parameters k is 72.5329917597797 and b is -70.00023721426678\n",
      "Iteration 20, the loss is 362.91530463596473, parameters k is 72.47014541590619 and b is -70.00025697711263\n",
      "Iteration 21, the loss is 362.52031857929165, parameters k is 72.40729907203267 and b is -70.00027673995848\n",
      "Iteration 22, the loss is 362.1253325226191, parameters k is 72.34445272815915 and b is -70.00029650280433\n",
      "Iteration 23, the loss is 361.7303464659464, parameters k is 72.28160638428564 and b is -70.00031626565018\n",
      "Iteration 24, the loss is 361.335360409274, parameters k is 72.21876004041212 and b is -70.00033602849604\n",
      "Iteration 25, the loss is 360.9403743526011, parameters k is 72.1559136965386 and b is -70.00035579134189\n",
      "Iteration 26, the loss is 360.54538829592906, parameters k is 72.09306735266509 and b is -70.00037555418774\n",
      "Iteration 27, the loss is 360.1504022392558, parameters k is 72.03022100879157 and b is -70.0003953170336\n",
      "Iteration 28, the loss is 359.75541618258313, parameters k is 71.96737466491805 and b is -70.00041507987945\n",
      "Iteration 29, the loss is 359.3604301259103, parameters k is 71.90452832104454 and b is -70.0004348427253\n",
      "Iteration 30, the loss is 358.96544406923783, parameters k is 71.84168197717102 and b is -70.00045460557115\n",
      "Iteration 31, the loss is 358.5704580125648, parameters k is 71.7788356332975 and b is -70.000474368417\n",
      "Iteration 32, the loss is 358.17547195589196, parameters k is 71.71598928942399 and b is -70.00049413126285\n",
      "Iteration 33, the loss is 357.78048589921985, parameters k is 71.65314294555047 and b is -70.0005138941087\n",
      "Iteration 34, the loss is 357.3854998425468, parameters k is 71.59029660167695 and b is -70.00053365695456\n",
      "Iteration 35, the loss is 356.9905137858742, parameters k is 71.52745025780344 and b is -70.00055341980041\n",
      "Iteration 36, the loss is 356.5955277292015, parameters k is 71.46460391392992 and b is -70.00057318264626\n",
      "Iteration 37, the loss is 356.20054167252914, parameters k is 71.4017575700564 and b is -70.00059294549212\n",
      "Iteration 38, the loss is 355.8055556158561, parameters k is 71.33891122618289 and b is -70.00061270833797\n",
      "Iteration 39, the loss is 355.41056955918424, parameters k is 71.27606488230937 and b is -70.00063247118382\n",
      "Iteration 40, the loss is 355.0155835025111, parameters k is 71.21321853843585 and b is -70.00065223402967\n",
      "Iteration 41, the loss is 354.62059744583814, parameters k is 71.15037219456234 and b is -70.00067199687552\n",
      "Iteration 42, the loss is 354.2256113891656, parameters k is 71.08752585068882 and b is -70.00069175972138\n",
      "Iteration 43, the loss is 353.83062533249284, parameters k is 71.0246795068153 and b is -70.00071152256723\n",
      "Iteration 44, the loss is 353.4356392758201, parameters k is 70.96183316294179 and b is -70.00073128541308\n",
      "Iteration 45, the loss is 353.0406532191472, parameters k is 70.89898681906827 and b is -70.00075104825893\n",
      "Iteration 46, the loss is 352.64566716247447, parameters k is 70.83614047519475 and b is -70.00077081110479\n",
      "Iteration 47, the loss is 352.2506811058022, parameters k is 70.77329413132124 and b is -70.00079057395064\n",
      "Iteration 48, the loss is 351.85569504912957, parameters k is 70.71044778744772 and b is -70.00081033679649\n",
      "Iteration 49, the loss is 351.4607089924567, parameters k is 70.6476014435742 and b is -70.00083009964234\n",
      "Iteration 50, the loss is 351.0657229357836, parameters k is 70.58475509970069 and b is -70.0008498624882\n",
      "Iteration 51, the loss is 350.67073687911125, parameters k is 70.52190875582717 and b is -70.00086962533405\n",
      "Iteration 52, the loss is 350.2757508224389, parameters k is 70.45906241195365 and b is -70.0008893881799\n",
      "Iteration 53, the loss is 349.88076476576595, parameters k is 70.39621606808014 and b is -70.00090915102575\n",
      "Iteration 54, the loss is 349.48577870909287, parameters k is 70.33336972420662 and b is -70.0009289138716\n",
      "Iteration 55, the loss is 349.0907926524204, parameters k is 70.2705233803331 and b is -70.00094867671746\n",
      "Iteration 56, the loss is 348.69580659574814, parameters k is 70.20767703645959 and b is -70.00096843956331\n",
      "Iteration 57, the loss is 348.3008205390751, parameters k is 70.14483069258607 and b is -70.00098820240916\n",
      "Iteration 58, the loss is 347.90583448240255, parameters k is 70.08198434871255 and b is -70.00100796525501\n",
      "Iteration 59, the loss is 347.51084842572976, parameters k is 70.01913800483904 and b is -70.00102772810087\n",
      "Iteration 60, the loss is 347.1158623690573, parameters k is 69.95629166096552 and b is -70.00104749094672\n",
      "Iteration 61, the loss is 346.72087631238435, parameters k is 69.893445317092 and b is -70.00106725379257\n",
      "Iteration 62, the loss is 346.32589025571167, parameters k is 69.83059897321849 and b is -70.00108701663842\n",
      "Iteration 63, the loss is 345.93090419903865, parameters k is 69.76775262934497 and b is -70.00110677948427\n",
      "Iteration 64, the loss is 345.5359181423665, parameters k is 69.70490628547145 and b is -70.00112654233013\n",
      "Iteration 65, the loss is 345.14093208569375, parameters k is 69.64205994159794 and b is -70.00114630517598\n",
      "Iteration 66, the loss is 344.745946029021, parameters k is 69.57921359772442 and b is -70.00116606802183\n",
      "Iteration 67, the loss is 344.35095997234845, parameters k is 69.5163672538509 and b is -70.00118583086768\n",
      "Iteration 68, the loss is 343.9559739156756, parameters k is 69.45352090997739 and b is -70.00120559371354\n",
      "Iteration 69, the loss is 343.56098785900275, parameters k is 69.39067456610387 and b is -70.00122535655939\n",
      "Iteration 70, the loss is 343.16600180233024, parameters k is 69.32782822223035 and b is -70.00124511940524\n",
      "Iteration 71, the loss is 342.77101574565756, parameters k is 69.26498187835683 and b is -70.00126488225109\n",
      "Iteration 72, the loss is 342.3760296889842, parameters k is 69.20213553448332 and b is -70.00128464509694\n",
      "Iteration 73, the loss is 341.9810436323119, parameters k is 69.1392891906098 and b is -70.0013044079428\n",
      "Iteration 74, the loss is 341.58605757563913, parameters k is 69.07644284673628 and b is -70.00132417078865\n",
      "Iteration 75, the loss is 341.19107151896674, parameters k is 69.01359650286277 and b is -70.0013439336345\n",
      "Iteration 76, the loss is 340.79608546229434, parameters k is 68.95075015898925 and b is -70.00136369648035\n",
      "Iteration 77, the loss is 340.4010994056214, parameters k is 68.88790381511573 and b is -70.0013834593262\n",
      "Iteration 78, the loss is 340.00611334894836, parameters k is 68.82505747124222 and b is -70.00140322217206\n",
      "Iteration 79, the loss is 339.61112729227597, parameters k is 68.7622111273687 and b is -70.00142298501791\n",
      "Iteration 80, the loss is 339.2161412356031, parameters k is 68.69936478349518 and b is -70.00144274786376\n",
      "Iteration 81, the loss is 338.8211551789305, parameters k is 68.63651843962167 and b is -70.00146251070962\n",
      "Iteration 82, the loss is 338.4261691222583, parameters k is 68.57367209574815 and b is -70.00148227355547\n",
      "Iteration 83, the loss is 338.0311830655851, parameters k is 68.51082575187463 and b is -70.00150203640132\n",
      "Iteration 84, the loss is 337.63619700891263, parameters k is 68.44797940800112 and b is -70.00152179924717\n",
      "Iteration 85, the loss is 337.24121095223956, parameters k is 68.3851330641276 and b is -70.00154156209302\n",
      "Iteration 86, the loss is 336.846224895567, parameters k is 68.32228672025408 and b is -70.00156132493888\n",
      "Iteration 87, the loss is 336.451238838894, parameters k is 68.25944037638057 and b is -70.00158108778473\n",
      "Iteration 88, the loss is 336.05625278222135, parameters k is 68.19659403250705 and b is -70.00160085063058\n",
      "Iteration 89, the loss is 335.66126672554856, parameters k is 68.13374768863353 and b is -70.00162061347643\n",
      "Iteration 90, the loss is 335.26628066887633, parameters k is 68.07090134476002 and b is -70.00164037632229\n",
      "Iteration 91, the loss is 334.87129461220326, parameters k is 68.0080550008865 and b is -70.00166013916814\n",
      "Iteration 92, the loss is 334.47630855553103, parameters k is 67.94520865701298 and b is -70.00167990201399\n",
      "Iteration 93, the loss is 334.0813224988581, parameters k is 67.88236231313947 and b is -70.00169966485984\n",
      "Iteration 94, the loss is 333.68633644218517, parameters k is 67.81951596926595 and b is -70.0017194277057\n",
      "Iteration 95, the loss is 333.2913503855132, parameters k is 67.75666962539243 and b is -70.00173919055155\n",
      "Iteration 96, the loss is 332.8963643288408, parameters k is 67.69382328151892 and b is -70.0017589533974\n",
      "Iteration 97, the loss is 332.5013782721676, parameters k is 67.6309769376454 and b is -70.00177871624325\n",
      "Iteration 98, the loss is 332.1063922154947, parameters k is 67.56813059377188 and b is -70.0017984790891\n",
      "Iteration 99, the loss is 331.7114061588225, parameters k is 67.50528424989837 and b is -70.00181824193496\n",
      "Iteration 100, the loss is 331.3164201021494, parameters k is 67.44243790602485 and b is -70.00183800478081\n",
      "Iteration 101, the loss is 330.9214340454769, parameters k is 67.37959156215133 and b is -70.00185776762666\n",
      "Iteration 102, the loss is 330.52644798880425, parameters k is 67.31674521827782 and b is -70.00187753047251\n",
      "Iteration 103, the loss is 330.13146193213146, parameters k is 67.2538988744043 and b is -70.00189729331836\n",
      "Iteration 104, the loss is 329.73647587545895, parameters k is 67.19105253053078 and b is -70.00191705616422\n",
      "Iteration 105, the loss is 329.3414898187854, parameters k is 67.12820618665727 and b is -70.00193681901007\n",
      "Iteration 106, the loss is 328.94650376211365, parameters k is 67.06535984278375 and b is -70.00195658185592\n",
      "Iteration 107, the loss is 328.5515177054403, parameters k is 67.00251349891023 and b is -70.00197634470177\n",
      "Iteration 108, the loss is 328.1565316487682, parameters k is 66.93966715503672 and b is -70.00199610754763\n",
      "Iteration 109, the loss is 327.7615455920954, parameters k is 66.8768208111632 and b is -70.00201587039348\n",
      "Iteration 110, the loss is 327.3665595354225, parameters k is 66.81397446728968 and b is -70.00203563323933\n",
      "Iteration 111, the loss is 326.9715734787496, parameters k is 66.75112812341617 and b is -70.00205539608518\n",
      "Iteration 112, the loss is 326.57658742207724, parameters k is 66.68828177954265 and b is -70.00207515893104\n",
      "Iteration 113, the loss is 326.1816013654041, parameters k is 66.62543543566913 and b is -70.00209492177689\n",
      "Iteration 114, the loss is 325.786615308732, parameters k is 66.56258909179562 and b is -70.00211468462274\n",
      "Iteration 115, the loss is 325.39162925205875, parameters k is 66.4997427479221 and b is -70.00213444746859\n",
      "Iteration 116, the loss is 324.9966431953864, parameters k is 66.43689640404858 and b is -70.00215421031444\n",
      "Iteration 117, the loss is 324.60165713871334, parameters k is 66.37405006017507 and b is -70.0021739731603\n",
      "Iteration 118, the loss is 324.2066710820407, parameters k is 66.31120371630155 and b is -70.00219373600615\n",
      "Iteration 119, the loss is 323.81168502536815, parameters k is 66.24835737242803 and b is -70.002213498852\n",
      "Iteration 120, the loss is 323.4166989686956, parameters k is 66.18551102855452 and b is -70.00223326169785\n",
      "Iteration 121, the loss is 323.0217129120229, parameters k is 66.122664684681 and b is -70.0022530245437\n",
      "Iteration 122, the loss is 322.62672685535034, parameters k is 66.05981834080748 and b is -70.00227278738956\n",
      "Iteration 123, the loss is 322.23174079867783, parameters k is 65.99697199693396 and b is -70.00229255023541\n",
      "Iteration 124, the loss is 321.83675474200464, parameters k is 65.93412565306045 and b is -70.00231231308126\n",
      "Iteration 125, the loss is 321.4417686853323, parameters k is 65.87127930918693 and b is -70.00233207592711\n",
      "Iteration 126, the loss is 321.04678262865946, parameters k is 65.80843296531341 and b is -70.00235183877297\n",
      "Iteration 127, the loss is 320.65179657198684, parameters k is 65.7455866214399 and b is -70.00237160161882\n",
      "Iteration 128, the loss is 320.25681051531404, parameters k is 65.68274027756638 and b is -70.00239136446467\n",
      "Iteration 129, the loss is 319.86182445864114, parameters k is 65.61989393369286 and b is -70.00241112731052\n",
      "Iteration 130, the loss is 319.4668384019684, parameters k is 65.55704758981935 and b is -70.00243089015638\n",
      "Iteration 131, the loss is 319.0718523452959, parameters k is 65.49420124594583 and b is -70.00245065300223\n",
      "Iteration 132, the loss is 318.67686628862356, parameters k is 65.43135490207231 and b is -70.00247041584808\n",
      "Iteration 133, the loss is 318.2818802319503, parameters k is 65.3685085581988 and b is -70.00249017869393\n",
      "Iteration 134, the loss is 317.886894175278, parameters k is 65.30566221432528 and b is -70.00250994153978\n",
      "Iteration 135, the loss is 317.4919081186049, parameters k is 65.24281587045176 and b is -70.00252970438564\n",
      "Iteration 136, the loss is 317.0969220619326, parameters k is 65.17996952657825 and b is -70.00254946723149\n",
      "Iteration 137, the loss is 316.70193600526, parameters k is 65.11712318270473 and b is -70.00256923007734\n",
      "Iteration 138, the loss is 316.3069499485871, parameters k is 65.05427683883121 and b is -70.0025889929232\n",
      "Iteration 139, the loss is 315.91196389191435, parameters k is 64.9914304949577 and b is -70.00260875576905\n",
      "Iteration 140, the loss is 315.5169778352418, parameters k is 64.92858415108418 and b is -70.0026285186149\n",
      "Iteration 141, the loss is 315.12199177856917, parameters k is 64.86573780721066 and b is -70.00264828146075\n",
      "Iteration 142, the loss is 314.7270057218961, parameters k is 64.80289146333715 and b is -70.0026680443066\n",
      "Iteration 143, the loss is 314.3320196652237, parameters k is 64.74004511946363 and b is -70.00268780715245\n",
      "Iteration 144, the loss is 313.9370336085508, parameters k is 64.67719877559011 and b is -70.00270756999831\n",
      "Iteration 145, the loss is 313.54204755187817, parameters k is 64.6143524317166 and b is -70.00272733284416\n",
      "Iteration 146, the loss is 313.1470614952055, parameters k is 64.55150608784308 and b is -70.00274709569001\n",
      "Iteration 147, the loss is 312.7520754385328, parameters k is 64.48865974396956 and b is -70.00276685853586\n",
      "Iteration 148, the loss is 312.3570893818603, parameters k is 64.42581340009605 and b is -70.00278662138172\n",
      "Iteration 149, the loss is 311.9621033251877, parameters k is 64.36296705622253 and b is -70.00280638422757\n",
      "Iteration 150, the loss is 311.56711726851466, parameters k is 64.30012071234901 and b is -70.00282614707342\n",
      "Iteration 151, the loss is 311.17213121184204, parameters k is 64.2372743684755 and b is -70.00284590991927\n",
      "Iteration 152, the loss is 310.7771451551697, parameters k is 64.17442802460198 and b is -70.00286567276513\n",
      "Iteration 153, the loss is 310.38215909849686, parameters k is 64.11158168072846 and b is -70.00288543561098\n",
      "Iteration 154, the loss is 309.9871730418238, parameters k is 64.04873533685495 and b is -70.00290519845683\n",
      "Iteration 155, the loss is 309.5921869851516, parameters k is 63.98588899298143 and b is -70.00292496130268\n",
      "Iteration 156, the loss is 309.1972009284787, parameters k is 63.92304264910791 and b is -70.00294472414853\n",
      "Iteration 157, the loss is 308.802214871806, parameters k is 63.860196305234396 and b is -70.00296448699439\n",
      "Iteration 158, the loss is 308.4072288151332, parameters k is 63.79734996136088 and b is -70.00298424984024\n",
      "Iteration 159, the loss is 308.0122427584605, parameters k is 63.73450361748736 and b is -70.00300401268609\n",
      "Iteration 160, the loss is 307.61725670178794, parameters k is 63.671657273613846 and b is -70.00302377553194\n",
      "Iteration 161, the loss is 307.2222706451149, parameters k is 63.60881092974033 and b is -70.0030435383778\n",
      "Iteration 162, the loss is 306.8272845884426, parameters k is 63.54596458586681 and b is -70.00306330122365\n",
      "Iteration 163, the loss is 306.43229853177013, parameters k is 63.483118241993296 and b is -70.0030830640695\n",
      "Iteration 164, the loss is 306.0373124750973, parameters k is 63.42027189811978 and b is -70.00310282691535\n",
      "Iteration 165, the loss is 305.64232641842466, parameters k is 63.35742555424626 and b is -70.0031225897612\n",
      "Iteration 166, the loss is 305.24734036175204, parameters k is 63.294579210372746 and b is -70.00314235260706\n",
      "Iteration 167, the loss is 304.8523543050791, parameters k is 63.23173286649923 and b is -70.00316211545291\n",
      "Iteration 168, the loss is 304.4573682484063, parameters k is 63.16888652262571 and b is -70.00318187829876\n",
      "Iteration 169, the loss is 304.0623821917338, parameters k is 63.106040178752195 and b is -70.00320164114461\n",
      "Iteration 170, the loss is 303.6673961350611, parameters k is 63.04319383487868 and b is -70.00322140399047\n",
      "Iteration 171, the loss is 303.27241007838825, parameters k is 62.98034749100516 and b is -70.00324116683632\n",
      "Iteration 172, the loss is 302.8774240217157, parameters k is 62.917501147131645 and b is -70.00326092968217\n",
      "Iteration 173, the loss is 302.48243796504283, parameters k is 62.85465480325813 and b is -70.00328069252802\n",
      "Iteration 174, the loss is 302.0874519083702, parameters k is 62.79180845938461 and b is -70.00330045537387\n",
      "Iteration 175, the loss is 301.69246585169753, parameters k is 62.728962115511095 and b is -70.00332021821973\n",
      "Iteration 176, the loss is 301.2974797950248, parameters k is 62.66611577163758 and b is -70.00333998106558\n",
      "Iteration 177, the loss is 300.9024937383522, parameters k is 62.60326942776406 and b is -70.00335974391143\n",
      "Iteration 178, the loss is 300.50750768167956, parameters k is 62.540423083890545 and b is -70.00337950675728\n",
      "Iteration 179, the loss is 300.11252162500705, parameters k is 62.47757674001703 and b is -70.00339926960314\n",
      "Iteration 180, the loss is 299.71753556833363, parameters k is 62.41473039614351 and b is -70.00341903244899\n",
      "Iteration 181, the loss is 299.32254951166124, parameters k is 62.351884052269995 and b is -70.00343879529484\n",
      "Iteration 182, the loss is 298.9275634549888, parameters k is 62.28903770839648 and b is -70.0034585581407\n",
      "Iteration 183, the loss is 298.5325773983162, parameters k is 62.22619136452296 and b is -70.00347832098655\n",
      "Iteration 184, the loss is 298.1375913416435, parameters k is 62.163345020649444 and b is -70.0034980838324\n",
      "Iteration 185, the loss is 297.74260528497086, parameters k is 62.10049867677593 and b is -70.00351784667825\n",
      "Iteration 186, the loss is 297.34761922829796, parameters k is 62.03765233290241 and b is -70.0035376095241\n",
      "Iteration 187, the loss is 296.9526331716253, parameters k is 61.974805989028894 and b is -70.00355737236995\n",
      "Iteration 188, the loss is 296.5576471149528, parameters k is 61.91195964515538 and b is -70.0035771352158\n",
      "Iteration 189, the loss is 296.16266105827975, parameters k is 61.84911330128186 and b is -70.00359689806166\n",
      "Iteration 190, the loss is 295.76767500160724, parameters k is 61.786266957408344 and b is -70.00361666090751\n",
      "Iteration 191, the loss is 295.3726889449348, parameters k is 61.72342061353483 and b is -70.00363642375336\n",
      "Iteration 192, the loss is 294.977702888262, parameters k is 61.66057426966131 and b is -70.00365618659922\n",
      "Iteration 193, the loss is 294.58271683158904, parameters k is 61.597727925787794 and b is -70.00367594944507\n",
      "Iteration 194, the loss is 294.18773077491664, parameters k is 61.53488158191428 and b is -70.00369571229092\n",
      "Iteration 195, the loss is 293.7927447182436, parameters k is 61.47203523804076 and b is -70.00371547513677\n",
      "Iteration 196, the loss is 293.3977586615712, parameters k is 61.409188894167244 and b is -70.00373523798262\n",
      "Iteration 197, the loss is 293.0027726048985, parameters k is 61.34634255029373 and b is -70.00375500082848\n",
      "Iteration 198, the loss is 292.6077865482258, parameters k is 61.28349620642021 and b is -70.00377476367433\n",
      "Iteration 199, the loss is 292.21280049155285, parameters k is 61.22064986254669 and b is -70.00379452652018\n",
      "Iteration 200, the loss is 291.8178144348802, parameters k is 61.15780351867318 and b is -70.00381428936603\n",
      "Iteration 201, the loss is 291.4228283782076, parameters k is 61.09495717479966 and b is -70.00383405221189\n",
      "Iteration 202, the loss is 291.0278423215352, parameters k is 61.03211083092614 and b is -70.00385381505774\n",
      "Iteration 203, the loss is 290.6328562648622, parameters k is 60.96926448705263 and b is -70.00387357790359\n",
      "Iteration 204, the loss is 290.23787020818963, parameters k is 60.90641814317911 and b is -70.00389334074944\n",
      "Iteration 205, the loss is 289.8428841515169, parameters k is 60.84357179930559 and b is -70.0039131035953\n",
      "Iteration 206, the loss is 289.44789809484433, parameters k is 60.780725455432076 and b is -70.00393286644115\n",
      "Iteration 207, the loss is 289.0529120381714, parameters k is 60.71787911155856 and b is -70.003952629287\n",
      "Iteration 208, the loss is 288.65792598149875, parameters k is 60.65503276768504 and b is -70.00397239213285\n",
      "Iteration 209, the loss is 288.2629399248261, parameters k is 60.592186423811526 and b is -70.0039921549787\n",
      "Iteration 210, the loss is 287.8679538681537, parameters k is 60.52934007993801 and b is -70.00401191782456\n",
      "Iteration 211, the loss is 287.4729678114809, parameters k is 60.46649373606449 and b is -70.00403168067041\n",
      "Iteration 212, the loss is 287.07798175480804, parameters k is 60.403647392190976 and b is -70.00405144351626\n",
      "Iteration 213, the loss is 286.6829956981353, parameters k is 60.34080104831746 and b is -70.00407120636211\n",
      "Iteration 214, the loss is 286.2880096414626, parameters k is 60.27795470444394 and b is -70.00409096920797\n",
      "Iteration 215, the loss is 285.89302358479006, parameters k is 60.215108360570426 and b is -70.00411073205382\n",
      "Iteration 216, the loss is 285.49803752811715, parameters k is 60.15226201669691 and b is -70.00413049489967\n",
      "Iteration 217, the loss is 285.10305147144487, parameters k is 60.08941567282339 and b is -70.00415025774552\n",
      "Iteration 218, the loss is 284.7080654147717, parameters k is 60.026569328949876 and b is -70.00417002059137\n",
      "Iteration 219, the loss is 284.31307935809923, parameters k is 59.96372298507636 and b is -70.00418978343723\n",
      "Iteration 220, the loss is 283.91809330142644, parameters k is 59.90087664120284 and b is -70.00420954628308\n",
      "Iteration 221, the loss is 283.5231072447536, parameters k is 59.838030297329325 and b is -70.00422930912893\n",
      "Iteration 222, the loss is 283.12812118808114, parameters k is 59.77518395345581 and b is -70.00424907197478\n",
      "Iteration 223, the loss is 282.7331351314085, parameters k is 59.71233760958229 and b is -70.00426883482064\n",
      "Iteration 224, the loss is 282.33814907473595, parameters k is 59.649491265708775 and b is -70.00428859766649\n",
      "Iteration 225, the loss is 281.94316301806316, parameters k is 59.58664492183526 and b is -70.00430836051234\n",
      "Iteration 226, the loss is 281.5481769613905, parameters k is 59.52379857796174 and b is -70.00432812335819\n",
      "Iteration 227, the loss is 281.1531909047172, parameters k is 59.460952234088225 and b is -70.00434788620404\n",
      "Iteration 228, the loss is 280.75820484804507, parameters k is 59.39810589021471 and b is -70.0043676490499\n",
      "Iteration 229, the loss is 280.36321879137216, parameters k is 59.33525954634119 and b is -70.00438741189575\n",
      "Iteration 230, the loss is 279.96823273469954, parameters k is 59.272413202467675 and b is -70.0044071747416\n",
      "Iteration 231, the loss is 279.5732466780271, parameters k is 59.20956685859416 and b is -70.00442693758745\n",
      "Iteration 232, the loss is 279.17826062135424, parameters k is 59.14672051472064 and b is -70.0044467004333\n",
      "Iteration 233, the loss is 278.7832745646817, parameters k is 59.083874170847125 and b is -70.00446646327916\n",
      "Iteration 234, the loss is 278.3882885080088, parameters k is 59.02102782697361 and b is -70.00448622612501\n",
      "Iteration 235, the loss is 277.9933024513361, parameters k is 58.95818148310009 and b is -70.00450598897086\n",
      "Iteration 236, the loss is 277.5983163946636, parameters k is 58.895335139226574 and b is -70.00452575181671\n",
      "Iteration 237, the loss is 277.2033303379909, parameters k is 58.83248879535306 and b is -70.00454551466257\n",
      "Iteration 238, the loss is 276.808344281318, parameters k is 58.76964245147954 and b is -70.00456527750842\n",
      "Iteration 239, the loss is 276.41335822464544, parameters k is 58.706796107606024 and b is -70.00458504035427\n",
      "Iteration 240, the loss is 276.01837216797253, parameters k is 58.64394976373251 and b is -70.00460480320012\n",
      "Iteration 241, the loss is 275.62338611130014, parameters k is 58.58110341985899 and b is -70.00462456604598\n",
      "Iteration 242, the loss is 275.2284000546271, parameters k is 58.518257075985474 and b is -70.00464432889183\n",
      "Iteration 243, the loss is 274.8334139979548, parameters k is 58.45541073211196 and b is -70.00466409173768\n",
      "Iteration 244, the loss is 274.438427941282, parameters k is 58.39256438823844 and b is -70.00468385458353\n",
      "Iteration 245, the loss is 274.0434418846093, parameters k is 58.329718044364924 and b is -70.00470361742939\n",
      "Iteration 246, the loss is 273.6484558279366, parameters k is 58.26687170049141 and b is -70.00472338027524\n",
      "Iteration 247, the loss is 273.25346977126395, parameters k is 58.20402535661789 and b is -70.00474314312109\n",
      "Iteration 248, the loss is 272.85848371459116, parameters k is 58.141179012744374 and b is -70.00476290596694\n",
      "Iteration 249, the loss is 272.4634976579189, parameters k is 58.07833266887086 and b is -70.0047826688128\n",
      "Iteration 250, the loss is 272.0685116012458, parameters k is 58.01548632499734 and b is -70.00480243165865\n",
      "Iteration 251, the loss is 271.67352554457295, parameters k is 57.95263998112382 and b is -70.0048221945045\n",
      "Iteration 252, the loss is 271.27853948790033, parameters k is 57.88979363725031 and b is -70.00484195735035\n",
      "Iteration 253, the loss is 270.883553431228, parameters k is 57.82694729337679 and b is -70.0048617201962\n",
      "Iteration 254, the loss is 270.488567374555, parameters k is 57.76410094950327 and b is -70.00488148304206\n",
      "Iteration 255, the loss is 270.09358131788235, parameters k is 57.70125460562976 and b is -70.00490124588791\n",
      "Iteration 256, the loss is 269.69859526120985, parameters k is 57.63840826175624 and b is -70.00492100873376\n",
      "Iteration 257, the loss is 269.3036092045369, parameters k is 57.57556191788272 and b is -70.00494077157961\n",
      "Iteration 258, the loss is 268.9086231478643, parameters k is 57.512715574009206 and b is -70.00496053442546\n",
      "Iteration 259, the loss is 268.5136370911916, parameters k is 57.44986923013569 and b is -70.00498029727132\n",
      "Iteration 260, the loss is 268.1186510345189, parameters k is 57.38702288626217 and b is -70.00500006011717\n",
      "Iteration 261, the loss is 267.72366497784645, parameters k is 57.324176542388656 and b is -70.00501982296302\n",
      "Iteration 262, the loss is 267.32867892117355, parameters k is 57.26133019851514 and b is -70.00503958580887\n",
      "Iteration 263, the loss is 266.933692864501, parameters k is 57.19848385464162 and b is -70.00505934865473\n",
      "Iteration 264, the loss is 266.538706807828, parameters k is 57.135637510768106 and b is -70.00507911150058\n",
      "Iteration 265, the loss is 266.1437207511554, parameters k is 57.07279116689459 and b is -70.00509887434643\n",
      "Iteration 266, the loss is 265.7487346944827, parameters k is 57.00994482302107 and b is -70.00511863719228\n",
      "Iteration 267, the loss is 265.3537486378102, parameters k is 56.947098479147556 and b is -70.00513840003813\n",
      "Iteration 268, the loss is 264.95876258113765, parameters k is 56.88425213527404 and b is -70.00515816288399\n",
      "Iteration 269, the loss is 264.5637765244647, parameters k is 56.82140579140052 and b is -70.00517792572984\n",
      "Iteration 270, the loss is 264.16879046779206, parameters k is 56.758559447527006 and b is -70.00519768857569\n",
      "Iteration 271, the loss is 263.7738044111194, parameters k is 56.69571310365349 and b is -70.00521745142154\n",
      "Iteration 272, the loss is 263.3788183544467, parameters k is 56.63286675977997 and b is -70.0052372142674\n",
      "Iteration 273, the loss is 262.98383229777403, parameters k is 56.570020415906455 and b is -70.00525697711325\n",
      "Iteration 274, the loss is 262.58884624110124, parameters k is 56.50717407203294 and b is -70.0052767399591\n",
      "Iteration 275, the loss is 262.1938601844285, parameters k is 56.44432772815942 and b is -70.00529650280495\n",
      "Iteration 276, the loss is 261.7988741277557, parameters k is 56.381481384285905 and b is -70.0053162656508\n",
      "Iteration 277, the loss is 261.40388807108314, parameters k is 56.31863504041239 and b is -70.00533602849666\n",
      "Iteration 278, the loss is 261.00890201441047, parameters k is 56.25578869653887 and b is -70.00535579134251\n",
      "Iteration 279, the loss is 260.6139159577377, parameters k is 56.192942352665355 and b is -70.00537555418836\n",
      "Iteration 280, the loss is 260.2189299010651, parameters k is 56.13009600879184 and b is -70.00539531703421\n",
      "Iteration 281, the loss is 259.82394384439266, parameters k is 56.06724966491832 and b is -70.00541507988007\n",
      "Iteration 282, the loss is 259.4289577877197, parameters k is 56.004403321044805 and b is -70.00543484272592\n",
      "Iteration 283, the loss is 259.03397173104685, parameters k is 55.94155697717129 and b is -70.00545460557177\n",
      "Iteration 284, the loss is 258.6389856743743, parameters k is 55.87871063329777 and b is -70.00547436841762\n",
      "Iteration 285, the loss is 258.24399961770195, parameters k is 55.815864289424255 and b is -70.00549413126348\n",
      "Iteration 286, the loss is 257.8490135610286, parameters k is 55.75301794555074 and b is -70.00551389410933\n",
      "Iteration 287, the loss is 257.4540275043563, parameters k is 55.69017160167722 and b is -70.00553365695518\n",
      "Iteration 288, the loss is 257.05904144768385, parameters k is 55.627325257803705 and b is -70.00555341980103\n",
      "Iteration 289, the loss is 256.66405539101106, parameters k is 55.56447891393019 and b is -70.00557318264688\n",
      "Iteration 290, the loss is 256.2690693343383, parameters k is 55.50163257005667 and b is -70.00559294549274\n",
      "Iteration 291, the loss is 255.87408327766553, parameters k is 55.438786226183154 and b is -70.00561270833859\n",
      "Iteration 292, the loss is 255.4790972209929, parameters k is 55.37593988230964 and b is -70.00563247118444\n",
      "Iteration 293, the loss is 255.08411116431998, parameters k is 55.31309353843612 and b is -70.0056522340303\n",
      "Iteration 294, the loss is 254.6891251076475, parameters k is 55.250247194562604 and b is -70.00567199687615\n",
      "Iteration 295, the loss is 254.29413905097468, parameters k is 55.18740085068909 and b is -70.005691759722\n",
      "Iteration 296, the loss is 253.89915299430209, parameters k is 55.12455450681557 and b is -70.00571152256785\n",
      "Iteration 297, the loss is 253.50416693762963, parameters k is 55.061708162942054 and b is -70.0057312854137\n",
      "Iteration 298, the loss is 253.1091808809567, parameters k is 54.99886181906854 and b is -70.00575104825955\n",
      "Iteration 299, the loss is 252.71419482428394, parameters k is 54.93601547519502 and b is -70.0057708111054\n",
      "Iteration 300, the loss is 252.31920876761143, parameters k is 54.873169131321504 and b is -70.00579057395126\n",
      "Iteration 301, the loss is 251.92422271093866, parameters k is 54.81032278744799 and b is -70.00581033679711\n",
      "Iteration 302, the loss is 251.52923665426601, parameters k is 54.74747644357447 and b is -70.00583009964296\n",
      "Iteration 303, the loss is 251.1342505975931, parameters k is 54.684630099700954 and b is -70.00584986248882\n",
      "Iteration 304, the loss is 250.73926454092046, parameters k is 54.62178375582744 and b is -70.00586962533467\n",
      "Iteration 305, the loss is 250.344278484248, parameters k is 54.55893741195392 and b is -70.00588938818052\n",
      "Iteration 306, the loss is 249.94929242757516, parameters k is 54.4960910680804 and b is -70.00590915102637\n",
      "Iteration 307, the loss is 249.55430637090257, parameters k is 54.43324472420689 and b is -70.00592891387222\n",
      "Iteration 308, the loss is 249.15932031422977, parameters k is 54.37039838033337 and b is -70.00594867671808\n",
      "Iteration 309, the loss is 248.76433425755724, parameters k is 54.30755203645985 and b is -70.00596843956393\n",
      "Iteration 310, the loss is 248.3693482008843, parameters k is 54.24470569258634 and b is -70.00598820240978\n",
      "Iteration 311, the loss is 247.9743621442117, parameters k is 54.18185934871282 and b is -70.00600796525563\n",
      "Iteration 312, the loss is 247.57937608753892, parameters k is 54.1190130048393 and b is -70.00602772810149\n",
      "Iteration 313, the loss is 247.18439003086644, parameters k is 54.056166660965786 and b is -70.00604749094734\n",
      "Iteration 314, the loss is 246.7894039741937, parameters k is 53.99332031709227 and b is -70.00606725379319\n",
      "Iteration 315, the loss is 246.39441791752114, parameters k is 53.93047397321875 and b is -70.00608701663904\n",
      "Iteration 316, the loss is 245.9994318608485, parameters k is 53.867627629345236 and b is -70.0061067794849\n",
      "Iteration 317, the loss is 245.6044458041756, parameters k is 53.80478128547172 and b is -70.00612654233075\n",
      "Iteration 318, the loss is 245.2094597475029, parameters k is 53.7419349415982 and b is -70.0061463051766\n",
      "Iteration 319, the loss is 244.81447369083045, parameters k is 53.679088597724686 and b is -70.00616606802245\n",
      "Iteration 320, the loss is 244.4194876341576, parameters k is 53.61624225385117 and b is -70.0061858308683\n",
      "Iteration 321, the loss is 244.02450157748513, parameters k is 53.55339590997765 and b is -70.00620559371416\n",
      "Iteration 322, the loss is 243.62951552081208, parameters k is 53.490549566104136 and b is -70.00622535656001\n",
      "Iteration 323, the loss is 243.2345294641394, parameters k is 53.42770322223062 and b is -70.00624511940586\n",
      "Iteration 324, the loss is 242.83954340746675, parameters k is 53.3648568783571 and b is -70.00626488225171\n",
      "Iteration 325, the loss is 242.4445573507941, parameters k is 53.302010534483586 and b is -70.00628464509757\n",
      "Iteration 326, the loss is 242.04957129412125, parameters k is 53.23916419061007 and b is -70.00630440794342\n",
      "Iteration 327, the loss is 241.65458523744863, parameters k is 53.17631784673655 and b is -70.00632417078927\n",
      "Iteration 328, the loss is 241.2595991807758, parameters k is 53.113471502863035 and b is -70.00634393363512\n",
      "Iteration 329, the loss is 240.8646131241034, parameters k is 53.05062515898952 and b is -70.00636369648097\n",
      "Iteration 330, the loss is 240.46962706743057, parameters k is 52.987778815116 and b is -70.00638345932683\n",
      "Iteration 331, the loss is 240.07464101075777, parameters k is 52.924932471242485 and b is -70.00640322217268\n",
      "Iteration 332, the loss is 239.67965495408518, parameters k is 52.86208612736897 and b is -70.00642298501853\n",
      "Iteration 333, the loss is 239.2846688974125, parameters k is 52.79923978349545 and b is -70.00644274786438\n",
      "Iteration 334, the loss is 238.8896828407399, parameters k is 52.736393439621935 and b is -70.00646251071024\n",
      "Iteration 335, the loss is 238.4946967840672, parameters k is 52.67354709574842 and b is -70.00648227355609\n",
      "Iteration 336, the loss is 238.09971072739458, parameters k is 52.6107007518749 and b is -70.00650203640194\n",
      "Iteration 337, the loss is 237.7047246707219, parameters k is 52.547854408001385 and b is -70.00652179924779\n",
      "Iteration 338, the loss is 237.3097386140489, parameters k is 52.48500806412787 and b is -70.00654156209364\n",
      "Iteration 339, the loss is 236.91475255737635, parameters k is 52.42216172025435 and b is -70.0065613249395\n",
      "Iteration 340, the loss is 236.51976650070358, parameters k is 52.359315376380835 and b is -70.00658108778535\n",
      "Iteration 341, the loss is 236.12478044403105, parameters k is 52.29646903250732 and b is -70.0066008506312\n",
      "Iteration 342, the loss is 235.7297943873583, parameters k is 52.2336226886338 and b is -70.00662061347705\n",
      "Iteration 343, the loss is 235.3348083306856, parameters k is 52.170776344760284 and b is -70.0066403763229\n",
      "Iteration 344, the loss is 234.93982227401324, parameters k is 52.10793000088677 and b is -70.00666013916876\n",
      "Iteration 345, the loss is 234.54483621734025, parameters k is 52.04508365701325 and b is -70.00667990201461\n",
      "Iteration 346, the loss is 234.14985016066748, parameters k is 51.982237313139734 and b is -70.00669966486046\n",
      "Iteration 347, the loss is 233.7548641039946, parameters k is 51.91939096926622 and b is -70.00671942770632\n",
      "Iteration 348, the loss is 233.35987804732227, parameters k is 51.8565446253927 and b is -70.00673919055217\n",
      "Iteration 349, the loss is 232.96489199064956, parameters k is 51.793698281519184 and b is -70.00675895339802\n",
      "Iteration 350, the loss is 232.56990593397666, parameters k is 51.73085193764567 and b is -70.00677871624387\n",
      "Iteration 351, the loss is 232.17491987730432, parameters k is 51.66800559377215 and b is -70.00679847908972\n",
      "Iteration 352, the loss is 231.7799338206315, parameters k is 51.605159249898634 and b is -70.00681824193558\n",
      "Iteration 353, the loss is 231.38494776395873, parameters k is 51.54231290602512 and b is -70.00683800478143\n",
      "Iteration 354, the loss is 230.98996170728591, parameters k is 51.4794665621516 and b is -70.00685776762728\n",
      "Iteration 355, the loss is 230.59497565061335, parameters k is 51.416620218278084 and b is -70.00687753047313\n",
      "Iteration 356, the loss is 230.1999895939408, parameters k is 51.35377387440457 and b is -70.00689729331899\n",
      "Iteration 357, the loss is 229.80500353726785, parameters k is 51.29092753053105 and b is -70.00691705616484\n",
      "Iteration 358, the loss is 229.41001748059534, parameters k is 51.22808118665753 and b is -70.00693681901069\n",
      "Iteration 359, the loss is 229.01503142392247, parameters k is 51.16523484278402 and b is -70.00695658185654\n",
      "Iteration 360, the loss is 228.62004536724973, parameters k is 51.1023884989105 and b is -70.0069763447024\n",
      "Iteration 361, the loss is 228.22505931057722, parameters k is 51.03954215503698 and b is -70.00699610754825\n",
      "Iteration 362, the loss is 227.83007325390471, parameters k is 50.97669581116347 and b is -70.0070158703941\n",
      "Iteration 363, the loss is 227.43508719723178, parameters k is 50.91384946728995 and b is -70.00703563323995\n",
      "Iteration 364, the loss is 227.04010114055913, parameters k is 50.85100312341643 and b is -70.0070553960858\n",
      "Iteration 365, the loss is 226.64511508388645, parameters k is 50.788156779542916 and b is -70.00707515893166\n",
      "Iteration 366, the loss is 226.25012902721377, parameters k is 50.7253104356694 and b is -70.00709492177751\n",
      "Iteration 367, the loss is 225.85514297054115, parameters k is 50.66246409179588 and b is -70.00711468462336\n",
      "Iteration 368, the loss is 225.4601569138684, parameters k is 50.599617747922366 and b is -70.00713444746921\n",
      "Iteration 369, the loss is 225.06517085719574, parameters k is 50.53677140404885 and b is -70.00715421031506\n",
      "Iteration 370, the loss is 224.670184800523, parameters k is 50.47392506017533 and b is -70.00717397316092\n",
      "Iteration 371, the loss is 224.2751987438501, parameters k is 50.411078716301816 and b is -70.00719373600677\n",
      "Iteration 372, the loss is 223.88021268717765, parameters k is 50.3482323724283 and b is -70.00721349885262\n",
      "Iteration 373, the loss is 223.48522663050503, parameters k is 50.28538602855478 and b is -70.00723326169847\n",
      "Iteration 374, the loss is 223.0902405738322, parameters k is 50.222539684681266 and b is -70.00725302454433\n",
      "Iteration 375, the loss is 222.6952545171595, parameters k is 50.15969334080775 and b is -70.00727278739018\n",
      "Iteration 376, the loss is 222.30026846048662, parameters k is 50.09684699693423 and b is -70.00729255023603\n",
      "Iteration 377, the loss is 221.90528240381406, parameters k is 50.034000653060716 and b is -70.00731231308188\n",
      "Iteration 378, the loss is 221.51029634714152, parameters k is 49.9711543091872 and b is -70.00733207592774\n",
      "Iteration 379, the loss is 221.11531029046867, parameters k is 49.90830796531368 and b is -70.00735183877359\n",
      "Iteration 380, the loss is 220.72032423379633, parameters k is 49.845461621440165 and b is -70.00737160161944\n",
      "Iteration 381, the loss is 220.3253381771235, parameters k is 49.78261527756665 and b is -70.00739136446529\n",
      "Iteration 382, the loss is 219.93035212045078, parameters k is 49.71976893369313 and b is -70.00741112731114\n",
      "Iteration 383, the loss is 219.535366063778, parameters k is 49.656922589819615 and b is -70.007430890157\n",
      "Iteration 384, the loss is 219.14038000710522, parameters k is 49.5940762459461 and b is -70.00745065300285\n",
      "Iteration 385, the loss is 218.74539395043237, parameters k is 49.53122990207258 and b is -70.0074704158487\n",
      "Iteration 386, the loss is 218.35040789376, parameters k is 49.468383558199065 and b is -70.00749017869455\n",
      "Iteration 387, the loss is 217.95542183708736, parameters k is 49.40553721432555 and b is -70.0075099415404\n",
      "Iteration 388, the loss is 217.56043578041474, parameters k is 49.34269087045203 and b is -70.00752970438626\n",
      "Iteration 389, the loss is 217.16544972374186, parameters k is 49.279844526578515 and b is -70.00754946723211\n",
      "Iteration 390, the loss is 216.77046366706918, parameters k is 49.216998182705 and b is -70.00756923007796\n",
      "Iteration 391, the loss is 216.3754776103963, parameters k is 49.15415183883148 and b is -70.00758899292381\n",
      "Iteration 392, the loss is 215.98049155372374, parameters k is 49.091305494957965 and b is -70.00760875576967\n",
      "Iteration 393, the loss is 215.58550549705095, parameters k is 49.02845915108445 and b is -70.00762851861552\n",
      "Iteration 394, the loss is 215.1905194403783, parameters k is 48.96561280721093 and b is -70.00764828146137\n",
      "Iteration 395, the loss is 214.79553338370565, parameters k is 48.902766463337414 and b is -70.00766804430722\n",
      "Iteration 396, the loss is 214.40054732703297, parameters k is 48.8399201194639 and b is -70.00768780715308\n",
      "Iteration 397, the loss is 214.00556127036023, parameters k is 48.77707377559038 and b is -70.00770756999893\n",
      "Iteration 398, the loss is 213.61057521368772, parameters k is 48.714227431716864 and b is -70.00772733284478\n",
      "Iteration 399, the loss is 213.21558915701502, parameters k is 48.65138108784335 and b is -70.00774709569063\n",
      "Iteration 400, the loss is 212.8206031003425, parameters k is 48.58853474396983 and b is -70.00776685853648\n",
      "Iteration 401, the loss is 212.42561704366955, parameters k is 48.525688400096314 and b is -70.00778662138234\n",
      "Iteration 402, the loss is 212.03063098699684, parameters k is 48.4628420562228 and b is -70.00780638422819\n",
      "Iteration 403, the loss is 211.63564493032416, parameters k is 48.39999571234928 and b is -70.00782614707404\n",
      "Iteration 404, the loss is 211.2406588736517, parameters k is 48.337149368475764 and b is -70.0078459099199\n",
      "Iteration 405, the loss is 210.8456728169789, parameters k is 48.27430302460225 and b is -70.00786567276575\n",
      "Iteration 406, the loss is 210.45068676030627, parameters k is 48.21145668072873 and b is -70.0078854356116\n",
      "Iteration 407, the loss is 210.05570070363353, parameters k is 48.148610336855214 and b is -70.00790519845745\n",
      "Iteration 408, the loss is 209.66071464696083, parameters k is 48.0857639929817 and b is -70.0079249613033\n",
      "Iteration 409, the loss is 209.26572859028818, parameters k is 48.02291764910818 and b is -70.00794472414916\n",
      "Iteration 410, the loss is 208.87074253361553, parameters k is 47.96007130523466 and b is -70.00796448699501\n",
      "Iteration 411, the loss is 208.47575647694285, parameters k is 47.89722496136115 and b is -70.00798424984086\n",
      "Iteration 412, the loss is 208.08077042027, parameters k is 47.83437861748763 and b is -70.00800401268671\n",
      "Iteration 413, the loss is 207.6857843635971, parameters k is 47.77153227361411 and b is -70.00802377553256\n",
      "Iteration 414, the loss is 207.29079830692442, parameters k is 47.7086859297406 and b is -70.00804353837842\n",
      "Iteration 415, the loss is 206.89581225025177, parameters k is 47.64583958586708 and b is -70.00806330122427\n",
      "Iteration 416, the loss is 206.5008261935792, parameters k is 47.58299324199356 and b is -70.00808306407012\n",
      "Iteration 417, the loss is 206.10584013690655, parameters k is 47.520146898120046 and b is -70.00810282691597\n",
      "Iteration 418, the loss is 205.71085408023384, parameters k is 47.45730055424653 and b is -70.00812258976183\n",
      "Iteration 419, the loss is 205.31586802356128, parameters k is 47.39445421037301 and b is -70.00814235260768\n",
      "Iteration 420, the loss is 204.92088196688854, parameters k is 47.331607866499496 and b is -70.00816211545353\n",
      "Iteration 421, the loss is 204.52589591021592, parameters k is 47.26876152262598 and b is -70.00818187829938\n",
      "Iteration 422, the loss is 204.13090985354296, parameters k is 47.20591517875246 and b is -70.00820164114523\n",
      "Iteration 423, the loss is 203.73592379687034, parameters k is 47.143068834878946 and b is -70.00822140399109\n",
      "Iteration 424, the loss is 203.34093774019755, parameters k is 47.08022249100543 and b is -70.00824116683694\n",
      "Iteration 425, the loss is 202.9459516835249, parameters k is 47.01737614713191 and b is -70.00826092968279\n",
      "Iteration 426, the loss is 202.5509656268524, parameters k is 46.954529803258396 and b is -70.00828069252864\n",
      "Iteration 427, the loss is 202.15597957017982, parameters k is 46.89168345938488 and b is -70.0083004553745\n",
      "Iteration 428, the loss is 201.76099351350695, parameters k is 46.82883711551136 and b is -70.00832021822035\n",
      "Iteration 429, the loss is 201.36600745683444, parameters k is 46.765990771637846 and b is -70.0083399810662\n",
      "Iteration 430, the loss is 200.9710214001614, parameters k is 46.70314442776433 and b is -70.00835974391205\n",
      "Iteration 431, the loss is 200.5760353434889, parameters k is 46.64029808389081 and b is -70.0083795067579\n",
      "Iteration 432, the loss is 200.18104928681635, parameters k is 46.577451740017295 and b is -70.00839926960376\n",
      "Iteration 433, the loss is 199.78606323014364, parameters k is 46.51460539614378 and b is -70.00841903244961\n",
      "Iteration 434, the loss is 199.39107717347085, parameters k is 46.45175905227026 and b is -70.00843879529546\n",
      "Iteration 435, the loss is 198.99609111679808, parameters k is 46.388912708396745 and b is -70.00845855814131\n",
      "Iteration 436, the loss is 198.60110506012535, parameters k is 46.32606636452323 and b is -70.00847832098717\n",
      "Iteration 437, the loss is 198.20611900345241, parameters k is 46.26322002064971 and b is -70.00849808383302\n",
      "Iteration 438, the loss is 197.81113294678005, parameters k is 46.200373676776195 and b is -70.00851784667887\n",
      "Iteration 439, the loss is 197.41614689010734, parameters k is 46.13752733290268 and b is -70.00853760952472\n",
      "Iteration 440, the loss is 197.02116083343483, parameters k is 46.07468098902916 and b is -70.00855737237057\n",
      "Iteration 441, the loss is 196.626174776762, parameters k is 46.011834645155645 and b is -70.00857713521643\n",
      "Iteration 442, the loss is 196.23118872008942, parameters k is 45.94898830128213 and b is -70.00859689806228\n",
      "Iteration 443, the loss is 195.8362026634166, parameters k is 45.88614195740861 and b is -70.00861666090813\n",
      "Iteration 444, the loss is 195.4412166067438, parameters k is 45.823295613535095 and b is -70.00863642375398\n",
      "Iteration 445, the loss is 195.046230550071, parameters k is 45.76044926966158 and b is -70.00865618659984\n",
      "Iteration 446, the loss is 194.65124449339842, parameters k is 45.69760292578806 and b is -70.00867594944569\n",
      "Iteration 447, the loss is 194.25625843672597, parameters k is 45.634756581914544 and b is -70.00869571229154\n",
      "Iteration 448, the loss is 193.86127238005326, parameters k is 45.57191023804103 and b is -70.0087154751374\n",
      "Iteration 449, the loss is 193.4662863233805, parameters k is 45.50906389416751 and b is -70.00873523798325\n",
      "Iteration 450, the loss is 193.07130026670794, parameters k is 45.446217550293994 and b is -70.0087550008291\n",
      "Iteration 451, the loss is 192.67631421003483, parameters k is 45.38337120642048 and b is -70.00877476367495\n",
      "Iteration 452, the loss is 192.28132815336207, parameters k is 45.32052486254696 and b is -70.0087945265208\n",
      "Iteration 453, the loss is 191.88634209668982, parameters k is 45.257678518673444 and b is -70.00881428936665\n",
      "Iteration 454, the loss is 191.4913560400172, parameters k is 45.19483217479993 and b is -70.0088340522125\n",
      "Iteration 455, the loss is 191.09636998334406, parameters k is 45.13198583092641 and b is -70.00885381505836\n",
      "Iteration 456, the loss is 190.70138392667158, parameters k is 45.069139487052894 and b is -70.00887357790421\n",
      "Iteration 457, the loss is 190.30639786999896, parameters k is 45.00629314317938 and b is -70.00889334075006\n",
      "Iteration 458, the loss is 189.91141181332637, parameters k is 44.94344679930586 and b is -70.00891310359592\n",
      "Iteration 459, the loss is 189.51642575665358, parameters k is 44.880600455432344 and b is -70.00893286644177\n",
      "Iteration 460, the loss is 189.12143969998095, parameters k is 44.81775411155883 and b is -70.00895262928762\n",
      "Iteration 461, the loss is 188.72645364330822, parameters k is 44.75490776768531 and b is -70.00897239213347\n",
      "Iteration 462, the loss is 188.33146758663554, parameters k is 44.69206142381179 and b is -70.00899215497932\n",
      "Iteration 463, the loss is 187.93648152996278, parameters k is 44.62921507993828 and b is -70.00901191782518\n",
      "Iteration 464, the loss is 187.54149547329, parameters k is 44.56636873606476 and b is -70.00903168067103\n",
      "Iteration 465, the loss is 187.14650941661736, parameters k is 44.50352239219124 and b is -70.00905144351688\n",
      "Iteration 466, the loss is 186.75152335994468, parameters k is 44.44067604831773 and b is -70.00907120636273\n",
      "Iteration 467, the loss is 186.3565373032722, parameters k is 44.37782970444421 and b is -70.00909096920859\n",
      "Iteration 468, the loss is 185.9615512465993, parameters k is 44.31498336057069 and b is -70.00911073205444\n",
      "Iteration 469, the loss is 185.56656518992648, parameters k is 44.252137016697176 and b is -70.00913049490029\n",
      "Iteration 470, the loss is 185.171579133254, parameters k is 44.18929067282366 and b is -70.00915025774614\n",
      "Iteration 471, the loss is 184.7765930765812, parameters k is 44.12644432895014 and b is -70.009170020592\n",
      "Iteration 472, the loss is 184.38160701990844, parameters k is 44.063597985076626 and b is -70.00918978343785\n",
      "Iteration 473, the loss is 183.9866209632361, parameters k is 44.00075164120311 and b is -70.0092095462837\n",
      "Iteration 474, the loss is 183.5916349065634, parameters k is 43.93790529732959 and b is -70.00922930912955\n",
      "Iteration 475, the loss is 183.19664884989052, parameters k is 43.875058953456076 and b is -70.0092490719754\n",
      "Iteration 476, the loss is 182.80166279321784, parameters k is 43.81221260958256 and b is -70.00926883482126\n",
      "Iteration 477, the loss is 182.40667673654528, parameters k is 43.74936626570904 and b is -70.00928859766711\n",
      "Iteration 478, the loss is 182.01169067987257, parameters k is 43.686519921835526 and b is -70.00930836051296\n",
      "Iteration 479, the loss is 181.61670462319972, parameters k is 43.62367357796201 and b is -70.00932812335881\n",
      "Iteration 480, the loss is 181.2217185665271, parameters k is 43.56082723408849 and b is -70.00934788620467\n",
      "Iteration 481, the loss is 180.82673250985442, parameters k is 43.497980890214976 and b is -70.00936764905052\n",
      "Iteration 482, the loss is 180.43174645318157, parameters k is 43.43513454634146 and b is -70.00938741189637\n",
      "Iteration 483, the loss is 180.036760396509, parameters k is 43.37228820246794 and b is -70.00940717474222\n",
      "Iteration 484, the loss is 179.64177433983645, parameters k is 43.309441858594425 and b is -70.00942693758807\n",
      "Iteration 485, the loss is 179.24678828316357, parameters k is 43.24659551472091 and b is -70.00944670043393\n",
      "Iteration 486, the loss is 178.85180222649086, parameters k is 43.18374917084739 and b is -70.00946646327978\n",
      "Iteration 487, the loss is 178.45681616981824, parameters k is 43.120902826973875 and b is -70.00948622612563\n",
      "Iteration 488, the loss is 178.0618301131454, parameters k is 43.05805648310036 and b is -70.00950598897148\n",
      "Iteration 489, the loss is 177.6668440564729, parameters k is 42.99521013922684 and b is -70.00952575181734\n",
      "Iteration 490, the loss is 177.27185799980015, parameters k is 42.932363795353325 and b is -70.00954551466319\n",
      "Iteration 491, the loss is 176.87687194312755, parameters k is 42.86951745147981 and b is -70.00956527750904\n",
      "Iteration 492, the loss is 176.48188588645493, parameters k is 42.80667110760629 and b is -70.00958504035489\n",
      "Iteration 493, the loss is 176.08689982978203, parameters k is 42.743824763732775 and b is -70.00960480320074\n",
      "Iteration 494, the loss is 175.69191377310926, parameters k is 42.68097841985926 and b is -70.0096245660466\n",
      "Iteration 495, the loss is 175.29692771643667, parameters k is 42.61813207598574 and b is -70.00964432889245\n",
      "Iteration 496, the loss is 174.90194165976425, parameters k is 42.555285732112225 and b is -70.0096640917383\n",
      "Iteration 497, the loss is 174.50695560309146, parameters k is 42.49243938823871 and b is -70.00968385458415\n",
      "Iteration 498, the loss is 174.11196954641875, parameters k is 42.42959304436519 and b is -70.00970361743\n",
      "Iteration 499, the loss is 173.71698348974599, parameters k is 42.366746700491674 and b is -70.00972338027586\n",
      "Iteration 500, the loss is 173.3219974330734, parameters k is 42.30390035661816 and b is -70.00974314312171\n",
      "Iteration 501, the loss is 172.92701137640063, parameters k is 42.24105401274464 and b is -70.00976290596756\n",
      "Iteration 502, the loss is 172.5320253197278, parameters k is 42.178207668871124 and b is -70.00978266881341\n",
      "Iteration 503, the loss is 172.13703926305527, parameters k is 42.11536132499761 and b is -70.00980243165927\n",
      "Iteration 504, the loss is 171.7420532063826, parameters k is 42.05251498112409 and b is -70.00982219450512\n",
      "Iteration 505, the loss is 171.34706714970986, parameters k is 41.989668637250574 and b is -70.00984195735097\n",
      "Iteration 506, the loss is 170.9520810930372, parameters k is 41.92682229337706 and b is -70.00986172019682\n",
      "Iteration 507, the loss is 170.5570950363644, parameters k is 41.86397594950354 and b is -70.00988148304268\n",
      "Iteration 508, the loss is 170.16210897969182, parameters k is 41.801129605630024 and b is -70.00990124588853\n",
      "Iteration 509, the loss is 169.76712292301883, parameters k is 41.73828326175651 and b is -70.00992100873438\n",
      "Iteration 510, the loss is 169.37213686634644, parameters k is 41.67543691788299 and b is -70.00994077158023\n",
      "Iteration 511, the loss is 168.97715080967362, parameters k is 41.612590574009474 and b is -70.00996053442609\n",
      "Iteration 512, the loss is 168.58216475300114, parameters k is 41.54974423013596 and b is -70.00998029727194\n",
      "Iteration 513, the loss is 168.1871786963282, parameters k is 41.48689788626244 and b is -70.01000006011779\n",
      "Iteration 514, the loss is 167.7921926396555, parameters k is 41.42405154238892 and b is -70.01001982296364\n",
      "Iteration 515, the loss is 167.39720658298305, parameters k is 41.36120519851541 and b is -70.0100395858095\n",
      "Iteration 516, the loss is 167.0022205263103, parameters k is 41.29835885464189 and b is -70.01005934865535\n",
      "Iteration 517, the loss is 166.60723446963735, parameters k is 41.23551251076837 and b is -70.0100791115012\n",
      "Iteration 518, the loss is 166.21224841296495, parameters k is 41.17266616689486 and b is -70.01009887434705\n",
      "Iteration 519, the loss is 165.8172623562921, parameters k is 41.10981982302134 and b is -70.0101186371929\n",
      "Iteration 520, the loss is 165.4222762996195, parameters k is 41.04697347914782 and b is -70.01013840003876\n",
      "Iteration 521, the loss is 165.02729024294666, parameters k is 40.984127135274306 and b is -70.01015816288461\n",
      "Iteration 522, the loss is 164.63230418627415, parameters k is 40.92128079140079 and b is -70.01017792573046\n",
      "Iteration 523, the loss is 164.23731812960142, parameters k is 40.85843444752727 and b is -70.01019768857631\n",
      "Iteration 524, the loss is 163.84233207292877, parameters k is 40.795588103653756 and b is -70.01021745142216\n",
      "Iteration 525, the loss is 163.4473460162558, parameters k is 40.73274175978024 and b is -70.01023721426802\n",
      "Iteration 526, the loss is 163.0523599595834, parameters k is 40.66989541590672 and b is -70.01025697711387\n",
      "Iteration 527, the loss is 162.65737390291054, parameters k is 40.607049072033206 and b is -70.01027673995972\n",
      "Iteration 528, the loss is 162.26238784623797, parameters k is 40.54420272815969 and b is -70.01029650280557\n",
      "Iteration 529, the loss is 161.8674017895651, parameters k is 40.48135638428617 and b is -70.01031626565143\n",
      "Iteration 530, the loss is 161.47241573289276, parameters k is 40.418510040412656 and b is -70.01033602849728\n",
      "Iteration 531, the loss is 161.0774296762199, parameters k is 40.35566369653914 and b is -70.01035579134313\n",
      "Iteration 532, the loss is 160.68244361954703, parameters k is 40.29281735266562 and b is -70.01037555418898\n",
      "Iteration 533, the loss is 160.28745756287455, parameters k is 40.229971008792106 and b is -70.01039531703483\n",
      "Iteration 534, the loss is 159.89247150620196, parameters k is 40.16712466491859 and b is -70.01041507988069\n",
      "Iteration 535, the loss is 159.49748544952917, parameters k is 40.10427832104507 and b is -70.01043484272654\n",
      "Iteration 536, the loss is 159.1024993928564, parameters k is 40.041431977171555 and b is -70.01045460557239\n",
      "Iteration 537, the loss is 158.7075133361839, parameters k is 39.97858563329804 and b is -70.01047436841824\n",
      "Iteration 538, the loss is 158.31252727951113, parameters k is 39.91573928942452 and b is -70.0104941312641\n",
      "Iteration 539, the loss is 157.91754122283828, parameters k is 39.852892945551005 and b is -70.01051389410995\n",
      "Iteration 540, the loss is 157.5225551661654, parameters k is 39.79004660167749 and b is -70.0105336569558\n",
      "Iteration 541, the loss is 157.12756910949298, parameters k is 39.72720025780397 and b is -70.01055341980165\n",
      "Iteration 542, the loss is 156.7325830528204, parameters k is 39.664353913930455 and b is -70.0105731826475\n",
      "Iteration 543, the loss is 156.33759699614757, parameters k is 39.60150757005694 and b is -70.01059294549336\n",
      "Iteration 544, the loss is 155.9426109394748, parameters k is 39.53866122618342 and b is -70.01061270833921\n",
      "Iteration 545, the loss is 155.5476248828022, parameters k is 39.475814882309905 and b is -70.01063247118506\n",
      "Iteration 546, the loss is 155.1526388261297, parameters k is 39.41296853843639 and b is -70.01065223403091\n",
      "Iteration 547, the loss is 154.75765276945688, parameters k is 39.35012219456287 and b is -70.01067199687677\n",
      "Iteration 548, the loss is 154.36266671278418, parameters k is 39.287275850689355 and b is -70.01069175972262\n",
      "Iteration 549, the loss is 153.9676806561114, parameters k is 39.22442950681584 and b is -70.01071152256847\n",
      "Iteration 550, the loss is 153.5726945994386, parameters k is 39.16158316294232 and b is -70.01073128541432\n",
      "Iteration 551, the loss is 153.17770854276606, parameters k is 39.098736819068804 and b is -70.01075104826018\n",
      "Iteration 552, the loss is 152.78272248609343, parameters k is 39.03589047519529 and b is -70.01077081110603\n",
      "Iteration 553, the loss is 152.38773642942073, parameters k is 38.97304413132177 and b is -70.01079057395188\n",
      "Iteration 554, the loss is 151.992750372748, parameters k is 38.910197787448254 and b is -70.01081033679773\n",
      "Iteration 555, the loss is 151.5977643160752, parameters k is 38.84735144357474 and b is -70.01083009964358\n",
      "Iteration 556, the loss is 151.20277825940252, parameters k is 38.78450509970122 and b is -70.01084986248944\n",
      "Iteration 557, the loss is 150.80779220273, parameters k is 38.721658755827704 and b is -70.01086962533529\n",
      "Iteration 558, the loss is 150.41280614605722, parameters k is 38.65881241195419 and b is -70.01088938818114\n",
      "Iteration 559, the loss is 150.01782008938466, parameters k is 38.59596606808067 and b is -70.010909151027\n",
      "Iteration 560, the loss is 149.62283403271184, parameters k is 38.533119724207154 and b is -70.01092891387285\n",
      "Iteration 561, the loss is 149.2278479760391, parameters k is 38.47027338033364 and b is -70.0109486767187\n",
      "Iteration 562, the loss is 148.83286191936645, parameters k is 38.40742703646012 and b is -70.01096843956455\n",
      "Iteration 563, the loss is 148.43787586269372, parameters k is 38.344580692586604 and b is -70.0109882024104\n",
      "Iteration 564, the loss is 148.04288980602115, parameters k is 38.28173434871309 and b is -70.01100796525625\n",
      "Iteration 565, the loss is 147.6479037493484, parameters k is 38.21888800483957 and b is -70.0110277281021\n",
      "Iteration 566, the loss is 147.25291769267565, parameters k is 38.15604166096605 and b is -70.01104749094796\n",
      "Iteration 567, the loss is 146.85793163600306, parameters k is 38.09319531709254 and b is -70.01106725379381\n",
      "Iteration 568, the loss is 146.46294557933044, parameters k is 38.03034897321902 and b is -70.01108701663966\n",
      "Iteration 569, the loss is 146.06795952265782, parameters k is 37.9675026293455 and b is -70.01110677948552\n",
      "Iteration 570, the loss is 145.67297346598485, parameters k is 37.90465628547199 and b is -70.01112654233137\n",
      "Iteration 571, the loss is 145.27798740931223, parameters k is 37.84180994159847 and b is -70.01114630517722\n",
      "Iteration 572, the loss is 144.88300135263947, parameters k is 37.77896359772495 and b is -70.01116606802307\n",
      "Iteration 573, the loss is 144.48801529596696, parameters k is 37.716117253851436 and b is -70.01118583086892\n",
      "Iteration 574, the loss is 144.0930292392942, parameters k is 37.65327090997792 and b is -70.01120559371478\n",
      "Iteration 575, the loss is 143.6980431826215, parameters k is 37.5904245661044 and b is -70.01122535656063\n",
      "Iteration 576, the loss is 143.3030571259489, parameters k is 37.527578222230886 and b is -70.01124511940648\n",
      "Iteration 577, the loss is 142.9080710692762, parameters k is 37.46473187835737 and b is -70.01126488225233\n",
      "Iteration 578, the loss is 142.51308501260334, parameters k is 37.40188553448385 and b is -70.01128464509819\n",
      "Iteration 579, the loss is 142.11809895593075, parameters k is 37.339039190610336 and b is -70.01130440794404\n",
      "Iteration 580, the loss is 141.72311289925804, parameters k is 37.27619284673682 and b is -70.01132417078989\n",
      "Iteration 581, the loss is 141.32812684258528, parameters k is 37.2133465028633 and b is -70.01134393363574\n",
      "Iteration 582, the loss is 140.93314078591263, parameters k is 37.150500158989786 and b is -70.0113636964816\n",
      "Iteration 583, the loss is 140.53815472924, parameters k is 37.08765381511627 and b is -70.01138345932745\n",
      "Iteration 584, the loss is 140.14316867256716, parameters k is 37.02480747124275 and b is -70.0114032221733\n",
      "Iteration 585, the loss is 139.7481826158945, parameters k is 36.961961127369236 and b is -70.01142298501915\n",
      "Iteration 586, the loss is 139.35319655922194, parameters k is 36.89911478349572 and b is -70.011442747865\n",
      "Iteration 587, the loss is 138.9582105025493, parameters k is 36.8362684396222 and b is -70.01146251071086\n",
      "Iteration 588, the loss is 138.56322444587653, parameters k is 36.773422095748685 and b is -70.01148227355671\n",
      "Iteration 589, the loss is 138.16823838920388, parameters k is 36.71057575187517 and b is -70.01150203640256\n",
      "Iteration 590, the loss is 137.77325233253097, parameters k is 36.64772940800165 and b is -70.01152179924841\n",
      "Iteration 591, the loss is 137.37826627585844, parameters k is 36.584883064128135 and b is -70.01154156209427\n",
      "Iteration 592, the loss is 136.98328021918576, parameters k is 36.52203672025462 and b is -70.01156132494012\n",
      "Iteration 593, the loss is 136.58829416251305, parameters k is 36.4591903763811 and b is -70.01158108778597\n",
      "Iteration 594, the loss is 136.19330810584043, parameters k is 36.396344032507585 and b is -70.01160085063182\n",
      "Iteration 595, the loss is 135.79832204916767, parameters k is 36.33349768863407 and b is -70.01162061347767\n",
      "Iteration 596, the loss is 135.40333599249502, parameters k is 36.27065134476055 and b is -70.01164037632353\n",
      "Iteration 597, the loss is 135.0083499358223, parameters k is 36.207805000887035 and b is -70.01166013916938\n",
      "Iteration 598, the loss is 134.61336387914952, parameters k is 36.14495865701352 and b is -70.01167990201523\n",
      "Iteration 599, the loss is 134.21837782247692, parameters k is 36.08211231314 and b is -70.01169966486108\n",
      "Iteration 600, the loss is 133.82339176580416, parameters k is 36.019265969266485 and b is -70.01171942770694\n",
      "Iteration 601, the loss is 133.4284057091315, parameters k is 35.95641962539297 and b is -70.01173919055279\n",
      "Iteration 602, the loss is 133.03341965245897, parameters k is 35.89357328151945 and b is -70.01175895339864\n",
      "Iteration 603, the loss is 132.6384335957862, parameters k is 35.830726937645935 and b is -70.01177871624449\n",
      "Iteration 604, the loss is 132.24344753911353, parameters k is 35.76788059377242 and b is -70.01179847909034\n",
      "Iteration 605, the loss is 131.84846148244074, parameters k is 35.7050342498989 and b is -70.0118182419362\n",
      "Iteration 606, the loss is 131.45347542576812, parameters k is 35.642187906025384 and b is -70.01183800478205\n",
      "Iteration 607, the loss is 131.05848936909535, parameters k is 35.57934156215187 and b is -70.0118577676279\n",
      "Iteration 608, the loss is 130.66350331242276, parameters k is 35.51649521827835 and b is -70.01187753047375\n",
      "Iteration 609, the loss is 130.26851725575008, parameters k is 35.453648874404834 and b is -70.0118972933196\n",
      "Iteration 610, the loss is 129.87353119907735, parameters k is 35.39080253053132 and b is -70.01191705616546\n",
      "Iteration 611, the loss is 129.47854514240456, parameters k is 35.3279561866578 and b is -70.01193681901131\n",
      "Iteration 612, the loss is 129.083559085732, parameters k is 35.265109842784284 and b is -70.01195658185716\n",
      "Iteration 613, the loss is 128.6885730290592, parameters k is 35.20226349891077 and b is -70.01197634470302\n",
      "Iteration 614, the loss is 128.29358697238678, parameters k is 35.13941715503725 and b is -70.01199610754887\n",
      "Iteration 615, the loss is 127.898600915714, parameters k is 35.076570811163734 and b is -70.01201587039472\n",
      "Iteration 616, the loss is 127.50361485904105, parameters k is 35.01372446729022 and b is -70.01203563324057\n",
      "Iteration 617, the loss is 127.10862880236859, parameters k is 34.9508781234167 and b is -70.01205539608642\n",
      "Iteration 618, the loss is 126.71364274569571, parameters k is 34.888031779543184 and b is -70.01207515893228\n",
      "Iteration 619, the loss is 126.31865668902317, parameters k is 34.82518543566967 and b is -70.01209492177813\n",
      "Iteration 620, the loss is 125.92367063235046, parameters k is 34.76233909179615 and b is -70.01211468462398\n",
      "Iteration 621, the loss is 125.52868457567776, parameters k is 34.69949274792263 and b is -70.01213444746983\n",
      "Iteration 622, the loss is 125.13369851900511, parameters k is 34.63664640404912 and b is -70.01215421031569\n",
      "Iteration 623, the loss is 124.73871246233243, parameters k is 34.5738000601756 and b is -70.01217397316154\n",
      "Iteration 624, the loss is 124.34372640565972, parameters k is 34.51095371630208 and b is -70.01219373600739\n",
      "Iteration 625, the loss is 123.948740348987, parameters k is 34.44810737242857 and b is -70.01221349885324\n",
      "Iteration 626, the loss is 123.55375429231417, parameters k is 34.38526102855505 and b is -70.0122332616991\n",
      "Iteration 627, the loss is 123.1587682356417, parameters k is 34.32241468468153 and b is -70.01225302454495\n",
      "Iteration 628, the loss is 122.76378217896892, parameters k is 34.259568340808016 and b is -70.0122727873908\n",
      "Iteration 629, the loss is 122.36879612229632, parameters k is 34.1967219969345 and b is -70.01229255023665\n",
      "Iteration 630, the loss is 121.9738100656235, parameters k is 34.13387565306098 and b is -70.0123123130825\n",
      "Iteration 631, the loss is 121.57882400895086, parameters k is 34.071029309187466 and b is -70.01233207592836\n",
      "Iteration 632, the loss is 121.1838379522781, parameters k is 34.00818296531395 and b is -70.01235183877421\n",
      "Iteration 633, the loss is 120.78885189560536, parameters k is 33.94533662144043 and b is -70.01237160162006\n",
      "Iteration 634, the loss is 120.39386583893275, parameters k is 33.882490277566916 and b is -70.01239136446591\n",
      "Iteration 635, the loss is 119.99887978226003, parameters k is 33.8196439336934 and b is -70.01241112731176\n",
      "Iteration 636, the loss is 119.60389372558731, parameters k is 33.75679758981988 and b is -70.01243089015762\n",
      "Iteration 637, the loss is 119.20890766891478, parameters k is 33.693951245946366 and b is -70.01245065300347\n",
      "Iteration 638, the loss is 118.81392161224193, parameters k is 33.63110490207285 and b is -70.01247041584932\n",
      "Iteration 639, the loss is 118.41893555556926, parameters k is 33.56825855819933 and b is -70.01249017869517\n",
      "Iteration 640, the loss is 118.02394949889651, parameters k is 33.505412214325816 and b is -70.01250994154103\n",
      "Iteration 641, the loss is 117.62896344222393, parameters k is 33.4425658704523 and b is -70.01252970438688\n",
      "Iteration 642, the loss is 117.2339773855512, parameters k is 33.37971952657878 and b is -70.01254946723273\n",
      "Iteration 643, the loss is 116.83899132887859, parameters k is 33.316873182705265 and b is -70.01256923007858\n",
      "Iteration 644, the loss is 116.44400527220586, parameters k is 33.25402683883175 and b is -70.01258899292444\n",
      "Iteration 645, the loss is 116.04901921553305, parameters k is 33.19118049495823 and b is -70.01260875577029\n",
      "Iteration 646, the loss is 115.65403315886044, parameters k is 33.128334151084715 and b is -70.01262851861614\n",
      "Iteration 647, the loss is 115.25904710218774, parameters k is 33.0654878072112 and b is -70.01264828146199\n",
      "Iteration 648, the loss is 114.86406104551516, parameters k is 33.00264146333768 and b is -70.01266804430784\n",
      "Iteration 649, the loss is 114.46907498884238, parameters k is 32.939795119464165 and b is -70.0126878071537\n",
      "Iteration 650, the loss is 114.07408893216979, parameters k is 32.87694877559065 and b is -70.01270756999955\n",
      "Iteration 651, the loss is 113.67910287549711, parameters k is 32.81410243171713 and b is -70.0127273328454\n",
      "Iteration 652, the loss is 113.2841168188243, parameters k is 32.751256087843615 and b is -70.01274709569125\n",
      "Iteration 653, the loss is 112.88913076215164, parameters k is 32.6884097439701 and b is -70.0127668585371\n",
      "Iteration 654, the loss is 112.49414470547903, parameters k is 32.62556340009658 and b is -70.01278662138296\n",
      "Iteration 655, the loss is 112.09915864880617, parameters k is 32.562717056223065 and b is -70.01280638422881\n",
      "Iteration 656, the loss is 111.70417259213357, parameters k is 32.49987071234955 and b is -70.01282614707466\n",
      "Iteration 657, the loss is 111.30918653546084, parameters k is 32.43702436847603 and b is -70.01284590992051\n",
      "Iteration 658, the loss is 110.91420047878817, parameters k is 32.374178024602514 and b is -70.01286567276637\n",
      "Iteration 659, the loss is 110.5192144221156, parameters k is 32.311331680729 and b is -70.01288543561222\n",
      "Iteration 660, the loss is 110.12422836544285, parameters k is 32.24848533685548 and b is -70.01290519845807\n",
      "Iteration 661, the loss is 109.72924230877018, parameters k is 32.185638992981964 and b is -70.01292496130392\n",
      "Iteration 662, the loss is 109.3342562520974, parameters k is 32.12279264910845 and b is -70.01294472414978\n",
      "Iteration 663, the loss is 108.93927019542485, parameters k is 32.05994630523493 and b is -70.01296448699563\n",
      "Iteration 664, the loss is 108.54428413875205, parameters k is 31.997099961361414 and b is -70.01298424984148\n",
      "Iteration 665, the loss is 108.1492980820794, parameters k is 31.934253617487897 and b is -70.01300401268733\n",
      "Iteration 666, the loss is 107.75431202540668, parameters k is 31.87140727361438 and b is -70.01302377553318\n",
      "Iteration 667, the loss is 107.35932596873397, parameters k is 31.808560929740864 and b is -70.01304353837904\n",
      "Iteration 668, the loss is 106.96433991206128, parameters k is 31.745714585867347 and b is -70.01306330122489\n",
      "Iteration 669, the loss is 106.56935385538857, parameters k is 31.68286824199383 and b is -70.01308306407074\n",
      "Iteration 670, the loss is 106.17436779871593, parameters k is 31.620021898120314 and b is -70.0131028269166\n",
      "Iteration 671, the loss is 105.77938174204323, parameters k is 31.557175554246797 and b is -70.01312258976245\n",
      "Iteration 672, the loss is 105.38439568537062, parameters k is 31.49432921037328 and b is -70.0131423526083\n",
      "Iteration 673, the loss is 104.98940962869796, parameters k is 31.431482866499763 and b is -70.01316211545415\n",
      "Iteration 674, the loss is 104.59442357202529, parameters k is 31.368636522626247 and b is -70.0131818783\n",
      "Iteration 675, the loss is 104.19943751535243, parameters k is 31.30579017875273 and b is -70.01320164114586\n",
      "Iteration 676, the loss is 103.80445145867985, parameters k is 31.242943834879213 and b is -70.01322140399171\n",
      "Iteration 677, the loss is 103.40946540200707, parameters k is 31.180097491005696 and b is -70.01324116683756\n",
      "Iteration 678, the loss is 103.01447934533442, parameters k is 31.11725114713218 and b is -70.01326092968341\n",
      "Iteration 679, the loss is 102.61949328866163, parameters k is 31.054404803258663 and b is -70.01328069252926\n",
      "Iteration 680, the loss is 102.22450723198905, parameters k is 30.991558459385146 and b is -70.01330045537512\n",
      "Iteration 681, the loss is 101.82952117531627, parameters k is 30.92871211551163 and b is -70.01332021822097\n",
      "Iteration 682, the loss is 101.43453511864357, parameters k is 30.865865771638113 and b is -70.01333998106682\n",
      "Iteration 683, the loss is 101.0395490619709, parameters k is 30.803019427764596 and b is -70.01335974391267\n",
      "Iteration 684, the loss is 100.64456300529822, parameters k is 30.74017308389108 and b is -70.01337950675853\n",
      "Iteration 685, the loss is 100.2495769486256, parameters k is 30.677326740017563 and b is -70.01339926960438\n",
      "Iteration 686, the loss is 99.85459089195287, parameters k is 30.614480396144046 and b is -70.01341903245023\n",
      "Iteration 687, the loss is 99.45960483528019, parameters k is 30.55163405227053 and b is -70.01343879529608\n",
      "Iteration 688, the loss is 99.06461877860752, parameters k is 30.488787708397012 and b is -70.01345855814193\n",
      "Iteration 689, the loss is 98.66963272193485, parameters k is 30.425941364523496 and b is -70.01347832098779\n",
      "Iteration 690, the loss is 98.27464666526203, parameters k is 30.36309502064998 and b is -70.01349808383364\n",
      "Iteration 691, the loss is 97.8796606085894, parameters k is 30.300248676776462 and b is -70.01351784667949\n",
      "Iteration 692, the loss is 97.48467455191673, parameters k is 30.237402332902946 and b is -70.01353760952534\n",
      "Iteration 693, the loss is 97.08968849524412, parameters k is 30.17455598902943 and b is -70.0135573723712\n",
      "Iteration 694, the loss is 96.69470243857147, parameters k is 30.111709645155912 and b is -70.01357713521705\n",
      "Iteration 695, the loss is 96.29971638189868, parameters k is 30.048863301282395 and b is -70.0135968980629\n",
      "Iteration 696, the loss is 95.90473032522597, parameters k is 29.98601695740888 and b is -70.01361666090875\n",
      "Iteration 697, the loss is 95.50974426855319, parameters k is 29.923170613535362 and b is -70.0136364237546\n",
      "Iteration 698, the loss is 95.11475821188054, parameters k is 29.860324269661845 and b is -70.01365618660046\n",
      "Iteration 699, the loss is 94.71977215520788, parameters k is 29.79747792578833 and b is -70.01367594944631\n",
      "Iteration 700, the loss is 94.32478609853521, parameters k is 29.73463158191481 and b is -70.01369571229216\n",
      "Iteration 701, the loss is 93.92980004186244, parameters k is 29.671785238041295 and b is -70.01371547513801\n",
      "Iteration 702, the loss is 93.53481398518986, parameters k is 29.60893889416778 and b is -70.01373523798387\n",
      "Iteration 703, the loss is 93.13982792851716, parameters k is 29.54609255029426 and b is -70.01375500082972\n",
      "Iteration 704, the loss is 92.74484187184444, parameters k is 29.483246206420745 and b is -70.01377476367557\n",
      "Iteration 705, the loss is 92.34985581517176, parameters k is 29.420399862547228 and b is -70.01379452652142\n",
      "Iteration 706, the loss is 91.95486975849902, parameters k is 29.35755351867371 and b is -70.01381428936728\n",
      "Iteration 707, the loss is 91.55988370182644, parameters k is 29.294707174800195 and b is -70.01383405221313\n",
      "Iteration 708, the loss is 91.16489764515372, parameters k is 29.231860830926678 and b is -70.01385381505898\n",
      "Iteration 709, the loss is 90.76991158848118, parameters k is 29.16901448705316 and b is -70.01387357790483\n",
      "Iteration 710, the loss is 90.37492553180843, parameters k is 29.106168143179644 and b is -70.01389334075068\n",
      "Iteration 711, the loss is 89.97993947513561, parameters k is 29.043321799306128 and b is -70.01391310359654\n",
      "Iteration 712, the loss is 89.58495341846293, parameters k is 28.98047545543261 and b is -70.01393286644239\n",
      "Iteration 713, the loss is 89.18996736179024, parameters k is 28.917629111559094 and b is -70.01395262928824\n",
      "Iteration 714, the loss is 88.79498130511753, parameters k is 28.854782767685577 and b is -70.0139723921341\n",
      "Iteration 715, the loss is 88.39999524844482, parameters k is 28.79193642381206 and b is -70.01399215497995\n",
      "Iteration 716, the loss is 88.00500919177226, parameters k is 28.729090079938544 and b is -70.0140119178258\n",
      "Iteration 717, the loss is 87.61002313509948, parameters k is 28.666243736065027 and b is -70.01403168067165\n",
      "Iteration 718, the loss is 87.21503707842679, parameters k is 28.60339739219151 and b is -70.0140514435175\n",
      "Iteration 719, the loss is 86.82005102175401, parameters k is 28.540551048317994 and b is -70.01407120636335\n",
      "Iteration 720, the loss is 86.42506496508133, parameters k is 28.477704704444477 and b is -70.0140909692092\n",
      "Iteration 721, the loss is 86.03007890840877, parameters k is 28.41485836057096 and b is -70.01411073205506\n",
      "Iteration 722, the loss is 85.6350928517361, parameters k is 28.352012016697444 and b is -70.01413049490091\n",
      "Iteration 723, the loss is 85.24010679506344, parameters k is 28.289165672823927 and b is -70.01415025774676\n",
      "Iteration 724, the loss is 84.8451207383906, parameters k is 28.22631932895041 and b is -70.01417002059262\n",
      "Iteration 725, the loss is 84.45013468171796, parameters k is 28.163472985076893 and b is -70.01418978343847\n",
      "Iteration 726, the loss is 84.05514862504515, parameters k is 28.100626641203377 and b is -70.01420954628432\n",
      "Iteration 727, the loss is 83.66016256837253, parameters k is 28.03778029732986 and b is -70.01422930913017\n",
      "Iteration 728, the loss is 83.26517651169985, parameters k is 27.974933953456343 and b is -70.01424907197602\n",
      "Iteration 729, the loss is 82.87019045502728, parameters k is 27.912087609582827 and b is -70.01426883482188\n",
      "Iteration 730, the loss is 82.47520439835452, parameters k is 27.84924126570931 and b is -70.01428859766773\n",
      "Iteration 731, the loss is 82.08021834168179, parameters k is 27.786394921835793 and b is -70.01430836051358\n",
      "Iteration 732, the loss is 81.6852322850092, parameters k is 27.723548577962276 and b is -70.01432812335943\n",
      "Iteration 733, the loss is 81.29024622833649, parameters k is 27.66070223408876 and b is -70.01434788620529\n",
      "Iteration 734, the loss is 80.89526017166376, parameters k is 27.597855890215243 and b is -70.01436764905114\n",
      "Iteration 735, the loss is 80.50027411499106, parameters k is 27.535009546341726 and b is -70.01438741189699\n",
      "Iteration 736, the loss is 80.10528805831842, parameters k is 27.47216320246821 and b is -70.01440717474284\n",
      "Iteration 737, the loss is 79.71030200164574, parameters k is 27.409316858594693 and b is -70.0144269375887\n",
      "Iteration 738, the loss is 79.31584426591176, parameters k is 27.346470514721176 and b is -70.01444670043455\n",
      "Iteration 739, the loss is 78.92262544326931, parameters k is 27.2837649218358 and b is -70.0144664632804\n",
      "Iteration 740, the loss is 78.52940662062683, parameters k is 27.221059328950425 and b is -70.01448622612625\n",
      "Iteration 741, the loss is 78.1361877979843, parameters k is 27.15835373606505 and b is -70.0145059889721\n",
      "Iteration 742, the loss is 77.74296897534177, parameters k is 27.095648143179673 and b is -70.01452575181796\n",
      "Iteration 743, the loss is 77.3497501526993, parameters k is 27.032942550294297 and b is -70.01454551466381\n",
      "Iteration 744, the loss is 76.9565313300569, parameters k is 26.970236957408922 and b is -70.01456527750966\n",
      "Iteration 745, the loss is 76.56331250741437, parameters k is 26.907531364523546 and b is -70.01458504035551\n",
      "Iteration 746, the loss is 76.17009368477186, parameters k is 26.84482577163817 and b is -70.01460480320137\n",
      "Iteration 747, the loss is 75.7768748621294, parameters k is 26.782120178752795 and b is -70.01462456604722\n",
      "Iteration 748, the loss is 75.3836560394869, parameters k is 26.71941458586742 and b is -70.01464432889307\n",
      "Iteration 749, the loss is 74.99043721684438, parameters k is 26.656708992982043 and b is -70.01466409173892\n",
      "Iteration 750, the loss is 74.5972183942019, parameters k is 26.594003400096668 and b is -70.01468385458477\n",
      "Iteration 751, the loss is 74.20399957155949, parameters k is 26.531297807211292 and b is -70.01470361743063\n",
      "Iteration 752, the loss is 73.81078074891698, parameters k is 26.468592214325916 and b is -70.01472338027648\n",
      "Iteration 753, the loss is 73.41756192627454, parameters k is 26.40588662144054 and b is -70.01474314312233\n",
      "Iteration 754, the loss is 73.02434310363203, parameters k is 26.343181028555165 and b is -70.01476290596818\n",
      "Iteration 755, the loss is 72.63112428098952, parameters k is 26.28047543566979 and b is -70.01478266881404\n",
      "Iteration 756, the loss is 72.23790545834697, parameters k is 26.217769842784413 and b is -70.01480243165989\n",
      "Iteration 757, the loss is 71.84468663570456, parameters k is 26.155064249899038 and b is -70.01482219450574\n",
      "Iteration 758, the loss is 71.45146781306208, parameters k is 26.092358657013662 and b is -70.01484195735159\n",
      "Iteration 759, the loss is 71.05824899041966, parameters k is 26.029653064128286 and b is -70.01486172019744\n",
      "Iteration 760, the loss is 70.66503016777712, parameters k is 25.96694747124291 and b is -70.0148814830433\n",
      "Iteration 761, the loss is 70.27181134513464, parameters k is 25.904241878357535 and b is -70.01490124588915\n",
      "Iteration 762, the loss is 69.87859252249216, parameters k is 25.84153628547216 and b is -70.014921008735\n",
      "Iteration 763, the loss is 69.48537369984966, parameters k is 25.778830692586784 and b is -70.01494077158085\n",
      "Iteration 764, the loss is 69.0921548772072, parameters k is 25.716125099701408 and b is -70.0149605344267\n",
      "Iteration 765, the loss is 68.69893605456467, parameters k is 25.653419506816032 and b is -70.01498029727256\n",
      "Iteration 766, the loss is 68.30571723192226, parameters k is 25.590713913930657 and b is -70.01500006011841\n",
      "Iteration 767, the loss is 67.9124984092797, parameters k is 25.52800832104528 and b is -70.01501982296426\n",
      "Iteration 768, the loss is 67.51927958663721, parameters k is 25.465302728159905 and b is -70.01503958581011\n",
      "Iteration 769, the loss is 67.1260607639948, parameters k is 25.40259713527453 and b is -70.01505934865597\n",
      "Iteration 770, the loss is 66.73284194135228, parameters k is 25.339891542389154 and b is -70.01507911150182\n",
      "Iteration 771, the loss is 66.33962311870987, parameters k is 25.277185949503778 and b is -70.01509887434767\n",
      "Iteration 772, the loss is 65.94640429606724, parameters k is 25.214480356618402 and b is -70.01511863719352\n",
      "Iteration 773, the loss is 65.5531854734248, parameters k is 25.151774763733027 and b is -70.01513840003938\n",
      "Iteration 774, the loss is 65.15996665078232, parameters k is 25.08906917084765 and b is -70.01515816288523\n",
      "Iteration 775, the loss is 64.76674782813987, parameters k is 25.026363577962275 and b is -70.01517792573108\n",
      "Iteration 776, the loss is 64.37352900549742, parameters k is 24.9636579850769 and b is -70.01519768857693\n",
      "Iteration 777, the loss is 63.980310182854964, parameters k is 24.900952392191524 and b is -70.01521745142279\n",
      "Iteration 778, the loss is 63.58709136021248, parameters k is 24.838246799306148 and b is -70.01523721426864\n",
      "Iteration 779, the loss is 63.193872537569945, parameters k is 24.775541206420773 and b is -70.01525697711449\n",
      "Iteration 780, the loss is 62.80065371492744, parameters k is 24.712835613535397 and b is -70.01527673996034\n",
      "Iteration 781, the loss is 62.40743489228494, parameters k is 24.65013002065002 and b is -70.0152965028062\n",
      "Iteration 782, the loss is 62.01421606964252, parameters k is 24.587424427764645 and b is -70.01531626565205\n",
      "Iteration 783, the loss is 61.620997247000005, parameters k is 24.52471883487927 and b is -70.0153360284979\n",
      "Iteration 784, the loss is 61.22777842435756, parameters k is 24.462013241993894 and b is -70.01535579134375\n",
      "Iteration 785, the loss is 60.834559601715, parameters k is 24.39930764910852 and b is -70.0153755541896\n",
      "Iteration 786, the loss is 60.441340779072526, parameters k is 24.336602056223143 and b is -70.01539531703546\n",
      "Iteration 787, the loss is 60.04812195643007, parameters k is 24.273896463337767 and b is -70.01541507988131\n",
      "Iteration 788, the loss is 59.654903133787634, parameters k is 24.21119087045239 and b is -70.01543484272716\n",
      "Iteration 789, the loss is 59.261684311145125, parameters k is 24.148485277567016 and b is -70.01545460557301\n",
      "Iteration 790, the loss is 58.86997243230515, parameters k is 24.08577968468164 and b is -70.01547436841886\n",
      "Iteration 791, the loss is 58.48112006286614, parameters k is 24.023423222231045 and b is -70.01549413126472\n",
      "Iteration 792, the loss is 58.09226769342704, parameters k is 23.96106675978045 and b is -70.01551389411057\n",
      "Iteration 793, the loss is 57.70341532398799, parameters k is 23.898710297329856 and b is -70.01553365695642\n",
      "Iteration 794, the loss is 57.31456295454895, parameters k is 23.83635383487926 and b is -70.01555341980227\n",
      "Iteration 795, the loss is 56.92571058511, parameters k is 23.773997372428667 and b is -70.01557318264813\n",
      "Iteration 796, the loss is 56.53685821567093, parameters k is 23.711640909978073 and b is -70.01559294549398\n",
      "Iteration 797, the loss is 56.14800584623186, parameters k is 23.649284447527478 and b is -70.01561270833983\n",
      "Iteration 798, the loss is 55.759153476792875, parameters k is 23.586927985076883 and b is -70.01563247118568\n",
      "Iteration 799, the loss is 55.37030110735385, parameters k is 23.52457152262629 and b is -70.01565223403153\n",
      "Iteration 800, the loss is 54.98144873791483, parameters k is 23.462215060175694 and b is -70.01567199687739\n",
      "Iteration 801, the loss is 54.59259636847576, parameters k is 23.3998585977251 and b is -70.01569175972324\n",
      "Iteration 802, the loss is 54.20374399903673, parameters k is 23.337502135274505 and b is -70.01571152256909\n",
      "Iteration 803, the loss is 53.81489162959765, parameters k is 23.27514567282391 and b is -70.01573128541494\n",
      "Iteration 804, the loss is 53.42603926015866, parameters k is 23.212789210373316 and b is -70.0157510482608\n",
      "Iteration 805, the loss is 53.03718689071962, parameters k is 23.15043274792272 and b is -70.01577081110665\n",
      "Iteration 806, the loss is 52.64833452128056, parameters k is 23.088076285472127 and b is -70.0157905739525\n",
      "Iteration 807, the loss is 52.25948215184154, parameters k is 23.025719823021532 and b is -70.01581033679835\n",
      "Iteration 808, the loss is 51.87062978240251, parameters k is 22.963363360570938 and b is -70.0158300996442\n",
      "Iteration 809, the loss is 51.48177741296342, parameters k is 22.901006898120343 and b is -70.01584986249006\n",
      "Iteration 810, the loss is 51.09292504352438, parameters k is 22.83865043566975 and b is -70.01586962533591\n",
      "Iteration 811, the loss is 50.70407267408538, parameters k is 22.776293973219154 and b is -70.01588938818176\n",
      "Iteration 812, the loss is 50.31522030464632, parameters k is 22.71393751076856 and b is -70.01590915102761\n",
      "Iteration 813, the loss is 49.92636793520731, parameters k is 22.651581048317965 and b is -70.01592891387347\n",
      "Iteration 814, the loss is 49.53751556576825, parameters k is 22.58922458586737 and b is -70.01594867671932\n",
      "Iteration 815, the loss is 49.148663196329295, parameters k is 22.526868123416776 and b is -70.01596843956517\n",
      "Iteration 816, the loss is 48.75981082689024, parameters k is 22.46451166096618 and b is -70.01598820241102\n",
      "Iteration 817, the loss is 48.37095845745115, parameters k is 22.402155198515587 and b is -70.01600796525688\n",
      "Iteration 818, the loss is 47.982106088012145, parameters k is 22.339798736064992 and b is -70.01602772810273\n",
      "Iteration 819, the loss is 47.59325371857308, parameters k is 22.277442273614398 and b is -70.01604749094858\n",
      "Iteration 820, the loss is 47.204401349134095, parameters k is 22.215085811163803 and b is -70.01606725379443\n",
      "Iteration 821, the loss is 46.815548979694974, parameters k is 22.15272934871321 and b is -70.01608701664028\n",
      "Iteration 822, the loss is 46.426696610255995, parameters k is 22.090372886262614 and b is -70.01610677948614\n",
      "Iteration 823, the loss is 46.03784424081693, parameters k is 22.02801642381202 and b is -70.01612654233199\n",
      "Iteration 824, the loss is 45.64899187137793, parameters k is 21.965659961361425 and b is -70.01614630517784\n",
      "Iteration 825, the loss is 45.260139501938895, parameters k is 21.90330349891083 and b is -70.0161660680237\n",
      "Iteration 826, the loss is 44.871287132499816, parameters k is 21.840947036460236 and b is -70.01618583086955\n",
      "Iteration 827, the loss is 44.4824347630608, parameters k is 21.77859057400964 and b is -70.0162055937154\n",
      "Iteration 828, the loss is 44.093582393621794, parameters k is 21.716234111559046 and b is -70.01622535656125\n",
      "Iteration 829, the loss is 43.7047300241827, parameters k is 21.653877649108452 and b is -70.0162451194071\n",
      "Iteration 830, the loss is 43.3158776547437, parameters k is 21.591521186657857 and b is -70.01626488225295\n",
      "Iteration 831, the loss is 42.92702528530468, parameters k is 21.529164724207263 and b is -70.0162846450988\n",
      "Iteration 832, the loss is 42.53817291586566, parameters k is 21.46680826175667 and b is -70.01630440794466\n",
      "Iteration 833, the loss is 42.14932054642659, parameters k is 21.404451799306074 and b is -70.01632417079051\n",
      "Iteration 834, the loss is 41.76046817698751, parameters k is 21.34209533685548 and b is -70.01634393363636\n",
      "Iteration 835, the loss is 41.37161580754853, parameters k is 21.279738874404885 and b is -70.01636369648222\n",
      "Iteration 836, the loss is 40.982763438109465, parameters k is 21.21738241195429 and b is -70.01638345932807\n",
      "Iteration 837, the loss is 40.59391106867042, parameters k is 21.155025949503695 and b is -70.01640322217392\n",
      "Iteration 838, the loss is 40.20505869923139, parameters k is 21.0926694870531 and b is -70.01642298501977\n",
      "Iteration 839, the loss is 39.816206329792315, parameters k is 21.030313024602506 and b is -70.01644274786563\n",
      "Iteration 840, the loss is 39.42735396035327, parameters k is 20.96795656215191 and b is -70.01646251071148\n",
      "Iteration 841, the loss is 39.03850159091425, parameters k is 20.905600099701317 and b is -70.01648227355733\n",
      "Iteration 842, the loss is 38.649649221475215, parameters k is 20.843243637250723 and b is -70.01650203640318\n",
      "Iteration 843, the loss is 38.260796852036215, parameters k is 20.780887174800128 and b is -70.01652179924903\n",
      "Iteration 844, the loss is 37.87194448259716, parameters k is 20.718530712349533 and b is -70.01654156209489\n",
      "Iteration 845, the loss is 37.483092113158094, parameters k is 20.65617424989894 and b is -70.01656132494074\n",
      "Iteration 846, the loss is 37.0942397437191, parameters k is 20.593817787448344 and b is -70.01658108778659\n",
      "Iteration 847, the loss is 36.70538737428008, parameters k is 20.53146132499775 and b is -70.01660085063244\n",
      "Iteration 848, the loss is 36.31653500484103, parameters k is 20.469104862547155 and b is -70.0166206134783\n",
      "Iteration 849, the loss is 35.92818458609665, parameters k is 20.40674840009656 and b is -70.01664037632415\n",
      "Iteration 850, the loss is 35.54222290417813, parameters k is 20.3446241510847 and b is -70.01666013917\n",
      "Iteration 851, the loss is 35.156261222259616, parameters k is 20.282499902072843 and b is -70.01667990201585\n",
      "Iteration 852, the loss is 34.77087181390826, parameters k is 20.220375653060984 and b is -70.0166996648617\n",
      "Iteration 853, the loss is 34.38693971004312, parameters k is 20.15841496136138 and b is -70.01671942770756\n",
      "Iteration 854, the loss is 34.00300760617803, parameters k is 20.096454269661777 and b is -70.01673919055341\n",
      "Iteration 855, the loss is 33.61907550231286, parameters k is 20.034493577962174 and b is -70.01675895339926\n",
      "Iteration 856, the loss is 33.23514339844778, parameters k is 19.97253288626257 and b is -70.01677871624511\n",
      "Iteration 857, the loss is 32.851211294582654, parameters k is 19.910572194562967 and b is -70.01679847909097\n",
      "Iteration 858, the loss is 32.46727919071755, parameters k is 19.848611502863363 and b is -70.01681824193682\n",
      "Iteration 859, the loss is 32.08350391273461, parameters k is 19.78665081116376 and b is -70.01683800478267\n",
      "Iteration 860, the loss is 31.701596036723974, parameters k is 19.72485367677641 and b is -70.01685776762852\n",
      "Iteration 861, the loss is 31.31968816071343, parameters k is 19.663056542389057 and b is -70.01687753047437\n",
      "Iteration 862, the loss is 30.93778028470282, parameters k is 19.601259408001706 and b is -70.01689729332023\n",
      "Iteration 863, the loss is 30.555872408692228, parameters k is 19.539462273614355 and b is -70.01691705616608\n",
      "Iteration 864, the loss is 30.173964532681627, parameters k is 19.477665139227003 and b is -70.01693681901193\n",
      "Iteration 865, the loss is 29.792056656671036, parameters k is 19.415868004839652 and b is -70.01695658185778\n",
      "Iteration 866, the loss is 29.410148780660403, parameters k is 19.3540708704523 and b is -70.01697634470364\n",
      "Iteration 867, the loss is 29.02862123099042, parameters k is 19.29227373606495 and b is -70.01699610754949\n",
      "Iteration 868, the loss is 28.649744005248362, parameters k is 19.2307222933772 and b is -70.01701587039534\n",
      "Iteration 869, the loss is 28.270866779506278, parameters k is 19.169170850689454 and b is -70.01703563324119\n",
      "Iteration 870, the loss is 27.891989553764272, parameters k is 19.107619408001707 and b is -70.01705539608704\n",
      "Iteration 871, the loss is 27.513112328022228, parameters k is 19.04606796531396 and b is -70.0170751589329\n",
      "Iteration 872, the loss is 27.13445942028435, parameters k is 18.984516522626212 and b is -70.01709492177875\n",
      "Iteration 873, the loss is 26.757830784818676, parameters k is 18.92314800483965 and b is -70.0171146846246\n",
      "Iteration 874, the loss is 26.381202149352998, parameters k is 18.861779487053088 and b is -70.01713444747045\n",
      "Iteration 875, the loss is 26.004573513887316, parameters k is 18.800410969266526 and b is -70.0171542103163\n",
      "Iteration 876, the loss is 25.627944878421623, parameters k is 18.739042451479964 and b is -70.01717397316216\n",
      "Iteration 877, the loss is 25.251316242955927, parameters k is 18.677673933693402 and b is -70.01719373600801\n",
      "Iteration 878, the loss is 24.8746876074903, parameters k is 18.61630541590684 and b is -70.01721349885386\n",
      "Iteration 879, the loss is 24.498058972024594, parameters k is 18.55493689812028 and b is -70.01723326169972\n",
      "Iteration 880, the loss is 24.121958688942097, parameters k is 18.493568380333716 and b is -70.01725302454557\n",
      "Iteration 881, the loss is 23.747733966909216, parameters k is 18.43239602855506 and b is -70.01727278739142\n",
      "Iteration 882, the loss is 23.37350924487634, parameters k is 18.371223676776406 and b is -70.01729255023727\n",
      "Iteration 883, the loss is 22.99928452284349, parameters k is 18.31005132499775 and b is -70.01731231308312\n",
      "Iteration 884, the loss is 22.625059800810615, parameters k is 18.248878973219096 and b is -70.01733207592898\n",
      "Iteration 885, the loss is 22.250835078777726, parameters k is 18.18770662144044 and b is -70.01735183877483\n",
      "Iteration 886, the loss is 21.876610356744873, parameters k is 18.126534269661786 and b is -70.01737160162068\n",
      "Iteration 887, the loss is 21.50238563471198, parameters k is 18.06536191788313 and b is -70.01739136446653\n",
      "Iteration 888, the loss is 21.128853051424453, parameters k is 18.004189566104476 and b is -70.01741112731239\n",
      "Iteration 889, the loss is 20.757145168677358, parameters k is 17.943189862547165 and b is -70.01743089015824\n",
      "Iteration 890, the loss is 20.38824552089499, parameters k is 17.88245430918748 and b is -70.01745065300409\n",
      "Iteration 891, the loss is 20.019345873112595, parameters k is 17.821718755827796 and b is -70.01747041584994\n",
      "Iteration 892, the loss is 19.650446225330203, parameters k is 17.76098320246811 and b is -70.0174901786958\n",
      "Iteration 893, the loss is 19.281546577547818, parameters k is 17.700247649108427 and b is -70.01750994154165\n",
      "Iteration 894, the loss is 18.91264692976541, parameters k is 17.639512095748742 and b is -70.0175297043875\n",
      "Iteration 895, the loss is 18.543747281983023, parameters k is 17.578776542389058 and b is -70.01754946723335\n",
      "Iteration 896, the loss is 18.17484763420064, parameters k is 17.518040989029373 and b is -70.0175692300792\n",
      "Iteration 897, the loss is 17.806015259579084, parameters k is 17.45730543566969 and b is -70.01758899292506\n",
      "Iteration 898, the loss is 17.439803306103425, parameters k is 17.396791542389057 and b is -70.01760875577091\n",
      "Iteration 899, the loss is 17.073591352627805, parameters k is 17.336277649108425 and b is -70.01762851861676\n",
      "Iteration 900, the loss is 16.709959007901787, parameters k is 17.275763755827793 and b is -70.01764828146261\n",
      "Iteration 901, the loss is 16.353809647308715, parameters k is 17.21608705622305 and b is -70.01766804430846\n",
      "Iteration 902, the loss is 15.9991558383327, parameters k is 17.156410356618306 and b is -70.01768780715432\n",
      "Iteration 903, the loss is 15.648210241808648, parameters k is 17.09714986254716 and b is -70.01770757000017\n",
      "Iteration 904, the loss is 15.301281431681222, parameters k is 17.038166680728978 and b is -70.01772733284602\n",
      "Iteration 905, the loss is 14.958597048398882, parameters k is 16.979556028555063 and b is -70.01774709569187\n",
      "Iteration 906, the loss is 14.617974483565664, parameters k is 16.921143479147947 and b is -70.01776685853773\n",
      "Iteration 907, the loss is 14.281607257580227, parameters k is 16.86314784673688 and b is -70.01778662138358\n",
      "Iteration 908, the loss is 13.946361851244562, parameters k is 16.805152214325812 and b is -70.01780638422943\n",
      "Iteration 909, the loss is 13.614729061105313, parameters k is 16.747566186657828 and b is -70.01782614707528\n",
      "Iteration 910, the loss is 13.284771491836821, parameters k is 16.689980158989844 and b is -70.01784590992114\n",
      "Iteration 911, the loss is 12.961823500509059, parameters k is 16.63303788626257 and b is -70.01786567276699\n",
      "Iteration 912, the loss is 12.642332660859854, parameters k is 16.57650245147996 and b is -70.01788543561284\n",
      "Iteration 913, the loss is 12.325601043387632, parameters k is 16.520180613535295 and b is -70.01790519845869\n",
      "Iteration 914, the loss is 12.01318578665713, parameters k is 16.46428802460249 and b is -70.01792496130454\n",
      "Iteration 915, the loss is 11.70166901847202, parameters k is 16.408395435669682 and b is -70.0179447241504\n",
      "Iteration 916, the loss is 11.393897927132326, parameters k is 16.352919842784306 and b is -70.01796448699625\n",
      "Iteration 917, the loss is 11.086126835792625, parameters k is 16.29744424989893 and b is -70.0179842498421\n",
      "Iteration 918, the loss is 10.778355744452913, parameters k is 16.241968657013555 and b is -70.01800401268795\n",
      "Iteration 919, the loss is 10.47067371221186, parameters k is 16.18649306412818 and b is -70.0180237755338\n",
      "Iteration 920, the loss is 10.165595134664912, parameters k is 16.13126067282383 and b is -70.01804353837966\n",
      "Iteration 921, the loss is 9.860883456192266, parameters k is 16.076028281519484 and b is -70.01806330122551\n",
      "Iteration 922, the loss is 9.561207382203568, parameters k is 16.021244704444385 and b is -70.01808306407136\n",
      "Iteration 923, the loss is 9.267998805957971, parameters k is 15.967031562151895 and b is -70.01810282691721\n",
      "Iteration 924, the loss is 8.980209032329029, parameters k is 15.913046996934504 and b is -70.01812258976307\n",
      "Iteration 925, the loss is 8.701373286878537, parameters k is 15.860207649108418 and b is -70.01814235260892\n",
      "Iteration 926, the loss is 8.426762295133253, parameters k is 15.80760411155901 and b is -70.01816211545477\n",
      "Iteration 927, the loss is 8.161131135142432, parameters k is 15.755895317092607 and b is -70.01818187830062\n",
      "Iteration 928, the loss is 7.904560724281222, parameters k is 15.705078657013555 and b is -70.01820164114648\n",
      "Iteration 929, the loss is 7.6573488516179875, parameters k is 15.655183498910787 and b is -70.01822140399233\n",
      "Iteration 930, the loss is 7.422662609558618, parameters k is 15.606417787448336 and b is -70.01824116683818\n",
      "Iteration 931, the loss is 7.20133051194217, parameters k is 15.559021522626201 and b is -70.01826092968403\n",
      "Iteration 932, the loss is 6.997896623449837, parameters k is 15.513486601677585 and b is -70.01828069252988\n",
      "Iteration 933, the loss is 6.805312056935499, parameters k is 15.46934895345624 and b is -70.01830045537574\n",
      "Iteration 934, the loss is 6.6262236244112085, parameters k is 15.42636371630209 and b is -70.01832021822159\n",
      "Iteration 935, the loss is 6.467462253706476, parameters k is 15.38616919061039 and b is -70.01833998106744\n",
      "Iteration 936, the loss is 6.3273672223142015, parameters k is 15.348131660966121 and b is -70.0183597439133\n",
      "Iteration 937, the loss is 6.200614423966139, parameters k is 15.312104605630154 and b is -70.01837950675915\n",
      "Iteration 938, the loss is 6.083867550809269, parameters k is 15.277528617487862 and b is -70.018399269605\n",
      "Iteration 939, the loss is 5.979164284548711, parameters k is 15.244480613535293 and b is -70.01841903245085\n",
      "Iteration 940, the loss is 5.886867087452392, parameters k is 15.21377762934557 and b is -70.0184387952967\n",
      "Iteration 941, the loss is 5.800458948517564, parameters k is 15.184105989029364 and b is -70.01845855814256\n",
      "Iteration 942, the loss is 5.724793393166542, parameters k is 15.15645834080802 and b is -70.01847832098841\n",
      "Iteration 943, the loss is 5.656159367121197, parameters k is 15.129562945551104 and b is -70.01849808383426\n",
      "Iteration 944, the loss is 5.597346467506413, parameters k is 15.104740317092606 and b is -70.01851784668011\n",
      "Iteration 945, the loss is 5.546099422448815, parameters k is 15.081865712349524 and b is -70.01853760952596\n",
      "Iteration 946, the loss is 5.4989152038345, parameters k is 15.060055653060987 and b is -70.01855737237182\n",
      "Iteration 947, the loss is 5.456517769597102, parameters k is 15.039200336855453 and b is -70.01857713521767\n",
      "Iteration 948, the loss is 5.421896659261206, parameters k is 15.020391502863358 and b is -70.01859689806352\n",
      "Iteration 949, the loss is 5.391285352016011, parameters k is 15.002526265709207 and b is -70.01861666090937\n",
      "Iteration 950, the loss is 5.364533583260829, parameters k is 14.98615110760644 and b is -70.01863642375523\n",
      "Iteration 951, the loss is 5.339509577814178, parameters k is 14.970231759780354 and b is -70.01865618660108\n",
      "Iteration 952, the loss is 5.315666343728461, parameters k is 14.95456201669735 and b is -70.01867594944693\n",
      "Iteration 953, the loss is 5.295549967441436, parameters k is 14.940304803258615 and b is -70.01869571229278\n",
      "Iteration 954, the loss is 5.27660480325918, parameters k is 14.926289566104465 and b is -70.01871547513863\n",
      "Iteration 955, the loss is 5.258990184235303, parameters k is 14.912869506815928 and b is -70.01873523798449\n",
      "Iteration 956, the loss is 5.243184404802399, parameters k is 14.899965850689446 and b is -70.01875500083034\n",
      "Iteration 957, the loss is 5.229988190389944, parameters k is 14.888257728159802 and b is -70.01877476367619\n",
      "Iteration 958, the loss is 5.218287064686964, parameters k is 14.877251344760593 and b is -70.01879452652204\n",
      "Iteration 959, the loss is 5.20888456970516, parameters k is 14.867257649108419 and b is -70.0188142893679\n",
      "Iteration 960, the loss is 5.201608555391763, parameters k is 14.858506957408814 and b is -70.01883405221375\n",
      "Iteration 961, the loss is 5.195409962038957, parameters k is 14.850537174800118 and b is -70.0188538150596\n",
      "Iteration 962, the loss is 5.190286368223482, parameters k is 14.843082135274427 and b is -70.01887357790545\n",
      "Iteration 963, the loss is 5.186452759916373, parameters k is 14.836842668871265 and b is -70.0188933407513\n",
      "Iteration 964, the loss is 5.182931746513638, parameters k is 14.830855494958222 and b is -70.01891310359716\n",
      "Iteration 965, the loss is 5.179798887335456, parameters k is 14.825111917883122 and b is -70.01893286644301\n",
      "Iteration 966, the loss is 5.177335416187605, parameters k is 14.820149289424624 and b is -70.01895262928886\n",
      "Iteration 967, the loss is 5.17493808538149, parameters k is 14.815186660966125 and b is -70.01897239213471\n",
      "Iteration 968, the loss is 5.1727442788573725, parameters k is 14.81046201669735 and b is -70.01899215498057\n",
      "Iteration 969, the loss is 5.170863997488456, parameters k is 14.805990929740828 and b is -70.01901191782642\n",
      "Iteration 970, the loss is 5.169402629853286, parameters k is 14.801999052270473 and b is -70.01903168067227\n",
      "Iteration 971, the loss is 5.168336476935377, parameters k is 14.798734091796165 and b is -70.01905144351812\n",
      "Iteration 972, the loss is 5.167322091591578, parameters k is 14.795469131321857 and b is -70.01907120636398\n",
      "Iteration 973, the loss is 5.166405467768321, parameters k is 14.792441680728972 and b is -70.01909096920983\n",
      "Iteration 974, the loss is 5.1655366991190625, parameters k is 14.789414230136087 and b is -70.01911073205568\n",
      "Iteration 975, the loss is 5.164791208300491, parameters k is 14.786629467290236 and b is -70.01913049490153\n",
      "Iteration 976, the loss is 5.164180499593426, parameters k is 14.78407782697403 and b is -70.01915025774738\n",
      "Iteration 977, the loss is 5.163653649952289, parameters k is 14.781767095748734 and b is -70.01917002059324\n",
      "Iteration 978, the loss is 5.163243453529871, parameters k is 14.779741186657825 and b is -70.01918978343909\n",
      "Iteration 979, the loss is 5.162852063565445, parameters k is 14.777715277566916 and b is -70.01920954628494\n",
      "Iteration 980, the loss is 5.162546368068524, parameters k is 14.77596596926652 and b is -70.0192293091308\n",
      "Iteration 981, the loss is 5.162240672571599, parameters k is 14.774216660966125 and b is -70.01924907197665\n",
      "Iteration 982, the loss is 5.161934977074676, parameters k is 14.77246735266573 and b is -70.0192688348225\n",
      "Iteration 983, the loss is 5.1616292815777545, parameters k is 14.770718044365335 and b is -70.01928859766835\n",
      "Iteration 984, the loss is 5.161361419014786, parameters k is 14.76896873606494 and b is -70.0193083605142\n",
      "Iteration 985, the loss is 5.161268423275085, parameters k is 14.768001562151897 and b is -70.01932812336005\n",
      "Iteration 986, the loss is 5.161217416288984, parameters k is 14.767034388238853 and b is -70.0193478862059\n",
      "Iteration 987, the loss is 5.161195492935978, parameters k is 14.766558716302095 and b is -70.01936764905176\n",
      "Iteration 988, the loss is 5.161173569582973, parameters k is 14.766083044365336 and b is -70.01938741189761\n",
      "Iteration 989, the loss is 5.161151646229974, parameters k is 14.765607372428578 and b is -70.01940717474346\n",
      "Iteration 990, the loss is 5.161129722876971, parameters k is 14.76513170049182 and b is -70.01942693758932\n",
      "Iteration 991, the loss is 5.161107799523962, parameters k is 14.764656028555061 and b is -70.01944670043517\n",
      "Iteration 992, the loss is 5.161085876170954, parameters k is 14.764180356618303 and b is -70.01946646328102\n",
      "Iteration 993, the loss is 5.161063952817951, parameters k is 14.763704684681544 and b is -70.01948622612687\n",
      "Iteration 994, the loss is 5.161042029464944, parameters k is 14.763229012744786 and b is -70.01950598897272\n",
      "Iteration 995, the loss is 5.161020106111943, parameters k is 14.762753340808027 and b is -70.01952575181858\n",
      "Iteration 996, the loss is 5.160998182758936, parameters k is 14.762277668871269 and b is -70.01954551466443\n",
      "Iteration 997, the loss is 5.160976259405928, parameters k is 14.76180199693451 and b is -70.01956527751028\n",
      "Iteration 998, the loss is 5.160954336052927, parameters k is 14.761326324997752 and b is -70.01958504035613\n",
      "Iteration 999, the loss is 5.160932412699917, parameters k is 14.760850653060993 and b is -70.01960480320199\n",
      "Iteration 1000, the loss is 5.160910489346917, parameters k is 14.760374981124235 and b is -70.01962456604784\n",
      "Iteration 1001, the loss is 5.160888565993906, parameters k is 14.759899309187476 and b is -70.01964432889369\n",
      "Iteration 1002, the loss is 5.160872261062649, parameters k is 14.759423637250718 and b is -70.01966409173954\n",
      "Iteration 1003, the loss is 5.160873729635668, parameters k is 14.759557846736884 and b is -70.0196838545854\n",
      "Iteration 1004, the loss is 5.160878562320992, parameters k is 14.759082174800126 and b is -70.01970361743125\n",
      "Iteration 1005, the loss is 5.160877620356534, parameters k is 14.759216384286292 and b is -70.0197233802771\n",
      "Iteration 1006, the loss is 5.160876678392082, parameters k is 14.759350593772458 and b is -70.01974314312295\n",
      "Iteration 1007, the loss is 5.160875736427637, parameters k is 14.759484803258625 and b is -70.0197629059688\n",
      "Iteration 1008, the loss is 5.1608801542617, parameters k is 14.75961901274479 and b is -70.01978266881466\n",
      "Iteration 1009, the loss is 5.160882037685976, parameters k is 14.759143340808032 and b is -70.01980243166051\n",
      "Iteration 1010, the loss is 5.160881095721529, parameters k is 14.759277550294199 and b is -70.01982219450636\n",
      "Iteration 1011, the loss is 5.16088015375708, parameters k is 14.759411759780365 and b is -70.01984195735221\n",
      "Iteration 1012, the loss is 5.160880330788359, parameters k is 14.759545969266531 and b is -70.01986172019807\n",
      "Iteration 1013, the loss is 5.160882306722202, parameters k is 14.759379388238862 and b is -70.01988148304392\n",
      "Iteration 1014, the loss is 5.160881364757755, parameters k is 14.759513597725029 and b is -70.01990124588977\n",
      "Iteration 1015, the loss is 5.160886445116625, parameters k is 14.759647807211195 and b is -70.01992100873562\n",
      "Iteration 1016, the loss is 5.160887666016095, parameters k is 14.759172135274436 and b is -70.01994077158147\n",
      "Iteration 1017, the loss is 5.160886724051648, parameters k is 14.759306344760603 and b is -70.01996053442733\n",
      "Iteration 1018, the loss is 5.160885782087195, parameters k is 14.759440554246769 and b is -70.01998029727318\n",
      "Iteration 1019, the loss is 5.160886278430605, parameters k is 14.759574763732935 and b is -70.02000006011903\n",
      "Iteration 1020, the loss is 5.160887935052326, parameters k is 14.759408182705267 and b is -70.02001982296488\n",
      "Iteration 1021, the loss is 5.160887301462444, parameters k is 14.759542392191433 and b is -70.02003958581074\n",
      "Iteration 1022, the loss is 5.1608900880174575, parameters k is 14.759375811163764 and b is -70.02005934865659\n",
      "Iteration 1023, the loss is 5.160889146053002, parameters k is 14.75951002064993 and b is -70.02007911150244\n",
      "Iteration 1024, the loss is 5.160892602200422, parameters k is 14.759644230136097 and b is -70.02009887434829\n",
      "Iteration 1025, the loss is 5.160895447311342, parameters k is 14.759168558199338 and b is -70.02011863719414\n",
      "Iteration 1026, the loss is 5.1608945053468895, parameters k is 14.759302767685504 and b is -70.02013840004\n",
      "Iteration 1027, the loss is 5.160893563382439, parameters k is 14.75943697717167 and b is -70.02015816288585\n",
      "Iteration 1028, the loss is 5.160893249104687, parameters k is 14.759571186657837 and b is -70.0201779257317\n",
      "Iteration 1029, the loss is 5.160895716347576, parameters k is 14.759404605630168 and b is -70.02019768857755\n",
      "Iteration 1030, the loss is 5.160894774383122, parameters k is 14.759538815116334 and b is -70.0202174514234\n",
      "Iteration 1031, the loss is 5.160898893055347, parameters k is 14.7596730246025 and b is -70.02023721426926\n",
      "Iteration 1032, the loss is 5.160901075641459, parameters k is 14.759197352665742 and b is -70.02025697711511\n",
      "Iteration 1033, the loss is 5.1609001336770115, parameters k is 14.759331562151909 and b is -70.02027673996096\n",
      "Iteration 1034, the loss is 5.1608991917125575, parameters k is 14.759465771638075 and b is -70.02029650280681\n",
      "Iteration 1035, the loss is 5.160899196746937, parameters k is 14.759599981124241 and b is -70.02031626565267\n",
      "Iteration 1036, the loss is 5.160901344677698, parameters k is 14.759433400096572 and b is -70.02033602849852\n",
      "Iteration 1037, the loss is 5.160900402713245, parameters k is 14.759567609582739 and b is -70.02035579134437\n",
      "Iteration 1038, the loss is 5.160905183910266, parameters k is 14.759701819068905 and b is -70.02037555419022\n",
      "Iteration 1039, the loss is 5.160906703971582, parameters k is 14.759226147132146 and b is -70.02039531703608\n",
      "Iteration 1040, the loss is 5.160905762007128, parameters k is 14.759360356618313 and b is -70.02041507988193\n",
      "Iteration 1041, the loss is 5.16090482004268, parameters k is 14.759494566104479 and b is -70.02043484272778\n",
      "Iteration 1042, the loss is 5.160905144389185, parameters k is 14.759628775590645 and b is -70.02045460557363\n",
      "Iteration 1043, the loss is 5.16090697300781, parameters k is 14.759462194562976 and b is -70.02047436841949\n",
      "Iteration 1044, the loss is 5.160906167421023, parameters k is 14.759596404049143 and b is -70.02049413126534\n",
      "Iteration 1045, the loss is 5.1609091259729425, parameters k is 14.759429823021474 and b is -70.02051389411119\n",
      "Iteration 1046, the loss is 5.160908184008492, parameters k is 14.75956403250764 and b is -70.02053365695704\n",
      "Iteration 1047, the loss is 5.160911340994076, parameters k is 14.759698241993807 and b is -70.0205534198029\n",
      "Iteration 1048, the loss is 5.16091448526683, parameters k is 14.759222570057048 and b is -70.02057318264875\n",
      "Iteration 1049, the loss is 5.160913543302378, parameters k is 14.759356779543214 and b is -70.0205929454946\n",
      "Iteration 1050, the loss is 5.160912601337927, parameters k is 14.75949098902938 and b is -70.02061270834045\n",
      "Iteration 1051, the loss is 5.160912115063267, parameters k is 14.759625198515547 and b is -70.0206324711863\n",
      "Iteration 1052, the loss is 5.160914754303058, parameters k is 14.759458617487878 and b is -70.02065223403216\n",
      "Iteration 1053, the loss is 5.160913812338606, parameters k is 14.759592826974044 and b is -70.02067199687801\n",
      "Iteration 1054, the loss is 5.160917631848993, parameters k is 14.75972703646021 and b is -70.02069175972386\n",
      "Iteration 1055, the loss is 5.160920113596952, parameters k is 14.759251364523452 and b is -70.02071152256971\n",
      "Iteration 1056, the loss is 5.16091917163249, parameters k is 14.759385574009618 and b is -70.02073128541556\n",
      "Iteration 1057, the loss is 5.160918229668038, parameters k is 14.759519783495785 and b is -70.02075104826142\n",
      "Iteration 1058, the loss is 5.160918062705522, parameters k is 14.759653992981951 and b is -70.02077081110727\n",
      "Iteration 1059, the loss is 5.160920382633177, parameters k is 14.759487411954282 and b is -70.02079057395312\n",
      "Iteration 1060, the loss is 5.160919440668728, parameters k is 14.759621621440449 and b is -70.02081033679897\n",
      "Iteration 1061, the loss is 5.160923922703916, parameters k is 14.759755830926615 and b is -70.02083009964483\n",
      "Iteration 1062, the loss is 5.1609257419270635, parameters k is 14.759280158989856 and b is -70.02084986249068\n",
      "Iteration 1063, the loss is 5.160924799962616, parameters k is 14.759414368476023 and b is -70.02086962533653\n",
      "Iteration 1064, the loss is 5.160923857998167, parameters k is 14.759548577962189 and b is -70.02088938818238\n",
      "Iteration 1065, the loss is 5.160924010347764, parameters k is 14.759682787448355 and b is -70.02090915102823\n",
      "Iteration 1066, the loss is 5.1609260109632915, parameters k is 14.759516206420686 and b is -70.02092891387409\n",
      "Iteration 1067, the loss is 5.160925068998846, parameters k is 14.759650415906853 and b is -70.02094867671994\n",
      "Iteration 1068, the loss is 5.160930213558827, parameters k is 14.759784625393019 and b is -70.02096843956579\n",
      "Iteration 1069, the loss is 5.160931370257185, parameters k is 14.75930895345626 and b is -70.02098820241164\n",
      "Iteration 1070, the loss is 5.160930428292731, parameters k is 14.759443162942427 and b is -70.0210079652575\n",
      "Iteration 1071, the loss is 5.160929486328283, parameters k is 14.759577372428593 and b is -70.02102772810335\n",
      "Iteration 1072, the loss is 5.160929957990007, parameters k is 14.75971158191476 and b is -70.0210474909492\n",
      "Iteration 1073, the loss is 5.160931639293412, parameters k is 14.75954500088709 and b is -70.02106725379505\n",
      "Iteration 1074, the loss is 5.160930981021851, parameters k is 14.759679210373257 and b is -70.0210870166409\n",
      "Iteration 1075, the loss is 5.160933792258544, parameters k is 14.759512629345588 and b is -70.02110677948676\n",
      "Iteration 1076, the loss is 5.160932850294093, parameters k is 14.759646838831754 and b is -70.02112654233261\n",
      "Iteration 1077, the loss is 5.160936370642642, parameters k is 14.75978104831792 and b is -70.02114630517846\n",
      "Iteration 1078, the loss is 5.160939151552432, parameters k is 14.759305376381162 and b is -70.02116606802431\n",
      "Iteration 1079, the loss is 5.1609382095879806, parameters k is 14.759439585867328 and b is -70.02118583087017\n",
      "Iteration 1080, the loss is 5.16093726762353, parameters k is 14.759573795353495 and b is -70.02120559371602\n",
      "Iteration 1081, the loss is 5.160936928664097, parameters k is 14.759708004839661 and b is -70.02122535656187\n",
      "Iteration 1082, the loss is 5.160939420588665, parameters k is 14.759541423811992 and b is -70.02124511940772\n",
      "Iteration 1083, the loss is 5.160938478624213, parameters k is 14.759675633298158 and b is -70.02126488225358\n",
      "Iteration 1084, the loss is 5.160942661497566, parameters k is 14.759809842784325 and b is -70.02128464509943\n",
      "Iteration 1085, the loss is 5.160944779882558, parameters k is 14.759334170847566 and b is -70.02130440794528\n",
      "Iteration 1086, the loss is 5.160943837918097, parameters k is 14.759468380333733 and b is -70.02132417079113\n",
      "Iteration 1087, the loss is 5.160942895953649, parameters k is 14.759602589819899 and b is -70.02134393363698\n",
      "Iteration 1088, the loss is 5.160942876306343, parameters k is 14.759736799306065 and b is -70.02136369648284\n",
      "Iteration 1089, the loss is 5.160945048918779, parameters k is 14.759570218278396 and b is -70.02138345932869\n",
      "Iteration 1090, the loss is 5.160944106954332, parameters k is 14.759704427764563 and b is -70.02140322217454\n",
      "Iteration 1091, the loss is 5.160948952352482, parameters k is 14.759838637250729 and b is -70.0214229850204\n",
      "Iteration 1092, the loss is 5.16095040821267, parameters k is 14.75936296531397 and b is -70.02144274786625\n",
      "Iteration 1093, the loss is 5.160949466248214, parameters k is 14.759497174800137 and b is -70.0214625107121\n",
      "Iteration 1094, the loss is 5.160948524283772, parameters k is 14.759631384286303 and b is -70.02148227355795\n",
      "Iteration 1095, the loss is 5.160948823948587, parameters k is 14.75976559377247 and b is -70.0215020364038\n",
      "Iteration 1096, the loss is 5.160950677248895, parameters k is 14.7595990127448 and b is -70.02152179924965\n",
      "Iteration 1097, the loss is 5.160949846980421, parameters k is 14.759733222230967 and b is -70.0215415620955\n",
      "Iteration 1098, the loss is 5.16095283021403, parameters k is 14.759566641203298 and b is -70.02156132494136\n",
      "Iteration 1099, the loss is 5.160951888249577, parameters k is 14.759700850689464 and b is -70.02158108778721\n",
      "Iteration 1100, the loss is 5.160955109436293, parameters k is 14.75983506017563 and b is -70.02160085063306\n",
      "Iteration 1101, the loss is 5.160958189507918, parameters k is 14.759359388238872 and b is -70.02162061347892\n",
      "Iteration 1102, the loss is 5.160957247543463, parameters k is 14.759493597725038 and b is -70.02164037632477\n",
      "Iteration 1103, the loss is 5.160956305579011, parameters k is 14.759627807211205 and b is -70.02166013917062\n",
      "Iteration 1104, the loss is 5.160955794622674, parameters k is 14.75976201669737 and b is -70.02167990201647\n",
      "Iteration 1105, the loss is 5.160958458544148, parameters k is 14.759595435669702 and b is -70.02169966486233\n",
      "Iteration 1106, the loss is 5.1609575165797015, parameters k is 14.759729645155868 and b is -70.02171942770818\n",
      "Iteration 1107, the loss is 5.160961400291208, parameters k is 14.759863854642035 and b is -70.02173919055403\n",
      "Iteration 1108, the loss is 5.160963817838037, parameters k is 14.759388182705276 and b is -70.02175895339988\n",
      "Iteration 1109, the loss is 5.16096287587358, parameters k is 14.759522392191442 and b is -70.02177871624573\n",
      "Iteration 1110, the loss is 5.160961933909141, parameters k is 14.759656601677609 and b is -70.02179847909159\n",
      "Iteration 1111, the loss is 5.160961742264917, parameters k is 14.759790811163775 and b is -70.02181824193744\n",
      "Iteration 1112, the loss is 5.160964086874264, parameters k is 14.759624230136106 and b is -70.02183800478329\n",
      "Iteration 1113, the loss is 5.160963144909815, parameters k is 14.759758439622273 and b is -70.02185776762914\n",
      "Iteration 1114, the loss is 5.160967691146127, parameters k is 14.759892649108439 and b is -70.021877530475\n",
      "Iteration 1115, the loss is 5.160969446168153, parameters k is 14.75941697717168 and b is -70.02189729332085\n",
      "Iteration 1116, the loss is 5.160968504203704, parameters k is 14.759551186657847 and b is -70.0219170561667\n",
      "Iteration 1117, the loss is 5.160967562239253, parameters k is 14.759685396144013 and b is -70.02193681901255\n",
      "Iteration 1118, the loss is 5.160967689907163, parameters k is 14.75981960563018 and b is -70.0219565818584\n",
      "Iteration 1119, the loss is 5.1609697152043745, parameters k is 14.75965302460251 and b is -70.02197634470426\n",
      "Iteration 1120, the loss is 5.160968773239933, parameters k is 14.759787234088677 and b is -70.02199610755011\n",
      "Iteration 1121, the loss is 5.160973982001047, parameters k is 14.759921443574843 and b is -70.02201587039596\n",
      "Iteration 1122, the loss is 5.160975074498273, parameters k is 14.759445771638084 and b is -70.02203563324181\n",
      "Iteration 1123, the loss is 5.160974132533818, parameters k is 14.75957998112425 and b is -70.02205539608767\n",
      "Iteration 1124, the loss is 5.160973190569373, parameters k is 14.759714190610417 and b is -70.02207515893352\n",
      "Iteration 1125, the loss is 5.160973637549416, parameters k is 14.759848400096583 and b is -70.02209492177937\n",
      "Iteration 1126, the loss is 5.160975343534502, parameters k is 14.759681819068915 and b is -70.02211468462522\n",
      "Iteration 1127, the loss is 5.160974660581248, parameters k is 14.75981602855508 and b is -70.02213444747107\n",
      "Iteration 1128, the loss is 5.160977496499634, parameters k is 14.759649447527412 and b is -70.02215421031693\n",
      "Iteration 1129, the loss is 5.160976554535179, parameters k is 14.759783657013578 and b is -70.02217397316278\n",
      "Iteration 1130, the loss is 5.1609801390848595, parameters k is 14.759917866499745 and b is -70.02219373600863\n",
      "Iteration 1131, the loss is 5.160982855793521, parameters k is 14.759442194562986 and b is -70.02221349885448\n",
      "Iteration 1132, the loss is 5.160981913829072, parameters k is 14.759576404049152 and b is -70.02223326170034\n",
      "Iteration 1133, the loss is 5.16098097186462, parameters k is 14.759710613535319 and b is -70.02225302454619\n",
      "Iteration 1134, the loss is 5.160980608223497, parameters k is 14.759844823021485 and b is -70.02227278739204\n",
      "Iteration 1135, the loss is 5.160983124829749, parameters k is 14.759678241993816 and b is -70.02229255023789\n",
      "Iteration 1136, the loss is 5.1609821828653, parameters k is 14.759812451479982 and b is -70.02231231308375\n",
      "Iteration 1137, the loss is 5.160986429939778, parameters k is 14.759946660966149 and b is -70.0223320759296\n",
      "Iteration 1138, the loss is 5.160988484123645, parameters k is 14.75947098902939 and b is -70.02235183877545\n",
      "Iteration 1139, the loss is 5.160987542159187, parameters k is 14.759605198515557 and b is -70.0223716016213\n",
      "Iteration 1140, the loss is 5.160986600194739, parameters k is 14.759739408001723 and b is -70.02239136446715\n",
      "Iteration 1141, the loss is 5.160986555865744, parameters k is 14.759873617487889 and b is -70.022411127313\n",
      "Iteration 1142, the loss is 5.160988753159873, parameters k is 14.75970703646022 and b is -70.02243089015886\n",
      "Iteration 1143, the loss is 5.160987811195417, parameters k is 14.759841245946387 and b is -70.02245065300471\n",
      "Iteration 1144, the loss is 5.160992720794695, parameters k is 14.759975455432553 and b is -70.02247041585056\n",
      "Iteration 1145, the loss is 5.160994112453762, parameters k is 14.759499783495794 and b is -70.02249017869642\n",
      "Iteration 1146, the loss is 5.160993170489308, parameters k is 14.75963399298196 and b is -70.02250994154227\n",
      "Iteration 1147, the loss is 5.160992228524859, parameters k is 14.759768202468127 and b is -70.02252970438812\n",
      "Iteration 1148, the loss is 5.160992503507991, parameters k is 14.759902411954293 and b is -70.02254946723397\n",
      "Iteration 1149, the loss is 5.160994381489988, parameters k is 14.759735830926624 and b is -70.02256923007982\n",
      "Iteration 1150, the loss is 5.160993526539829, parameters k is 14.75987004041279 and b is -70.02258899292568\n",
      "Iteration 1151, the loss is 5.160996534455115, parameters k is 14.759703459385122 and b is -70.02260875577153\n",
      "Iteration 1152, the loss is 5.160995592490666, parameters k is 14.759837668871288 and b is -70.02262851861738\n",
      "Iteration 1153, the loss is 5.16099887787851, parameters k is 14.759971878357455 and b is -70.02264828146323\n",
      "Iteration 1154, the loss is 5.161001893749005, parameters k is 14.759496206420696 and b is -70.02266804430909\n",
      "Iteration 1155, the loss is 5.161000951784556, parameters k is 14.759630415906862 and b is -70.02268780715494\n",
      "Iteration 1156, the loss is 5.161000009820108, parameters k is 14.759764625393029 and b is -70.02270757000079\n",
      "Iteration 1157, the loss is 5.160999474182074, parameters k is 14.759898834879195 and b is -70.02272733284664\n",
      "Iteration 1158, the loss is 5.161002162785235, parameters k is 14.759732253851526 and b is -70.0227470956925\n",
      "Iteration 1159, the loss is 5.161001220820781, parameters k is 14.759866463337692 and b is -70.02276685853835\n",
      "Iteration 1160, the loss is 5.161005168733422, parameters k is 14.760000672823859 and b is -70.0227866213842\n",
      "Iteration 1161, the loss is 5.161007522079125, parameters k is 14.7595250008871 and b is -70.02280638423005\n",
      "Iteration 1162, the loss is 5.1610065801146705, parameters k is 14.759659210373266 and b is -70.0228261470759\n",
      "Iteration 1163, the loss is 5.161005638150217, parameters k is 14.759793419859433 and b is -70.02284590992176\n",
      "Iteration 1164, the loss is 5.161005421824323, parameters k is 14.759927629345599 and b is -70.02286567276761\n",
      "Iteration 1165, the loss is 5.161007791115353, parameters k is 14.75976104831793 and b is -70.02288543561346\n",
      "Iteration 1166, the loss is 5.1610068491508985, parameters k is 14.759895257804097 and b is -70.02290519845931\n",
      "Iteration 1167, the loss is 5.161011459588348, parameters k is 14.760029467290263 and b is -70.02292496130516\n",
      "Iteration 1168, the loss is 5.161013150409237, parameters k is 14.759553795353504 and b is -70.02294472415102\n",
      "Iteration 1169, the loss is 5.161012208444797, parameters k is 14.75968800483967 and b is -70.02296448699687\n",
      "Iteration 1170, the loss is 5.16101126648034, parameters k is 14.759822214325837 and b is -70.02298424984272\n",
      "Iteration 1171, the loss is 5.161011369466566, parameters k is 14.759956423812003 and b is -70.02300401268857\n",
      "Iteration 1172, the loss is 5.161013419445468, parameters k is 14.759789842784334 and b is -70.02302377553443\n",
      "Iteration 1173, the loss is 5.161012477481023, parameters k is 14.7599240522705 and b is -70.02304353838028\n",
      "Iteration 1174, the loss is 5.1610177504432615, parameters k is 14.760058261756667 and b is -70.02306330122613\n",
      "Iteration 1175, the loss is 5.161018778739358, parameters k is 14.759582589819908 and b is -70.02308306407198\n",
      "Iteration 1176, the loss is 5.161017836774913, parameters k is 14.759716799306075 and b is -70.02310282691784\n",
      "Iteration 1177, the loss is 5.16101689481046, parameters k is 14.759851008792241 and b is -70.02312258976369\n",
      "Iteration 1178, the loss is 5.161017317108811, parameters k is 14.759985218278407 and b is -70.02314235260954\n",
      "Iteration 1179, the loss is 5.1610190477755955, parameters k is 14.759818637250739 and b is -70.02316211545539\n",
      "Iteration 1180, the loss is 5.161018340140654, parameters k is 14.759952846736905 and b is -70.02318187830124\n",
      "Iteration 1181, the loss is 5.161021200740721, parameters k is 14.759786265709236 and b is -70.0232016411471\n",
      "Iteration 1182, the loss is 5.161020258776273, parameters k is 14.759920475195402 and b is -70.02322140399295\n",
      "Iteration 1183, the loss is 5.161023907527075, parameters k is 14.760054684681569 and b is -70.0232411668388\n",
      "Iteration 1184, the loss is 5.161026560034612, parameters k is 14.75957901274481 and b is -70.02326092968465\n",
      "Iteration 1185, the loss is 5.161025618070161, parameters k is 14.759713222230976 and b is -70.0232806925305\n",
      "Iteration 1186, the loss is 5.161024676105712, parameters k is 14.759847431717143 and b is -70.02330045537636\n",
      "Iteration 1187, the loss is 5.161024287782895, parameters k is 14.759981641203309 and b is -70.02332021822221\n",
      "Iteration 1188, the loss is 5.1610268290708365, parameters k is 14.75981506017564 and b is -70.02333998106806\n",
      "Iteration 1189, the loss is 5.16102588710639, parameters k is 14.759949269661806 and b is -70.02335974391391\n",
      "Iteration 1190, the loss is 5.161030198381992, parameters k is 14.760083479147973 and b is -70.02337950675977\n",
      "Iteration 1191, the loss is 5.161032188364732, parameters k is 14.759607807211214 and b is -70.02339926960562\n",
      "Iteration 1192, the loss is 5.161031246400274, parameters k is 14.75974201669738 and b is -70.02341903245147\n",
      "Iteration 1193, the loss is 5.161030304435828, parameters k is 14.759876226183547 and b is -70.02343879529732\n",
      "Iteration 1194, the loss is 5.161030235425148, parameters k is 14.760010435669713 and b is -70.02345855814318\n",
      "Iteration 1195, the loss is 5.1610324574009585, parameters k is 14.759843854642044 and b is -70.02347832098903\n",
      "Iteration 1196, the loss is 5.161031515436504, parameters k is 14.75997806412821 and b is -70.02349808383488\n",
      "Iteration 1197, the loss is 5.161036489236917, parameters k is 14.760112273614377 and b is -70.02351784668073\n",
      "Iteration 1198, the loss is 5.161037816694848, parameters k is 14.759636601677618 and b is -70.02353760952658\n",
      "Iteration 1199, the loss is 5.161036874730396, parameters k is 14.759770811163785 and b is -70.02355737237244\n",
      "Iteration 1200, the loss is 5.1610359327659445, parameters k is 14.759905020649951 and b is -70.02357713521829\n",
      "Iteration 1201, the loss is 5.161036183067397, parameters k is 14.760039230136117 and b is -70.02359689806414\n",
      "Iteration 1202, the loss is 5.1610380857310725, parameters k is 14.759872649108448 and b is -70.02361666091\n",
      "Iteration 1203, the loss is 5.161037206099232, parameters k is 14.760006858594615 and b is -70.02363642375585\n",
      "Iteration 1204, the loss is 5.161040238696207, parameters k is 14.759840277566946 and b is -70.0236561866017\n",
      "Iteration 1205, the loss is 5.161039296731755, parameters k is 14.759974487053112 and b is -70.02367594944755\n",
      "Iteration 1206, the loss is 5.161042646320722, parameters k is 14.760108696539278 and b is -70.0236957122934\n",
      "Iteration 1207, the loss is 5.161045597990091, parameters k is 14.75963302460252 and b is -70.02371547513926\n",
      "Iteration 1208, the loss is 5.161044656025645, parameters k is 14.759767234088686 and b is -70.02373523798511\n",
      "Iteration 1209, the loss is 5.161043714061195, parameters k is 14.759901443574853 and b is -70.02375500083096\n",
      "Iteration 1210, the loss is 5.161043153741482, parameters k is 14.760035653061019 and b is -70.02377476367681\n",
      "Iteration 1211, the loss is 5.161045867026329, parameters k is 14.75986907203335 and b is -70.02379452652266\n",
      "Iteration 1212, the loss is 5.161044925061874, parameters k is 14.760003281519516 and b is -70.02381428936852\n",
      "Iteration 1213, the loss is 5.161048937175641, parameters k is 14.760137491005683 and b is -70.02383405221437\n",
      "Iteration 1214, the loss is 5.161051226320213, parameters k is 14.759661819068924 and b is -70.02385381506022\n",
      "Iteration 1215, the loss is 5.1610502843557615, parameters k is 14.75979602855509 and b is -70.02387357790607\n",
      "Iteration 1216, the loss is 5.1610493423913155, parameters k is 14.759930238041257 and b is -70.02389334075193\n",
      "Iteration 1217, the loss is 5.161049101383725, parameters k is 14.760064447527423 and b is -70.02391310359778\n",
      "Iteration 1218, the loss is 5.1610514953564435, parameters k is 14.759897866499754 and b is -70.02393286644363\n",
      "Iteration 1219, the loss is 5.161050553391991, parameters k is 14.76003207598592 and b is -70.02395262928948\n",
      "Iteration 1220, the loss is 5.161055228030565, parameters k is 14.760166285472087 and b is -70.02397239213533\n",
      "Iteration 1221, the loss is 5.161056854650328, parameters k is 14.759690613535328 and b is -70.02399215498119\n",
      "Iteration 1222, the loss is 5.161055912685879, parameters k is 14.759824823021495 and b is -70.02401191782704\n",
      "Iteration 1223, the loss is 5.161054970721431, parameters k is 14.75995903250766 and b is -70.02403168067289\n",
      "Iteration 1224, the loss is 5.1610550490259675, parameters k is 14.760093241993827 and b is -70.02405144351874\n",
      "Iteration 1225, the loss is 5.161057123686563, parameters k is 14.759926660966158 and b is -70.0240712063646\n",
      "Iteration 1226, the loss is 5.16105618172211, parameters k is 14.760060870452325 and b is -70.02409096921045\n",
      "Iteration 1227, the loss is 5.161061518885481, parameters k is 14.760195079938491 and b is -70.0241107320563\n",
      "Iteration 1228, the loss is 5.161062482980449, parameters k is 14.759719408001732 and b is -70.02413049490215\n",
      "Iteration 1229, the loss is 5.161061541016002, parameters k is 14.759853617487899 and b is -70.024150257748\n",
      "Iteration 1230, the loss is 5.161060599051547, parameters k is 14.759987826974065 and b is -70.02417002059386\n",
      "Iteration 1231, the loss is 5.161060996668223, parameters k is 14.760122036460231 and b is -70.02418978343971\n",
      "Iteration 1232, the loss is 5.1610627520166785, parameters k is 14.759955455432562 and b is -70.02420954628556\n",
      "Iteration 1233, the loss is 5.161062019700058, parameters k is 14.760089664918729 and b is -70.02422930913141\n",
      "Iteration 1234, the loss is 5.161064904981807, parameters k is 14.75992308389106 and b is -70.02424907197727\n",
      "Iteration 1235, the loss is 5.161063963017362, parameters k is 14.760057293377226 and b is -70.02426883482312\n",
      "Iteration 1236, the loss is 5.161067675969289, parameters k is 14.760191502863393 and b is -70.02428859766897\n",
      "Iteration 1237, the loss is 5.161070264275698, parameters k is 14.759715830926634 and b is -70.02430836051482\n",
      "Iteration 1238, the loss is 5.161069322311241, parameters k is 14.7598500404128 and b is -70.02432812336068\n",
      "Iteration 1239, the loss is 5.161068380346796, parameters k is 14.759984249898967 and b is -70.02434788620653\n",
      "Iteration 1240, the loss is 5.161067967342302, parameters k is 14.760118459385133 and b is -70.02436764905238\n",
      "Iteration 1241, the loss is 5.1610705333119276, parameters k is 14.759951878357464 and b is -70.02438741189823\n",
      "Iteration 1242, the loss is 5.161069591347477, parameters k is 14.76008608784363 and b is -70.02440717474408\n",
      "Iteration 1243, the loss is 5.161073966824211, parameters k is 14.760220297329797 and b is -70.02442693758994\n",
      "Iteration 1244, the loss is 5.161075892605818, parameters k is 14.759744625393038 and b is -70.02444670043579\n",
      "Iteration 1245, the loss is 5.16107495064136, parameters k is 14.759878834879204 and b is -70.02446646328164\n",
      "Iteration 1246, the loss is 5.161074008676912, parameters k is 14.76001304436537 and b is -70.0244862261275\n",
      "Iteration 1247, the loss is 5.161073914984552, parameters k is 14.760147253851537 and b is -70.02450598897335\n",
      "Iteration 1248, the loss is 5.16107616164205, parameters k is 14.759980672823868 and b is -70.0245257518192\n",
      "Iteration 1249, the loss is 5.161075219677598, parameters k is 14.760114882310035 and b is -70.02454551466505\n",
      "Iteration 1250, the loss is 5.161080257679128, parameters k is 14.7602490917962 and b is -70.0245652775109\n",
      "Iteration 1251, the loss is 5.161081520935936, parameters k is 14.759773419859442 and b is -70.02458504035675\n",
      "Iteration 1252, the loss is 5.1610805789714895, parameters k is 14.759907629345609 and b is -70.0246048032026\n",
      "Iteration 1253, the loss is 5.1610796370070355, parameters k is 14.760041838831775 and b is -70.02462456604846\n",
      "Iteration 1254, the loss is 5.161079862626797, parameters k is 14.760176048317941 and b is -70.02464432889431\n",
      "Iteration 1255, the loss is 5.161081789972166, parameters k is 14.760009467290272 and b is -70.02466409174016\n",
      "Iteration 1256, the loss is 5.161080885658637, parameters k is 14.760143676776439 and b is -70.02468385458602\n",
      "Iteration 1257, the loss is 5.161083942937296, parameters k is 14.75997709574877 and b is -70.02470361743187\n",
      "Iteration 1258, the loss is 5.161083000972844, parameters k is 14.760111305234936 and b is -70.02472338027772\n",
      "Iteration 1259, the loss is 5.161086414762939, parameters k is 14.760245514721102 and b is -70.02474314312357\n",
      "Iteration 1260, the loss is 5.161089302231185, parameters k is 14.759769842784344 and b is -70.02476290596942\n",
      "Iteration 1261, the loss is 5.161088360266739, parameters k is 14.75990405227051 and b is -70.02478266881528\n",
      "Iteration 1262, the loss is 5.161087418302285, parameters k is 14.760038261756677 and b is -70.02480243166113\n",
      "Iteration 1263, the loss is 5.161086833300877, parameters k is 14.760172471242843 and b is -70.02482219450698\n",
      "Iteration 1264, the loss is 5.161089571267415, parameters k is 14.760005890215174 and b is -70.02484195735283\n",
      "Iteration 1265, the loss is 5.1610886293029665, parameters k is 14.76014009970134 and b is -70.02486172019869\n",
      "Iteration 1266, the loss is 5.161092705617853, parameters k is 14.760274309187507 and b is -70.02488148304454\n",
      "Iteration 1267, the loss is 5.1610949305613, parameters k is 14.759798637250748 and b is -70.02490124589039\n",
      "Iteration 1268, the loss is 5.1610939885968525, parameters k is 14.759932846736914 and b is -70.02492100873624\n",
      "Iteration 1269, the loss is 5.161093046632403, parameters k is 14.76006705622308 and b is -70.0249407715821\n",
      "Iteration 1270, the loss is 5.161092780943127, parameters k is 14.760201265709247 and b is -70.02496053442795\n",
      "Iteration 1271, the loss is 5.161095199597534, parameters k is 14.760034684681578 and b is -70.0249802972738\n",
      "Iteration 1272, the loss is 5.161094257633083, parameters k is 14.760168894167744 and b is -70.02500006011965\n",
      "Iteration 1273, the loss is 5.161098996472775, parameters k is 14.76030310365391 and b is -70.0250198229655\n",
      "Iteration 1274, the loss is 5.161100558891424, parameters k is 14.759827431717152 and b is -70.02503958581136\n",
      "Iteration 1275, the loss is 5.161099616926969, parameters k is 14.759961641203319 and b is -70.02505934865721\n",
      "Iteration 1276, the loss is 5.161098674962523, parameters k is 14.760095850689485 and b is -70.02507911150306\n",
      "Iteration 1277, the loss is 5.161098728585377, parameters k is 14.760230060175651 and b is -70.02509887434891\n",
      "Iteration 1278, the loss is 5.161100827927648, parameters k is 14.760063479147982 and b is -70.02511863719477\n",
      "Iteration 1279, the loss is 5.161099885963204, parameters k is 14.760197688634149 and b is -70.02513840004062\n",
      "Iteration 1280, the loss is 5.161105287327698, parameters k is 14.760331898120315 and b is -70.02515816288647\n",
      "Iteration 1281, the loss is 5.161106187221538, parameters k is 14.759856226183556 and b is -70.02517792573232\n",
      "Iteration 1282, the loss is 5.161105245257087, parameters k is 14.759990435669723 and b is -70.02519768857817\n",
      "Iteration 1283, the loss is 5.161104303292638, parameters k is 14.760124645155889 and b is -70.02521745142403\n",
      "Iteration 1284, the loss is 5.16110467622762, parameters k is 14.760258854642055 and b is -70.02523721426988\n",
      "Iteration 1285, the loss is 5.161106456257777, parameters k is 14.760092273614386 and b is -70.02525697711573\n",
      "Iteration 1286, the loss is 5.161105699259459, parameters k is 14.760226483100553 and b is -70.02527673996158\n",
      "Iteration 1287, the loss is 5.161108609222899, parameters k is 14.760059902072884 and b is -70.02529650280744\n",
      "Iteration 1288, the loss is 5.161107667258442, parameters k is 14.76019411155905 and b is -70.02531626565329\n",
      "Iteration 1289, the loss is 5.161111444411506, parameters k is 14.760328321045217 and b is -70.02533602849914\n",
      "Iteration 1290, the loss is 5.161113968516784, parameters k is 14.759852649108458 and b is -70.02535579134499\n",
      "Iteration 1291, the loss is 5.161113026552333, parameters k is 14.759986858594624 and b is -70.02537555419084\n",
      "Iteration 1292, the loss is 5.161112084587887, parameters k is 14.76012106808079 and b is -70.0253953170367\n",
      "Iteration 1293, the loss is 5.161111646901706, parameters k is 14.760255277566957 and b is -70.02541507988255\n",
      "Iteration 1294, the loss is 5.161114237553014, parameters k is 14.760088696539288 and b is -70.0254348427284\n",
      "Iteration 1295, the loss is 5.16111329558857, parameters k is 14.760222906025454 and b is -70.02545460557425\n",
      "Iteration 1296, the loss is 5.161117735266419, parameters k is 14.76035711551162 and b is -70.0254743684201\n",
      "Iteration 1297, the loss is 5.161119596846905, parameters k is 14.759881443574862 and b is -70.02549413126596\n",
      "Iteration 1298, the loss is 5.161118654882453, parameters k is 14.760015653061028 and b is -70.02551389411181\n",
      "Iteration 1299, the loss is 5.161117712918006, parameters k is 14.760149862547195 and b is -70.02553365695766\n",
      "Iteration 1300, the loss is 5.161117594543954, parameters k is 14.760284072033361 and b is -70.02555341980351\n",
      "Iteration 1301, the loss is 5.161119865883134, parameters k is 14.760117491005692 and b is -70.02557318264937\n",
      "Iteration 1302, the loss is 5.161118923918685, parameters k is 14.760251700491859 and b is -70.02559294549522\n",
      "Iteration 1303, the loss is 5.161124026121344, parameters k is 14.760385909978025 and b is -70.02561270834107\n",
      "Iteration 1304, the loss is 5.1611252251770185, parameters k is 14.759910238041266 and b is -70.02563247118692\n",
      "Iteration 1305, the loss is 5.161124283212571, parameters k is 14.760044447527433 and b is -70.02565223403278\n",
      "Iteration 1306, the loss is 5.161123341248126, parameters k is 14.760178657013599 and b is -70.02567199687863\n",
      "Iteration 1307, the loss is 5.1611235421862025, parameters k is 14.760312866499765 and b is -70.02569175972448\n",
      "Iteration 1308, the loss is 5.161125494213257, parameters k is 14.760146285472096 and b is -70.02571152257033\n",
      "Iteration 1309, the loss is 5.1611245652180395, parameters k is 14.760280494958263 and b is -70.02573128541619\n",
      "Iteration 1310, the loss is 5.161127647178384, parameters k is 14.760113913930594 and b is -70.02575104826204\n",
      "Iteration 1311, the loss is 5.161126705213933, parameters k is 14.76024812341676 and b is -70.02577081110789\n",
      "Iteration 1312, the loss is 5.161130183205151, parameters k is 14.760382332902926 and b is -70.02579057395374\n",
      "Iteration 1313, the loss is 5.16113300647227, parameters k is 14.759906660966168 and b is -70.0258103367996\n",
      "Iteration 1314, the loss is 5.161132064507822, parameters k is 14.760040870452334 and b is -70.02583009964545\n",
      "Iteration 1315, the loss is 5.161131122543371, parameters k is 14.7601750799385 and b is -70.0258498624913\n",
      "Iteration 1316, the loss is 5.161130512860285, parameters k is 14.760309289424667 and b is -70.02586962533715\n",
      "Iteration 1317, the loss is 5.161133275508501, parameters k is 14.760142708396998 and b is -70.025889388183\n",
      "Iteration 1318, the loss is 5.161132333544055, parameters k is 14.760276917883164 and b is -70.02590915102886\n",
      "Iteration 1319, the loss is 5.1611364740600765, parameters k is 14.76041112736933 and b is -70.02592891387471\n",
      "Iteration 1320, the loss is 5.1611386348023895, parameters k is 14.759935455432572 and b is -70.02594867672056\n",
      "Iteration 1321, the loss is 5.161137692837938, parameters k is 14.760069664918738 and b is -70.02596843956641\n",
      "Iteration 1322, the loss is 5.161136750873492, parameters k is 14.760203874404905 and b is -70.02598820241226\n",
      "Iteration 1323, the loss is 5.16113646050253, parameters k is 14.760338083891071 and b is -70.02600796525812\n",
      "Iteration 1324, the loss is 5.161138903838618, parameters k is 14.760171502863402 and b is -70.02602772810397\n",
      "Iteration 1325, the loss is 5.161137961874171, parameters k is 14.760305712349568 and b is -70.02604749094982\n",
      "Iteration 1326, the loss is 5.161142764914996, parameters k is 14.760439921835735 and b is -70.02606725379567\n",
      "Iteration 1327, the loss is 5.161144263132509, parameters k is 14.759964249898976 and b is -70.02608701664153\n",
      "Iteration 1328, the loss is 5.161143321168059, parameters k is 14.760098459385143 and b is -70.02610677948738\n",
      "Iteration 1329, the loss is 5.16114237920361, parameters k is 14.760232668871309 and b is -70.02612654233323\n",
      "Iteration 1330, the loss is 5.161142408144782, parameters k is 14.760366878357475 and b is -70.02614630517908\n",
      "Iteration 1331, the loss is 5.161144532168736, parameters k is 14.760200297329806 and b is -70.02616606802493\n",
      "Iteration 1332, the loss is 5.161143590204289, parameters k is 14.760334506815973 and b is -70.02618583087079\n",
      "Iteration 1333, the loss is 5.161149055769909, parameters k is 14.760468716302139 and b is -70.02620559371664\n",
      "Iteration 1334, the loss is 5.161149891462628, parameters k is 14.75999304436538 and b is -70.02622535656249\n",
      "Iteration 1335, the loss is 5.1611489494981795, parameters k is 14.760127253851547 and b is -70.02624511940834\n",
      "Iteration 1336, the loss is 5.161148007533724, parameters k is 14.760261463337713 and b is -70.0262648822542\n",
      "Iteration 1337, the loss is 5.161148393401184, parameters k is 14.76039567282388 and b is -70.02628464510005\n",
      "Iteration 1338, the loss is 5.161154308792066, parameters k is 14.75992000088712 and b is -70.0263044079459\n",
      "Iteration 1339, the loss is 5.161153366827621, parameters k is 14.760054210373287 and b is -70.02632417079175\n",
      "Iteration 1340, the loss is 5.161152424863171, parameters k is 14.760188419859453 and b is -70.0263439336376\n",
      "Iteration 1341, the loss is 5.161151482898717, parameters k is 14.76032262934562 and b is -70.02636369648346\n",
      "Iteration 1342, the loss is 5.16115481802721, parameters k is 14.760456838831786 and b is -70.02638345932931\n",
      "Iteration 1343, the loss is 5.161157784157057, parameters k is 14.759981166895027 and b is -70.02640322217516\n",
      "Iteration 1344, the loss is 5.161156842192601, parameters k is 14.760115376381194 and b is -70.02642298502101\n",
      "Iteration 1345, the loss is 5.16115590022815, parameters k is 14.76024958586736 and b is -70.02644274786687\n",
      "Iteration 1346, the loss is 5.161155188192269, parameters k is 14.760383795353526 and b is -70.02646251071272\n",
      "Iteration 1347, the loss is 5.1611580531932795, parameters k is 14.760217214325857 and b is -70.02648227355857\n",
      "Iteration 1348, the loss is 5.161157111228834, parameters k is 14.760351423812024 and b is -70.02650203640442\n",
      "Iteration 1349, the loss is 5.161161108882133, parameters k is 14.76048563329819 and b is -70.02652179925028\n",
      "Iteration 1350, the loss is 5.161163412487179, parameters k is 14.760009961361432 and b is -70.02654156209613\n",
      "Iteration 1351, the loss is 5.1611624705227195, parameters k is 14.760144170847598 and b is -70.02656132494198\n",
      "Iteration 1352, the loss is 5.161161528558271, parameters k is 14.760278380333764 and b is -70.02658108778783\n",
      "Iteration 1353, the loss is 5.161161135834523, parameters k is 14.76041258981993 and b is -70.02660085063368\n",
      "Iteration 1354, the loss is 5.161163681523401, parameters k is 14.760246008792262 and b is -70.02662061347954\n",
      "Iteration 1355, the loss is 5.161162739558951, parameters k is 14.760380218278428 and b is -70.02664037632539\n",
      "Iteration 1356, the loss is 5.16116739973705, parameters k is 14.760514427764594 and b is -70.02666013917124\n",
      "Iteration 1357, the loss is 5.161169040817292, parameters k is 14.760038755827836 and b is -70.0266799020171\n",
      "Iteration 1358, the loss is 5.1611680988528414, parameters k is 14.760172965314002 and b is -70.02669966486295\n",
      "Iteration 1359, the loss is 5.161167156888388, parameters k is 14.760307174800168 and b is -70.0267194277088\n",
      "Iteration 1360, the loss is 5.161167083476762, parameters k is 14.760441384286334 and b is -70.02673919055465\n",
      "Iteration 1361, the loss is 5.16116930985353, parameters k is 14.760274803258666 and b is -70.0267589534005\n",
      "Iteration 1362, the loss is 5.16116836788907, parameters k is 14.760409012744832 and b is -70.02677871624635\n",
      "Iteration 1363, the loss is 5.161173690591971, parameters k is 14.760543222230998 and b is -70.02679847909221\n",
      "Iteration 1364, the loss is 5.161174669147413, parameters k is 14.76006755029424 and b is -70.02681824193806\n",
      "Iteration 1365, the loss is 5.16117372718296, parameters k is 14.760201759780406 and b is -70.02683800478391\n",
      "Iteration 1366, the loss is 5.16117278521851, parameters k is 14.760335969266572 and b is -70.02685776762976\n",
      "Iteration 1367, the loss is 5.161173031119011, parameters k is 14.760470178752739 and b is -70.02687753047562\n",
      "Iteration 1368, the loss is 5.161174938183638, parameters k is 14.76030359772507 and b is -70.02689729332147\n",
      "Iteration 1369, the loss is 5.1611740541508535, parameters k is 14.760437807211236 and b is -70.02691705616732\n",
      "Iteration 1370, the loss is 5.161177091148764, parameters k is 14.760271226183567 and b is -70.02693681901317\n",
      "Iteration 1371, the loss is 5.161176149184322, parameters k is 14.760405435669734 and b is -70.02695658185903\n",
      "Iteration 1372, the loss is 5.161179847675785, parameters k is 14.7605396451559 and b is -70.02697634470488\n",
      "Iteration 1373, the loss is 5.161182450442657, parameters k is 14.760063973219141 and b is -70.02699610755073\n",
      "Iteration 1374, the loss is 5.161181508478206, parameters k is 14.760198182705308 and b is -70.02701587039658\n",
      "Iteration 1375, the loss is 5.1611805665137584, parameters k is 14.760332392191474 and b is -70.02703563324243\n",
      "Iteration 1376, the loss is 5.161180001793099, parameters k is 14.76046660167764 and b is -70.02705539608829\n",
      "Iteration 1377, the loss is 5.161182719478885, parameters k is 14.760300020649971 and b is -70.02707515893414\n",
      "Iteration 1378, the loss is 5.161181777514438, parameters k is 14.760434230136138 and b is -70.02709492177999\n",
      "Iteration 1379, the loss is 5.161186138530699, parameters k is 14.760568439622304 and b is -70.02711468462584\n",
      "Iteration 1380, the loss is 5.1611880787727715, parameters k is 14.760092767685546 and b is -70.0271344474717\n",
      "Iteration 1381, the loss is 5.161187136808325, parameters k is 14.760226977171712 and b is -70.02715421031755\n",
      "Iteration 1382, the loss is 5.161186194843875, parameters k is 14.760361186657878 and b is -70.0271739731634\n",
      "Iteration 1383, the loss is 5.161185949435346, parameters k is 14.760495396144044 and b is -70.02719373600925\n",
      "Iteration 1384, the loss is 5.1611883478090075, parameters k is 14.760328815116376 and b is -70.0272134988551\n",
      "Iteration 1385, the loss is 5.161187405844553, parameters k is 14.760463024602542 and b is -70.02723326170096\n",
      "Iteration 1386, the loss is 5.1611924293856175, parameters k is 14.760597234088708 and b is -70.02725302454681\n",
      "Iteration 1387, the loss is 5.161193707102896, parameters k is 14.76012156215195 and b is -70.02727278739266\n",
      "Iteration 1388, the loss is 5.161192765138448, parameters k is 14.760255771638116 and b is -70.02729255023851\n",
      "Iteration 1389, the loss is 5.161191823173997, parameters k is 14.760389981124282 and b is -70.02731231308437\n",
      "Iteration 1390, the loss is 5.161191897077588, parameters k is 14.760524190610449 and b is -70.02733207593022\n",
      "Iteration 1391, the loss is 5.161193976139121, parameters k is 14.76035760958278 and b is -70.02735183877607\n",
      "Iteration 1392, the loss is 5.161193034174674, parameters k is 14.760491819068946 and b is -70.02737160162192\n",
      "Iteration 1393, the loss is 5.161198720240542, parameters k is 14.760626028555112 and b is -70.02739136446777\n",
      "Iteration 1394, the loss is 5.161199335433012, parameters k is 14.760150356618354 and b is -70.02741112731363\n",
      "Iteration 1395, the loss is 5.161198393468562, parameters k is 14.76028456610452 and b is -70.02743089015948\n",
      "Iteration 1396, the loss is 5.16119745150411, parameters k is 14.760418775590686 and b is -70.02745065300533\n",
      "Iteration 1397, the loss is 5.161198057871812, parameters k is 14.760552985076853 and b is -70.02747041585118\n",
      "Iteration 1398, the loss is 5.1612037527624555, parameters k is 14.760077313140094 and b is -70.02749017869704\n",
      "Iteration 1399, the loss is 5.161202810798001, parameters k is 14.76021152262626 and b is -70.02750994154289\n",
      "Iteration 1400, the loss is 5.161201868833551, parameters k is 14.760345732112427 and b is -70.02752970438874\n",
      "Iteration 1401, the loss is 5.161200926869105, parameters k is 14.760479941598593 and b is -70.02754946723459\n",
      "Iteration 1402, the loss is 5.161204482497842, parameters k is 14.76061415108476 and b is -70.02756923008045\n",
      "Iteration 1403, the loss is 5.161207228127441, parameters k is 14.760138479148 and b is -70.0275889929263\n",
      "Iteration 1404, the loss is 5.161206286162992, parameters k is 14.760272688634167 and b is -70.02760875577215\n",
      "Iteration 1405, the loss is 5.1612053441985415, parameters k is 14.760406898120333 and b is -70.027628518618\n",
      "Iteration 1406, the loss is 5.161204677125085, parameters k is 14.7605411076065 and b is -70.02764828146385\n",
      "Iteration 1407, the loss is 5.161207497163669, parameters k is 14.76037452657883 and b is -70.0276680443097\n",
      "Iteration 1408, the loss is 5.161206555199222, parameters k is 14.760508736064997 and b is -70.02768780715556\n",
      "Iteration 1409, the loss is 5.161210773352766, parameters k is 14.760642945551163 and b is -70.02770757000141\n",
      "Iteration 1410, the loss is 5.16121285645756, parameters k is 14.760167273614405 and b is -70.02772733284726\n",
      "Iteration 1411, the loss is 5.161211914493111, parameters k is 14.760301483100571 and b is -70.02774709569312\n",
      "Iteration 1412, the loss is 5.161210972528657, parameters k is 14.760435692586737 and b is -70.02776685853897\n",
      "Iteration 1413, the loss is 5.1612106247673335, parameters k is 14.760569902072904 and b is -70.02778662138482\n",
      "Iteration 1414, the loss is 5.16121312549379, parameters k is 14.760403321045235 and b is -70.02780638423067\n",
      "Iteration 1415, the loss is 5.161212183529338, parameters k is 14.760537530531401 and b is -70.02782614707652\n",
      "Iteration 1416, the loss is 5.161217064207677, parameters k is 14.760671740017568 and b is -70.02784590992238\n",
      "Iteration 1417, the loss is 5.161218484787679, parameters k is 14.760196068080809 and b is -70.02786567276823\n",
      "Iteration 1418, the loss is 5.161217542823229, parameters k is 14.760330277566975 and b is -70.02788543561408\n",
      "Iteration 1419, the loss is 5.161216600858777, parameters k is 14.760464487053142 and b is -70.02790519845993\n",
      "Iteration 1420, the loss is 5.16121657240958, parameters k is 14.760598696539308 and b is -70.02792496130579\n",
      "Iteration 1421, the loss is 5.1612187538239045, parameters k is 14.76043211551164 and b is -70.02794472415164\n",
      "Iteration 1422, the loss is 5.1612178118594585, parameters k is 14.760566324997805 and b is -70.02796448699749\n",
      "Iteration 1423, the loss is 5.1612233550625986, parameters k is 14.760700534483972 and b is -70.02798424984334\n",
      "Iteration 1424, the loss is 5.161224113117787, parameters k is 14.760224862547213 and b is -70.0280040126892\n",
      "Iteration 1425, the loss is 5.161223171153339, parameters k is 14.76035907203338 and b is -70.02802377553505\n",
      "Iteration 1426, the loss is 5.161222229188893, parameters k is 14.760493281519546 and b is -70.0280435383809\n",
      "Iteration 1427, the loss is 5.161222692693869, parameters k is 14.760627491005712 and b is -70.02806330122675\n",
      "Iteration 1428, the loss is 5.161228530447236, parameters k is 14.760151819068954 and b is -70.0280830640726\n",
      "Iteration 1429, the loss is 5.161227588482784, parameters k is 14.76028602855512 and b is -70.02810282691846\n",
      "Iteration 1430, the loss is 5.161226646518329, parameters k is 14.760420238041286 and b is -70.02812258976431\n",
      "Iteration 1431, the loss is 5.161225704553886, parameters k is 14.760554447527452 and b is -70.02814235261016\n",
      "Iteration 1432, the loss is 5.161229117319899, parameters k is 14.760688657013619 and b is -70.02816211545601\n",
      "Iteration 1433, the loss is 5.161232005812221, parameters k is 14.76021298507686 and b is -70.02818187830187\n",
      "Iteration 1434, the loss is 5.161231063847774, parameters k is 14.760347194563026 and b is -70.02820164114772\n",
      "Iteration 1435, the loss is 5.161230121883324, parameters k is 14.760481404049193 and b is -70.02822140399357\n",
      "Iteration 1436, the loss is 5.161229352457065, parameters k is 14.760615613535359 and b is -70.02824116683942\n",
      "Iteration 1437, the loss is 5.161232274848456, parameters k is 14.76044903250769 and b is -70.02826092968527\n",
      "Iteration 1438, the loss is 5.161231332884004, parameters k is 14.760583241993857 and b is -70.02828069253113\n",
      "Iteration 1439, the loss is 5.161235408174821, parameters k is 14.760717451480023 and b is -70.02830045537698\n",
      "Iteration 1440, the loss is 5.161237634142342, parameters k is 14.760241779543264 and b is -70.02832021822283\n",
      "Iteration 1441, the loss is 5.16123669217789, parameters k is 14.76037598902943 and b is -70.02833998106868\n",
      "Iteration 1442, the loss is 5.161235750213443, parameters k is 14.760510198515597 and b is -70.02835974391454\n",
      "Iteration 1443, the loss is 5.16123530009932, parameters k is 14.760644408001763 and b is -70.02837950676039\n",
      "Iteration 1444, the loss is 5.161237903178574, parameters k is 14.760477826974094 and b is -70.02839926960624\n",
      "Iteration 1445, the loss is 5.161236961214121, parameters k is 14.76061203646026 and b is -70.02841903245209\n",
      "Iteration 1446, the loss is 5.161241699029737, parameters k is 14.760746245946427 and b is -70.02843879529794\n",
      "Iteration 1447, the loss is 5.161243262472459, parameters k is 14.760270574009668 and b is -70.0284585581438\n",
      "Iteration 1448, the loss is 5.161242320508006, parameters k is 14.760404783495835 and b is -70.02847832098965\n",
      "Iteration 1449, the loss is 5.1612413785435525, parameters k is 14.760538992982001 and b is -70.0284980838355\n",
      "Iteration 1450, the loss is 5.161241247741561, parameters k is 14.760673202468167 and b is -70.02851784668135\n",
      "Iteration 1451, the loss is 5.161243531508691, parameters k is 14.760506621440499 and b is -70.0285376095272\n",
      "Iteration 1452, the loss is 5.161242589544238, parameters k is 14.760640830926665 and b is -70.02855737237306\n",
      "Iteration 1453, the loss is 5.161247989884664, parameters k is 14.760775040412831 and b is -70.02857713521891\n",
      "Iteration 1454, the loss is 5.161248890802579, parameters k is 14.760299368476073 and b is -70.02859689806476\n",
      "Iteration 1455, the loss is 5.161247948838135, parameters k is 14.760433577962239 and b is -70.02861666091061\n",
      "Iteration 1456, the loss is 5.161247006873674, parameters k is 14.760567787448405 and b is -70.02863642375647\n",
      "Iteration 1457, the loss is 5.161247327515928, parameters k is 14.760701996934571 and b is -70.02865618660232\n",
      "Iteration 1458, the loss is 5.1612533081320136, parameters k is 14.760226324997813 and b is -70.02867594944817\n",
      "Iteration 1459, the loss is 5.1612523661675676, parameters k is 14.76036053448398 and b is -70.02869571229402\n",
      "Iteration 1460, the loss is 5.1612514242031144, parameters k is 14.760494743970145 and b is -70.02871547513988\n",
      "Iteration 1461, the loss is 5.1612504822386684, parameters k is 14.760628953456312 and b is -70.02873523798573\n",
      "Iteration 1462, the loss is 5.161253752141958, parameters k is 14.760763162942478 and b is -70.02875500083158\n",
      "Iteration 1463, the loss is 5.161256783497004, parameters k is 14.76028749100572 and b is -70.02877476367743\n",
      "Iteration 1464, the loss is 5.161255841532554, parameters k is 14.760421700491886 and b is -70.02879452652328\n",
      "Iteration 1465, the loss is 5.161254899568107, parameters k is 14.760555909978052 and b is -70.02881428936914\n",
      "Iteration 1466, the loss is 5.1612540277890595, parameters k is 14.760690119464218 and b is -70.02883405221499\n",
      "Iteration 1467, the loss is 5.161257052533234, parameters k is 14.76052353843655 and b is -70.02885381506084\n",
      "Iteration 1468, the loss is 5.161256110568784, parameters k is 14.760657747922716 and b is -70.0288735779067\n",
      "Iteration 1469, the loss is 5.16126004299688, parameters k is 14.760791957408882 and b is -70.02889334075255\n",
      "Iteration 1470, the loss is 5.161262411827122, parameters k is 14.760316285472124 and b is -70.0289131035984\n",
      "Iteration 1471, the loss is 5.1612614698626755, parameters k is 14.76045049495829 and b is -70.02893286644425\n",
      "Iteration 1472, the loss is 5.161260527898219, parameters k is 14.760584704444456 and b is -70.0289526292901\n",
      "Iteration 1473, the loss is 5.1612599754313, parameters k is 14.760718913930623 and b is -70.02897239213596\n",
      "Iteration 1474, the loss is 5.161262680863351, parameters k is 14.760552332902954 and b is -70.02899215498181\n",
      "Iteration 1475, the loss is 5.1612617388988955, parameters k is 14.76068654238912 and b is -70.02901191782766\n",
      "Iteration 1476, the loss is 5.161266333851798, parameters k is 14.760820751875286 and b is -70.02903168067351\n",
      "Iteration 1477, the loss is 5.161268040157239, parameters k is 14.760345079938528 and b is -70.02905144351936\n",
      "Iteration 1478, the loss is 5.161267098192794, parameters k is 14.760479289424694 and b is -70.02907120636522\n",
      "Iteration 1479, the loss is 5.161266156228343, parameters k is 14.76061349891086 and b is -70.02909096921107\n",
      "Iteration 1480, the loss is 5.161265923073552, parameters k is 14.760747708397027 and b is -70.02911073205692\n",
      "Iteration 1481, the loss is 5.161268309193473, parameters k is 14.760581127369358 and b is -70.02913049490277\n",
      "Iteration 1482, the loss is 5.161267367229025, parameters k is 14.760715336855524 and b is -70.02915025774863\n",
      "Iteration 1483, the loss is 5.161272624706713, parameters k is 14.76084954634169 and b is -70.02917002059448\n",
      "Iteration 1484, the loss is 5.16127366848736, parameters k is 14.760373874404932 and b is -70.02918978344033\n",
      "Iteration 1485, the loss is 5.161272726522908, parameters k is 14.760508083891098 and b is -70.02920954628618\n",
      "Iteration 1486, the loss is 5.161271784558461, parameters k is 14.760642293377265 and b is -70.02922930913203\n",
      "Iteration 1487, the loss is 5.161271962337988, parameters k is 14.76077650286343 and b is -70.02924907197789\n",
      "Iteration 1488, the loss is 5.161278085816796, parameters k is 14.760300830926672 and b is -70.02926883482374\n",
      "Iteration 1489, the loss is 5.161277143852343, parameters k is 14.760435040412839 and b is -70.02928859766959\n",
      "Iteration 1490, the loss is 5.161276201887899, parameters k is 14.760569249899005 and b is -70.02930836051544\n",
      "Iteration 1491, the loss is 5.161275259923449, parameters k is 14.760703459385171 and b is -70.0293281233613\n",
      "Iteration 1492, the loss is 5.161278386964018, parameters k is 14.760837668871337 and b is -70.02934788620715\n",
      "Iteration 1493, the loss is 5.161281561181788, parameters k is 14.760361996934579 and b is -70.029367649053\n",
      "Iteration 1494, the loss is 5.161280619217336, parameters k is 14.760496206420745 and b is -70.02938741189885\n",
      "Iteration 1495, the loss is 5.161279677252884, parameters k is 14.760630415906911 and b is -70.0294071747447\n",
      "Iteration 1496, the loss is 5.161278735288436, parameters k is 14.760764625393078 and b is -70.02942693759056\n",
      "Iteration 1497, the loss is 5.161284811590049, parameters k is 14.760898834879244 and b is -70.02944670043641\n",
      "Iteration 1498, the loss is 5.161285036546771, parameters k is 14.760423162942486 and b is -70.02946646328226\n",
      "Iteration 1499, the loss is 5.161284094582326, parameters k is 14.760557372428652 and b is -70.02948622612811\n",
      "Iteration 1500, the loss is 5.161283152617877, parameters k is 14.760691581914818 and b is -70.02950598897397\n",
      "Iteration 1501, the loss is 5.161284149221318, parameters k is 14.760825791400984 and b is -70.02952575181982\n",
      "Iteration 1502, the loss is 5.161289453876213, parameters k is 14.760350119464226 and b is -70.02954551466567\n",
      "Iteration 1503, the loss is 5.161288511911768, parameters k is 14.760484328950392 and b is -70.02956527751152\n",
      "Iteration 1504, the loss is 5.161287569947313, parameters k is 14.760618538436558 and b is -70.02958504035738\n",
      "Iteration 1505, the loss is 5.1612866279828555, parameters k is 14.760752747922725 and b is -70.02960480320323\n",
      "Iteration 1506, the loss is 5.161290573847343, parameters k is 14.760886957408891 and b is -70.02962456604908\n",
      "Iteration 1507, the loss is 5.1612929292411955, parameters k is 14.760411285472133 and b is -70.02964432889493\n",
      "Iteration 1508, the loss is 5.161291987276752, parameters k is 14.760545494958299 and b is -70.02966409174078\n",
      "Iteration 1509, the loss is 5.161291045312303, parameters k is 14.760679704444465 and b is -70.02968385458664\n",
      "Iteration 1510, the loss is 5.1612904601367005, parameters k is 14.760813913930631 and b is -70.02970361743249\n",
      "Iteration 1511, the loss is 5.161293198277429, parameters k is 14.760647332902963 and b is -70.02972338027834\n",
      "Iteration 1512, the loss is 5.161292256312983, parameters k is 14.760781542389129 and b is -70.0297431431242\n",
      "Iteration 1513, the loss is 5.161296864702269, parameters k is 14.760915751875295 and b is -70.02976290597005\n",
      "Iteration 1514, the loss is 5.1612985575713175, parameters k is 14.760440079938537 and b is -70.0297826688159\n",
      "Iteration 1515, the loss is 5.161297615606867, parameters k is 14.760574289424703 and b is -70.02980243166175\n",
      "Iteration 1516, the loss is 5.1612966736424175, parameters k is 14.76070849891087 and b is -70.0298221945076\n",
      "Iteration 1517, the loss is 5.161296407778947, parameters k is 14.760842708397035 and b is -70.02984195735345\n",
      "Iteration 1518, the loss is 5.161298826607551, parameters k is 14.760676127369367 and b is -70.0298617201993\n",
      "Iteration 1519, the loss is 5.161297884643097, parameters k is 14.760810336855533 and b is -70.02988148304516\n",
      "Iteration 1520, the loss is 5.161303155557186, parameters k is 14.7609445463417 and b is -70.02990124589101\n",
      "Iteration 1521, the loss is 5.161304185901434, parameters k is 14.76046887440494 and b is -70.02992100873686\n",
      "Iteration 1522, the loss is 5.161303243936986, parameters k is 14.760603083891107 and b is -70.02994077158272\n",
      "Iteration 1523, the loss is 5.161302301972536, parameters k is 14.760737293377273 and b is -70.02996053442857\n",
      "Iteration 1524, the loss is 5.161302493188457, parameters k is 14.76087150286344 and b is -70.02998029727442\n",
      "Iteration 1525, the loss is 5.161308603230877, parameters k is 14.760395830926681 and b is -70.03000006012027\n",
      "Iteration 1526, the loss is 5.161307661266419, parameters k is 14.760530040412847 and b is -70.03001982296612\n",
      "Iteration 1527, the loss is 5.161306719301973, parameters k is 14.760664249899014 and b is -70.03003958581198\n",
      "Iteration 1528, the loss is 5.1613057773375255, parameters k is 14.76079845938518 and b is -70.03005934865783\n",
      "Iteration 1529, the loss is 5.161308917814494, parameters k is 14.760932668871346 and b is -70.03007911150368\n",
      "Iteration 1530, the loss is 5.16131207859586, parameters k is 14.760456996934588 and b is -70.03009887434953\n",
      "Iteration 1531, the loss is 5.161311136631413, parameters k is 14.760591206420754 and b is -70.03011863719539\n",
      "Iteration 1532, the loss is 5.16131019466696, parameters k is 14.76072541590692 and b is -70.03013840004124\n",
      "Iteration 1533, the loss is 5.16130925270251, parameters k is 14.760859625393087 and b is -70.03015816288709\n",
      "Iteration 1534, the loss is 5.161315342440518, parameters k is 14.760993834879253 and b is -70.03017792573294\n",
      "Iteration 1535, the loss is 5.161315553960848, parameters k is 14.760518162942494 and b is -70.0301976885788\n",
      "Iteration 1536, the loss is 5.161314611996404, parameters k is 14.76065237242866 and b is -70.03021745142465\n",
      "Iteration 1537, the loss is 5.161313670031957, parameters k is 14.760786581914827 and b is -70.0302372142705\n",
      "Iteration 1538, the loss is 5.161314680071793, parameters k is 14.760920791400993 and b is -70.03025697711635\n",
      "Iteration 1539, the loss is 5.161319971290284, parameters k is 14.760445119464235 and b is -70.0302767399622\n",
      "Iteration 1540, the loss is 5.161319029325839, parameters k is 14.760579328950401 and b is -70.03029650280806\n",
      "Iteration 1541, the loss is 5.16131808736139, parameters k is 14.760713538436567 and b is -70.03031626565391\n",
      "Iteration 1542, the loss is 5.161317145396938, parameters k is 14.760847747922734 and b is -70.03033602849976\n",
      "Iteration 1543, the loss is 5.161321104697816, parameters k is 14.7609819574089 and b is -70.03035579134561\n",
      "Iteration 1544, the loss is 5.161323446655278, parameters k is 14.760506285472141 and b is -70.03037555419147\n",
      "Iteration 1545, the loss is 5.161322504690828, parameters k is 14.760640494958308 and b is -70.03039531703732\n",
      "Iteration 1546, the loss is 5.161321562726381, parameters k is 14.760774704444474 and b is -70.03041507988317\n",
      "Iteration 1547, the loss is 5.161320944842102, parameters k is 14.76090891393064 and b is -70.03043484272902\n",
      "Iteration 1548, the loss is 5.161323715691501, parameters k is 14.760742332902971 and b is -70.03045460557487\n",
      "Iteration 1549, the loss is 5.161322773727061, parameters k is 14.760876542389138 and b is -70.03047436842073\n",
      "Iteration 1550, the loss is 5.16132739555274, parameters k is 14.761010751875304 and b is -70.03049413126658\n",
      "Iteration 1551, the loss is 5.161329074985396, parameters k is 14.760535079938546 and b is -70.03051389411243\n",
      "Iteration 1552, the loss is 5.1613281330209455, parameters k is 14.760669289424712 and b is -70.03053365695828\n",
      "Iteration 1553, the loss is 5.1613271910565, parameters k is 14.760803498910878 and b is -70.03055341980414\n",
      "Iteration 1554, the loss is 5.161326892484343, parameters k is 14.760937708397044 and b is -70.03057318264999\n",
      "Iteration 1555, the loss is 5.161329344021625, parameters k is 14.760771127369376 and b is -70.03059294549584\n",
      "Iteration 1556, the loss is 5.161328402057173, parameters k is 14.760905336855542 and b is -70.03061270834169\n",
      "Iteration 1557, the loss is 5.16133368640766, parameters k is 14.761039546341708 and b is -70.03063247118754\n",
      "Iteration 1558, the loss is 5.1613347033155135, parameters k is 14.76056387440495 and b is -70.0306522340334\n",
      "Iteration 1559, the loss is 5.161333761351063, parameters k is 14.760698083891116 and b is -70.03067199687925\n",
      "Iteration 1560, the loss is 5.161332819386616, parameters k is 14.760832293377282 and b is -70.0306917597251\n",
      "Iteration 1561, the loss is 5.161333024038925, parameters k is 14.760966502863448 and b is -70.03071152257095\n",
      "Iteration 1562, the loss is 5.161339120644958, parameters k is 14.76049083092669 and b is -70.0307312854168\n",
      "Iteration 1563, the loss is 5.1613381786805, parameters k is 14.760625040412856 and b is -70.03075104826266\n",
      "Iteration 1564, the loss is 5.161337236716054, parameters k is 14.760759249899023 and b is -70.03077081110851\n",
      "Iteration 1565, the loss is 5.161336294751596, parameters k is 14.760893459385189 and b is -70.03079057395436\n",
      "Iteration 1566, the loss is 5.161339448664959, parameters k is 14.761027668871355 and b is -70.03081033680022\n",
      "Iteration 1567, the loss is 5.161342596009941, parameters k is 14.760551996934597 and b is -70.03083009964607\n",
      "Iteration 1568, the loss is 5.161341654045496, parameters k is 14.760686206420763 and b is -70.03084986249192\n",
      "Iteration 1569, the loss is 5.161340712081036, parameters k is 14.76082041590693 and b is -70.03086962533777\n",
      "Iteration 1570, the loss is 5.161339770116592, parameters k is 14.760954625393095 and b is -70.03088938818362\n",
      "Iteration 1571, the loss is 5.161345873290993, parameters k is 14.761088834879262 and b is -70.03090915102948\n",
      "Iteration 1572, the loss is 5.161346071374931, parameters k is 14.760613162942503 and b is -70.03092891387533\n",
      "Iteration 1573, the loss is 5.161345129410484, parameters k is 14.76074737242867 and b is -70.03094867672118\n",
      "Iteration 1574, the loss is 5.161344187446033, parameters k is 14.760881581914836 and b is -70.03096843956703\n",
      "Iteration 1575, the loss is 5.161345210922257, parameters k is 14.761015791401002 and b is -70.03098820241289\n",
      "Iteration 1576, the loss is 5.161350488704369, parameters k is 14.760540119464244 and b is -70.03100796525874\n",
      "Iteration 1577, the loss is 5.161349546739919, parameters k is 14.76067432895041 and b is -70.03102772810459\n",
      "Iteration 1578, the loss is 5.161348604775466, parameters k is 14.760808538436576 and b is -70.03104749095044\n",
      "Iteration 1579, the loss is 5.1613476628110195, parameters k is 14.760942747922742 and b is -70.0310672537963\n",
      "Iteration 1580, the loss is 5.161351635548287, parameters k is 14.761076957408909 and b is -70.03108701664215\n",
      "Iteration 1581, the loss is 5.1613539640693595, parameters k is 14.76060128547215 and b is -70.031106779488\n",
      "Iteration 1582, the loss is 5.161353022104904, parameters k is 14.760735494958316 and b is -70.03112654233385\n",
      "Iteration 1583, the loss is 5.161352080140451, parameters k is 14.760869704444483 and b is -70.0311463051797\n",
      "Iteration 1584, the loss is 5.1613514295475, parameters k is 14.761003913930649 and b is -70.03116606802556\n",
      "Iteration 1585, the loss is 5.161354233105581, parameters k is 14.76083733290298 and b is -70.03118583087141\n",
      "Iteration 1586, the loss is 5.161353291141134, parameters k is 14.760971542389147 and b is -70.03120559371726\n",
      "Iteration 1587, the loss is 5.161357926403213, parameters k is 14.761105751875313 and b is -70.03122535656311\n",
      "Iteration 1588, the loss is 5.161359592399475, parameters k is 14.760630079938554 and b is -70.03124511940896\n",
      "Iteration 1589, the loss is 5.161358650435028, parameters k is 14.76076428942472 and b is -70.03126488225482\n",
      "Iteration 1590, the loss is 5.1613577084705735, parameters k is 14.760898498910887 and b is -70.03128464510067\n",
      "Iteration 1591, the loss is 5.1613573771897405, parameters k is 14.761032708397053 and b is -70.03130440794652\n",
      "Iteration 1592, the loss is 5.161359861435707, parameters k is 14.760866127369384 and b is -70.03132417079237\n",
      "Iteration 1593, the loss is 5.161358919471253, parameters k is 14.76100033685555 and b is -70.03134393363823\n",
      "Iteration 1594, the loss is 5.161364217258125, parameters k is 14.761134546341717 and b is -70.03136369648408\n",
      "Iteration 1595, the loss is 5.161365220729592, parameters k is 14.760658874404958 and b is -70.03138345932993\n",
      "Iteration 1596, the loss is 5.161364278765144, parameters k is 14.760793083891125 and b is -70.03140322217578\n",
      "Iteration 1597, the loss is 5.161363336800694, parameters k is 14.760927293377291 and b is -70.03142298502163\n",
      "Iteration 1598, the loss is 5.161363554889402, parameters k is 14.761061502863457 and b is -70.03144274786749\n",
      "Iteration 1599, the loss is 5.161369638059033, parameters k is 14.760585830926699 and b is -70.03146251071334\n",
      "Iteration 1600, the loss is 5.1613686960945815, parameters k is 14.760720040412865 and b is -70.03148227355919\n",
      "Iteration 1601, the loss is 5.161367754130132, parameters k is 14.760854249899031 and b is -70.03150203640504\n",
      "Iteration 1602, the loss is 5.161366812165684, parameters k is 14.760988459385198 and b is -70.0315217992509\n",
      "Iteration 1603, the loss is 5.161369979515428, parameters k is 14.761122668871364 and b is -70.03154156209675\n",
      "Iteration 1604, the loss is 5.161373113424019, parameters k is 14.760646996934605 and b is -70.0315613249426\n",
      "Iteration 1605, the loss is 5.161372171459569, parameters k is 14.760781206420772 and b is -70.03158108778845\n",
      "Iteration 1606, the loss is 5.161371229495121, parameters k is 14.760915415906938 and b is -70.0316008506343\n",
      "Iteration 1607, the loss is 5.16137028753067, parameters k is 14.761049625393104 and b is -70.03162061348016\n",
      "Iteration 1608, the loss is 5.161376404141458, parameters k is 14.76118383487927 and b is -70.03164037632601\n",
      "Iteration 1609, the loss is 5.1613765887890075, parameters k is 14.760708162942512 and b is -70.03166013917186\n",
      "Iteration 1610, the loss is 5.161375646824558, parameters k is 14.760842372428678 and b is -70.03167990201771\n",
      "Iteration 1611, the loss is 5.1613747048601075, parameters k is 14.760976581914845 and b is -70.03169966486357\n",
      "Iteration 1612, the loss is 5.1613757417727255, parameters k is 14.761110791401011 and b is -70.03171942770942\n",
      "Iteration 1613, the loss is 5.1613810061184475, parameters k is 14.760635119464252 and b is -70.03173919055527\n",
      "Iteration 1614, the loss is 5.161380064153999, parameters k is 14.760769328950419 and b is -70.03175895340112\n",
      "Iteration 1615, the loss is 5.161379122189545, parameters k is 14.760903538436585 and b is -70.03177871624698\n",
      "Iteration 1616, the loss is 5.161378180225099, parameters k is 14.761037747922751 and b is -70.03179847909283\n",
      "Iteration 1617, the loss is 5.161382166398758, parameters k is 14.761171957408918 and b is -70.03181824193868\n",
      "Iteration 1618, the loss is 5.161384481483436, parameters k is 14.760696285472159 and b is -70.03183800478453\n",
      "Iteration 1619, the loss is 5.161383539518979, parameters k is 14.760830494958325 and b is -70.03185776763038\n",
      "Iteration 1620, the loss is 5.16138259755453, parameters k is 14.760964704444492 and b is -70.03187753047624\n",
      "Iteration 1621, the loss is 5.161381914252899, parameters k is 14.761098913930658 and b is -70.03189729332209\n",
      "Iteration 1622, the loss is 5.161384750519668, parameters k is 14.76093233290299 and b is -70.03191705616794\n",
      "Iteration 1623, the loss is 5.1613838085552155, parameters k is 14.761066542389155 and b is -70.0319368190138\n",
      "Iteration 1624, the loss is 5.161388457253676, parameters k is 14.761200751875322 and b is -70.03195658185965\n",
      "Iteration 1625, the loss is 5.161390109813553, parameters k is 14.760725079938563 and b is -70.0319763447055\n",
      "Iteration 1626, the loss is 5.161389167849104, parameters k is 14.76085928942473 and b is -70.03199610755135\n",
      "Iteration 1627, the loss is 5.161388225884651, parameters k is 14.760993498910896 and b is -70.0320158703972\n",
      "Iteration 1628, the loss is 5.161387861895135, parameters k is 14.761127708397062 and b is -70.03203563324305\n",
      "Iteration 1629, the loss is 5.161390378849788, parameters k is 14.760961127369393 and b is -70.03205539608891\n",
      "Iteration 1630, the loss is 5.16138943688533, parameters k is 14.76109533685556 and b is -70.03207515893476\n",
      "Iteration 1631, the loss is 5.161394748108596, parameters k is 14.761229546341726 and b is -70.03209492178061\n",
      "Iteration 1632, the loss is 5.16139573814367, parameters k is 14.760753874404967 and b is -70.03211468462646\n",
      "Iteration 1633, the loss is 5.161394796179219, parameters k is 14.760888083891134 and b is -70.03213444747232\n",
      "Iteration 1634, the loss is 5.161393854214771, parameters k is 14.7610222933773 and b is -70.03215421031817\n",
      "Iteration 1635, the loss is 5.161394085739869, parameters k is 14.761156502863466 and b is -70.03217397316402\n",
      "Iteration 1636, the loss is 5.161400155473112, parameters k is 14.760680830926708 and b is -70.03219373600987\n",
      "Iteration 1637, the loss is 5.161399213508654, parameters k is 14.760815040412874 and b is -70.03221349885573\n",
      "Iteration 1638, the loss is 5.1613982715442095, parameters k is 14.76094924989904 and b is -70.03223326170158\n",
      "Iteration 1639, the loss is 5.161397329579757, parameters k is 14.761083459385206 and b is -70.03225302454743\n",
      "Iteration 1640, the loss is 5.161400510365897, parameters k is 14.761217668871373 and b is -70.03227278739328\n",
      "Iteration 1641, the loss is 5.1614036308381, parameters k is 14.760741996934614 and b is -70.03229255023913\n",
      "Iteration 1642, the loss is 5.16140268887365, parameters k is 14.76087620642078 and b is -70.03231231308499\n",
      "Iteration 1643, the loss is 5.161401746909196, parameters k is 14.761010415906947 and b is -70.03233207593084\n",
      "Iteration 1644, the loss is 5.161400804944744, parameters k is 14.761144625393113 and b is -70.03235183877669\n",
      "Iteration 1645, the loss is 5.161406934991929, parameters k is 14.76127883487928 and b is -70.03237160162254\n",
      "Iteration 1646, the loss is 5.161407106203087, parameters k is 14.760803162942521 and b is -70.0323913644684\n",
      "Iteration 1647, the loss is 5.161406164238641, parameters k is 14.760937372428687 and b is -70.03241112731425\n",
      "Iteration 1648, the loss is 5.161405222274189, parameters k is 14.761071581914853 and b is -70.0324308901601\n",
      "Iteration 1649, the loss is 5.161406272623197, parameters k is 14.76120579140102 and b is -70.03245065300595\n",
      "Iteration 1650, the loss is 5.161411523532523, parameters k is 14.760730119464261 and b is -70.0324704158518\n",
      "Iteration 1651, the loss is 5.161410581568074, parameters k is 14.760864328950428 and b is -70.03249017869766\n",
      "Iteration 1652, the loss is 5.161409639603631, parameters k is 14.760998538436594 and b is -70.03250994154351\n",
      "Iteration 1653, the loss is 5.1614086976391755, parameters k is 14.76113274792276 and b is -70.03252970438936\n",
      "Iteration 1654, the loss is 5.161412697249233, parameters k is 14.761266957408926 and b is -70.03254946723521\n",
      "Iteration 1655, the loss is 5.161414998897516, parameters k is 14.760791285472168 and b is -70.03256923008107\n",
      "Iteration 1656, the loss is 5.161414056933068, parameters k is 14.760925494958334 and b is -70.03258899292692\n",
      "Iteration 1657, the loss is 5.161413114968607, parameters k is 14.7610597044445 and b is -70.03260875577277\n",
      "Iteration 1658, the loss is 5.161412398958285, parameters k is 14.761193913930667 and b is -70.03262851861862\n",
      "Iteration 1659, the loss is 5.16141526793374, parameters k is 14.761027332902998 and b is -70.03264828146447\n",
      "Iteration 1660, the loss is 5.16141432596929, parameters k is 14.761161542389164 and b is -70.03266804431033\n",
      "Iteration 1661, the loss is 5.161418988104143, parameters k is 14.76129575187533 and b is -70.03268780715618\n",
      "Iteration 1662, the loss is 5.1614206272276295, parameters k is 14.760820079938572 and b is -70.03270757000203\n",
      "Iteration 1663, the loss is 5.161419685263182, parameters k is 14.760954289424738 and b is -70.03272733284788\n",
      "Iteration 1664, the loss is 5.161418743298736, parameters k is 14.761088498910905 and b is -70.03274709569374\n",
      "Iteration 1665, the loss is 5.1614183466005334, parameters k is 14.76122270839707 and b is -70.03276685853959\n",
      "Iteration 1666, the loss is 5.161420896263866, parameters k is 14.761056127369402 and b is -70.03278662138544\n",
      "Iteration 1667, the loss is 5.161419954299414, parameters k is 14.761190336855568 and b is -70.0328063842313\n",
      "Iteration 1668, the loss is 5.16142527895907, parameters k is 14.761324546341735 and b is -70.03282614707715\n",
      "Iteration 1669, the loss is 5.16142625555775, parameters k is 14.760848874404976 and b is -70.032845909923\n",
      "Iteration 1670, the loss is 5.161425313593306, parameters k is 14.760983083891142 and b is -70.03286567276885\n",
      "Iteration 1671, the loss is 5.161424371628852, parameters k is 14.761117293377309 and b is -70.0328854356147\n",
      "Iteration 1672, the loss is 5.1614246165903355, parameters k is 14.761251502863475 and b is -70.03290519846055\n",
      "Iteration 1673, the loss is 5.161430672887188, parameters k is 14.760775830926717 and b is -70.0329249613064\n",
      "Iteration 1674, the loss is 5.161429730922738, parameters k is 14.760910040412883 and b is -70.03294472415226\n",
      "Iteration 1675, the loss is 5.161428788958285, parameters k is 14.761044249899049 and b is -70.03296448699811\n",
      "Iteration 1676, the loss is 5.161427846993844, parameters k is 14.761178459385215 and b is -70.03298424984396\n",
      "Iteration 1677, the loss is 5.161431041216366, parameters k is 14.761312668871382 and b is -70.03300401268982\n",
      "Iteration 1678, the loss is 5.161434148252183, parameters k is 14.760836996934623 and b is -70.03302377553567\n",
      "Iteration 1679, the loss is 5.161433206287731, parameters k is 14.76097120642079 and b is -70.03304353838152\n",
      "Iteration 1680, the loss is 5.161432264323281, parameters k is 14.761105415906956 and b is -70.03306330122737\n",
      "Iteration 1681, the loss is 5.161431322358824, parameters k is 14.761239625393122 and b is -70.03308306407322\n",
      "Iteration 1682, the loss is 5.161437465842402, parameters k is 14.761373834879288 and b is -70.03310282691908\n",
      "Iteration 1683, the loss is 5.161437623617162, parameters k is 14.76089816294253 and b is -70.03312258976493\n",
      "Iteration 1684, the loss is 5.161436681652716, parameters k is 14.761032372428696 and b is -70.03314235261078\n",
      "Iteration 1685, the loss is 5.16143573968827, parameters k is 14.761166581914862 and b is -70.03316211545663\n",
      "Iteration 1686, the loss is 5.161436803473667, parameters k is 14.761300791401029 and b is -70.03318187830249\n",
      "Iteration 1687, the loss is 5.161442040946601, parameters k is 14.76082511946427 and b is -70.03320164114834\n",
      "Iteration 1688, the loss is 5.161441098982159, parameters k is 14.760959328950436 and b is -70.03322140399419\n",
      "Iteration 1689, the loss is 5.161440157017703, parameters k is 14.761093538436603 and b is -70.03324116684004\n",
      "Iteration 1690, the loss is 5.161439215053256, parameters k is 14.761227747922769 and b is -70.0332609296859\n",
      "Iteration 1691, the loss is 5.1614432280996985, parameters k is 14.761361957408935 and b is -70.03328069253175\n",
      "Iteration 1692, the loss is 5.1614455163115895, parameters k is 14.760886285472177 and b is -70.0333004553776\n",
      "Iteration 1693, the loss is 5.161444574347146, parameters k is 14.761020494958343 and b is -70.03332021822345\n",
      "Iteration 1694, the loss is 5.161443632382691, parameters k is 14.76115470444451 and b is -70.0333399810693\n",
      "Iteration 1695, the loss is 5.161442883663683, parameters k is 14.761288913930676 and b is -70.03335974391516\n",
      "Iteration 1696, the loss is 5.161445785347824, parameters k is 14.761122332903007 and b is -70.03337950676101\n",
      "Iteration 1697, the loss is 5.16144484338337, parameters k is 14.761256542389173 and b is -70.03339926960686\n",
      "Iteration 1698, the loss is 5.161449518954613, parameters k is 14.76139075187534 and b is -70.03341903245271\n",
      "Iteration 1699, the loss is 5.161451144641708, parameters k is 14.76091507993858 and b is -70.03343879529857\n",
      "Iteration 1700, the loss is 5.161450202677258, parameters k is 14.761049289424747 and b is -70.03345855814442\n",
      "Iteration 1701, the loss is 5.161449260712808, parameters k is 14.761183498910913 and b is -70.03347832099027\n",
      "Iteration 1702, the loss is 5.161448856585889, parameters k is 14.76131770839708 and b is -70.03349808383612\n",
      "Iteration 1703, the loss is 5.161455561971146, parameters k is 14.760842036460321 and b is -70.03351784668197\n",
      "Iteration 1704, the loss is 5.161454620006693, parameters k is 14.760976245946487 and b is -70.03353760952783\n",
      "Iteration 1705, the loss is 5.16145367804225, parameters k is 14.761110455432654 and b is -70.03355737237368\n",
      "Iteration 1706, the loss is 5.1614527360777975, parameters k is 14.76124466491882 and b is -70.03357713521953\n",
      "Iteration 1707, the loss is 5.16145528121192, parameters k is 14.761378874404986 and b is -70.03359689806538\n",
      "Iteration 1708, the loss is 5.161459037336135, parameters k is 14.760903202468228 and b is -70.03361666091124\n",
      "Iteration 1709, the loss is 5.16145809537169, parameters k is 14.761037411954394 and b is -70.03363642375709\n",
      "Iteration 1710, the loss is 5.161457153407233, parameters k is 14.76117162144056 and b is -70.03365618660294\n",
      "Iteration 1711, the loss is 5.161456211442792, parameters k is 14.761305830926727 and b is -70.03367594944879\n",
      "Iteration 1712, the loss is 5.161461705837945, parameters k is 14.761440040412893 and b is -70.03369571229464\n",
      "Iteration 1713, the loss is 5.1614625127011236, parameters k is 14.760964368476134 and b is -70.0337154751405\n",
      "Iteration 1714, the loss is 5.161461570736678, parameters k is 14.7610985779623 and b is -70.03373523798635\n",
      "Iteration 1715, the loss is 5.161460628772223, parameters k is 14.761232787448467 and b is -70.0337550008322\n",
      "Iteration 1716, the loss is 5.161461043469218, parameters k is 14.761366996934633 and b is -70.03377476367805\n",
      "Iteration 1717, the loss is 5.161466930030564, parameters k is 14.760891324997875 and b is -70.0337945265239\n",
      "Iteration 1718, the loss is 5.161465988066116, parameters k is 14.761025534484041 and b is -70.03381428936976\n",
      "Iteration 1719, the loss is 5.161465046101661, parameters k is 14.761159743970207 and b is -70.03383405221561\n",
      "Iteration 1720, the loss is 5.161464104137211, parameters k is 14.761293953456374 and b is -70.03385381506146\n",
      "Iteration 1721, the loss is 5.161467468095244, parameters k is 14.76142816294254 and b is -70.03387357790731\n",
      "Iteration 1722, the loss is 5.161470405395555, parameters k is 14.760952491005781 and b is -70.03389334075317\n",
      "Iteration 1723, the loss is 5.161469463431098, parameters k is 14.761086700491948 and b is -70.03391310359902\n",
      "Iteration 1724, the loss is 5.161468521466651, parameters k is 14.761220909978114 and b is -70.03393286644487\n",
      "Iteration 1725, the loss is 5.161467579502201, parameters k is 14.76135511946428 and b is -70.03395262929072\n",
      "Iteration 1726, the loss is 5.161473892721278, parameters k is 14.761489328950447 and b is -70.03397239213658\n",
      "Iteration 1727, the loss is 5.161473880760542, parameters k is 14.761013657013688 and b is -70.03399215498243\n",
      "Iteration 1728, the loss is 5.16147293879609, parameters k is 14.761147866499854 and b is -70.03401191782828\n",
      "Iteration 1729, the loss is 5.161471996831635, parameters k is 14.76128207598602 and b is -70.03403168067413\n",
      "Iteration 1730, the loss is 5.161473230352553, parameters k is 14.761416285472187 and b is -70.03405144351999\n",
      "Iteration 1731, the loss is 5.161478298089979, parameters k is 14.760940613535428 and b is -70.03407120636584\n",
      "Iteration 1732, the loss is 5.161477356125528, parameters k is 14.761074823021595 and b is -70.03409096921169\n",
      "Iteration 1733, the loss is 5.161476414161084, parameters k is 14.761209032507761 and b is -70.03411073205754\n",
      "Iteration 1734, the loss is 5.1614754721966305, parameters k is 14.761343241993927 and b is -70.0341304949034\n",
      "Iteration 1735, the loss is 5.16147965497858, parameters k is 14.761477451480093 and b is -70.03415025774925\n",
      "Iteration 1736, the loss is 5.16148177345497, parameters k is 14.761001779543335 and b is -70.0341700205951\n",
      "Iteration 1737, the loss is 5.161480831490513, parameters k is 14.761135989029501 and b is -70.03418978344095\n",
      "Iteration 1738, the loss is 5.1614798895260625, parameters k is 14.761270198515668 and b is -70.0342095462868\n",
      "Iteration 1739, the loss is 5.16147917774249, parameters k is 14.761404408001834 and b is -70.03422930913266\n",
      "Iteration 1740, the loss is 5.161482042491195, parameters k is 14.761237826974165 and b is -70.03424907197851\n",
      "Iteration 1741, the loss is 5.161481100526742, parameters k is 14.761372036460331 and b is -70.03426883482436\n",
      "Iteration 1742, the loss is 5.161485945833499, parameters k is 14.761506245946498 and b is -70.03428859767021\n",
      "Iteration 1743, the loss is 5.161487401785092, parameters k is 14.76103057400974 and b is -70.03430836051606\n",
      "Iteration 1744, the loss is 5.161486459820635, parameters k is 14.761164783495905 and b is -70.03432812336192\n",
      "Iteration 1745, the loss is 5.16148551785618, parameters k is 14.761298992982072 and b is -70.03434788620777\n",
      "Iteration 1746, the loss is 5.161485283464768, parameters k is 14.761433202468238 and b is -70.03436764905362\n",
      "Iteration 1747, the loss is 5.161491819114521, parameters k is 14.76095753053148 and b is -70.03438741189947\n",
      "Iteration 1748, the loss is 5.161490877150079, parameters k is 14.761091740017646 and b is -70.03440717474533\n",
      "Iteration 1749, the loss is 5.16148993518562, parameters k is 14.761225949503812 and b is -70.03442693759118\n",
      "Iteration 1750, the loss is 5.161488993221173, parameters k is 14.761360158989978 and b is -70.03444670043703\n",
      "Iteration 1751, the loss is 5.161491708090803, parameters k is 14.761494368476145 and b is -70.03446646328288\n",
      "Iteration 1752, the loss is 5.1614952944795185, parameters k is 14.761018696539386 and b is -70.03448622612873\n",
      "Iteration 1753, the loss is 5.161494352515063, parameters k is 14.761152906025552 and b is -70.03450598897459\n",
      "Iteration 1754, the loss is 5.161493410550614, parameters k is 14.761287115511719 and b is -70.03452575182044\n",
      "Iteration 1755, the loss is 5.161492468586164, parameters k is 14.761421324997885 and b is -70.03454551466629\n",
      "Iteration 1756, the loss is 5.161498132716826, parameters k is 14.761555534484051 and b is -70.03456527751214\n",
      "Iteration 1757, the loss is 5.161498769844503, parameters k is 14.761079862547293 and b is -70.034585040358\n",
      "Iteration 1758, the loss is 5.161497827880055, parameters k is 14.761214072033459 and b is -70.03460480320385\n",
      "Iteration 1759, the loss is 5.161496885915602, parameters k is 14.761348281519625 and b is -70.0346245660497\n",
      "Iteration 1760, the loss is 5.1614974703481025, parameters k is 14.761482491005792 and b is -70.03464432889555\n",
      "Iteration 1761, the loss is 5.16150318717394, parameters k is 14.761006819069033 and b is -70.0346640917414\n",
      "Iteration 1762, the loss is 5.16150224520949, parameters k is 14.7611410285552 and b is -70.03468385458726\n",
      "Iteration 1763, the loss is 5.161501303245042, parameters k is 14.761275238041366 and b is -70.03470361743311\n",
      "Iteration 1764, the loss is 5.161500361280594, parameters k is 14.761409447527532 and b is -70.03472338027896\n",
      "Iteration 1765, the loss is 5.16150389497413, parameters k is 14.761543657013698 and b is -70.03474314312481\n",
      "Iteration 1766, the loss is 5.161506662538926, parameters k is 14.76106798507694 and b is -70.03476290597067\n",
      "Iteration 1767, the loss is 5.161505720574484, parameters k is 14.761202194563106 and b is -70.03478266881652\n",
      "Iteration 1768, the loss is 5.161504778610025, parameters k is 14.761336404049272 and b is -70.03480243166237\n",
      "Iteration 1769, the loss is 5.161503836645577, parameters k is 14.761470613535439 and b is -70.03482219450822\n",
      "Iteration 1770, the loss is 5.161510319600158, parameters k is 14.761604823021605 and b is -70.03484195735408\n",
      "Iteration 1771, the loss is 5.161510137903918, parameters k is 14.761129151084846 and b is -70.03486172019993\n",
      "Iteration 1772, the loss is 5.161509195939462, parameters k is 14.761263360571013 and b is -70.03488148304578\n",
      "Iteration 1773, the loss is 5.161508253975018, parameters k is 14.761397570057179 and b is -70.03490124589163\n",
      "Iteration 1774, the loss is 5.161509657231429, parameters k is 14.761531779543345 and b is -70.03492100873748\n",
      "Iteration 1775, the loss is 5.161514555233359, parameters k is 14.761056107606587 and b is -70.03494077158334\n",
      "Iteration 1776, the loss is 5.161513613268906, parameters k is 14.761190317092753 and b is -70.03496053442919\n",
      "Iteration 1777, the loss is 5.161512671304452, parameters k is 14.76132452657892 and b is -70.03498029727504\n",
      "Iteration 1778, the loss is 5.161511729340005, parameters k is 14.761458736065086 and b is -70.0350000601209\n",
      "Iteration 1779, the loss is 5.161516081857457, parameters k is 14.761592945551252 and b is -70.03501982296675\n",
      "Iteration 1780, the loss is 5.161518030598343, parameters k is 14.761117273614493 and b is -70.0350395858126\n",
      "Iteration 1781, the loss is 5.161517088633895, parameters k is 14.76125148310066 and b is -70.03505934865845\n",
      "Iteration 1782, the loss is 5.161516146669445, parameters k is 14.761385692586826 and b is -70.0350791115043\n",
      "Iteration 1783, the loss is 5.161515471821296, parameters k is 14.761519902072992 and b is -70.03509887435015\n",
      "Iteration 1784, the loss is 5.161518299634572, parameters k is 14.761353321045323 and b is -70.035118637196\n",
      "Iteration 1785, the loss is 5.161517357670121, parameters k is 14.76148753053149 and b is -70.03513840004186\n",
      "Iteration 1786, the loss is 5.161522372712376, parameters k is 14.761621740017656 and b is -70.03515816288771\n",
      "Iteration 1787, the loss is 5.161523658928457, parameters k is 14.761146068080897 and b is -70.03517792573356\n",
      "Iteration 1788, the loss is 5.161522716964008, parameters k is 14.761280277567064 and b is -70.03519768857942\n",
      "Iteration 1789, the loss is 5.161521774999557, parameters k is 14.76141448705323 and b is -70.03521745142527\n",
      "Iteration 1790, the loss is 5.161521710343648, parameters k is 14.761548696539396 and b is -70.03523721427112\n",
      "Iteration 1791, the loss is 5.161528076257897, parameters k is 14.761073024602638 and b is -70.03525697711697\n",
      "Iteration 1792, the loss is 5.161527134293451, parameters k is 14.761207234088804 and b is -70.03527673996282\n",
      "Iteration 1793, the loss is 5.161526192329004, parameters k is 14.76134144357497 and b is -70.03529650280868\n",
      "Iteration 1794, the loss is 5.16152525036455, parameters k is 14.761475653061137 and b is -70.03531626565453\n",
      "Iteration 1795, the loss is 5.161528134969676, parameters k is 14.761609862547303 and b is -70.03533602850038\n",
      "Iteration 1796, the loss is 5.161531551622883, parameters k is 14.761134190610544 and b is -70.03535579134623\n",
      "Iteration 1797, the loss is 5.161530609658435, parameters k is 14.76126840009671 and b is -70.03537555419209\n",
      "Iteration 1798, the loss is 5.161529667693989, parameters k is 14.761402609582877 and b is -70.03539531703794\n",
      "Iteration 1799, the loss is 5.161528725729542, parameters k is 14.761536819069043 and b is -70.03541507988379\n",
      "Iteration 1800, the loss is 5.161534559595712, parameters k is 14.76167102855521 and b is -70.03543484272964\n",
      "Iteration 1801, the loss is 5.161535026987874, parameters k is 14.761195356618451 and b is -70.0354546055755\n",
      "Iteration 1802, the loss is 5.161534085023428, parameters k is 14.761329566104617 and b is -70.03547436842135\n",
      "Iteration 1803, the loss is 5.161533143058978, parameters k is 14.761463775590784 and b is -70.0354941312672\n",
      "Iteration 1804, the loss is 5.1615338972269775, parameters k is 14.76159798507695 and b is -70.03551389411305\n",
      "Iteration 1805, the loss is 5.1615394443173175, parameters k is 14.761122313140191 and b is -70.0355336569589\n",
      "Iteration 1806, the loss is 5.1615385023528635, parameters k is 14.761256522626358 and b is -70.03555341980476\n",
      "Iteration 1807, the loss is 5.161537560388413, parameters k is 14.761390732112524 and b is -70.03557318265061\n",
      "Iteration 1808, the loss is 5.161536618423964, parameters k is 14.76152494159869 and b is -70.03559294549646\n",
      "Iteration 1809, the loss is 5.161540321853011, parameters k is 14.761659151084856 and b is -70.03561270834231\n",
      "Iteration 1810, the loss is 5.1615429196823, parameters k is 14.761183479148098 and b is -70.03563247118817\n",
      "Iteration 1811, the loss is 5.161541977717856, parameters k is 14.761317688634264 and b is -70.03565223403402\n",
      "Iteration 1812, the loss is 5.1615410357534035, parameters k is 14.76145189812043 and b is -70.03567199687987\n",
      "Iteration 1813, the loss is 5.1615400937889495, parameters k is 14.761586107606597 and b is -70.03569175972572\n",
      "Iteration 1814, the loss is 5.16154674647904, parameters k is 14.761720317092763 and b is -70.03571152257157\n",
      "Iteration 1815, the loss is 5.161546395047292, parameters k is 14.761244645156005 and b is -70.03573128541743\n",
      "Iteration 1816, the loss is 5.161545453082841, parameters k is 14.761378854642171 and b is -70.03575104826328\n",
      "Iteration 1817, the loss is 5.161544511118392, parameters k is 14.761513064128337 and b is -70.03577081110913\n",
      "Iteration 1818, the loss is 5.161546084110312, parameters k is 14.761647273614503 and b is -70.03579057395498\n",
      "Iteration 1819, the loss is 5.1615508123767295, parameters k is 14.761171601677745 and b is -70.03581033680084\n",
      "Iteration 1820, the loss is 5.16154987041228, parameters k is 14.761305811163911 and b is -70.03583009964669\n",
      "Iteration 1821, the loss is 5.161548928447832, parameters k is 14.761440020650078 and b is -70.03584986249254\n",
      "Iteration 1822, the loss is 5.161547986483377, parameters k is 14.761574230136244 and b is -70.03586962533839\n",
      "Iteration 1823, the loss is 5.161552508736346, parameters k is 14.76170843962241 and b is -70.03588938818424\n",
      "Iteration 1824, the loss is 5.16155428774172, parameters k is 14.761232767685652 and b is -70.0359091510301\n",
      "Iteration 1825, the loss is 5.1615533457772695, parameters k is 14.761366977171818 and b is -70.03592891387595\n",
      "Iteration 1826, the loss is 5.161552403812817, parameters k is 14.761501186657984 and b is -70.0359486767218\n",
      "Iteration 1827, the loss is 5.1615518463676135, parameters k is 14.76163539614415 and b is -70.03596843956765\n",
      "Iteration 1828, the loss is 5.161558705071153, parameters k is 14.761159724207392 and b is -70.0359882024135\n",
      "Iteration 1829, the loss is 5.161557763106706, parameters k is 14.761293933693558 and b is -70.03600796525936\n",
      "Iteration 1830, the loss is 5.1615568211422564, parameters k is 14.761428143179725 and b is -70.03602772810521\n",
      "Iteration 1831, the loss is 5.161555879177806, parameters k is 14.76156235266589 and b is -70.03604749095106\n",
      "Iteration 1832, the loss is 5.161558270993637, parameters k is 14.761696562152057 and b is -70.03606725379692\n",
      "Iteration 1833, the loss is 5.161562180436146, parameters k is 14.761220890215299 and b is -70.03608701664277\n",
      "Iteration 1834, the loss is 5.161561238471691, parameters k is 14.761355099701465 and b is -70.03610677948862\n",
      "Iteration 1835, the loss is 5.16156029650725, parameters k is 14.761489309187631 and b is -70.03612654233447\n",
      "Iteration 1836, the loss is 5.161559354542793, parameters k is 14.761623518673797 and b is -70.03614630518032\n",
      "Iteration 1837, the loss is 5.161564695619674, parameters k is 14.761757728159964 and b is -70.03616606802618\n",
      "Iteration 1838, the loss is 5.16156565580113, parameters k is 14.761282056223205 and b is -70.03618583087203\n",
      "Iteration 1839, the loss is 5.1615647138366825, parameters k is 14.761416265709371 and b is -70.03620559371788\n",
      "Iteration 1840, the loss is 5.161563771872236, parameters k is 14.761550475195538 and b is -70.03622535656373\n",
      "Iteration 1841, the loss is 5.161564033250943, parameters k is 14.761684684681704 and b is -70.03624511940959\n",
      "Iteration 1842, the loss is 5.161570073130569, parameters k is 14.761209012744946 and b is -70.03626488225544\n",
      "Iteration 1843, the loss is 5.1615691311661225, parameters k is 14.761343222231112 and b is -70.03628464510129\n",
      "Iteration 1844, the loss is 5.161568189201668, parameters k is 14.761477431717278 and b is -70.03630440794714\n",
      "Iteration 1845, the loss is 5.161567247237223, parameters k is 14.761611641203444 and b is -70.036324170793\n",
      "Iteration 1846, the loss is 5.161570457876972, parameters k is 14.76174585068961 and b is -70.03634393363885\n",
      "Iteration 1847, the loss is 5.161573548495559, parameters k is 14.761270178752852 and b is -70.0363636964847\n",
      "Iteration 1848, the loss is 5.161572606531105, parameters k is 14.761404388239018 and b is -70.03638345933055\n",
      "Iteration 1849, the loss is 5.1615716645666625, parameters k is 14.761538597725185 and b is -70.0364032221764\n",
      "Iteration 1850, the loss is 5.161570722602208, parameters k is 14.761672807211351 and b is -70.03642298502226\n",
      "Iteration 1851, the loss is 5.161576882503, parameters k is 14.761807016697517 and b is -70.03644274786811\n",
      "Iteration 1852, the loss is 5.161577023860547, parameters k is 14.761331344760759 and b is -70.03646251071396\n",
      "Iteration 1853, the loss is 5.161576081896098, parameters k is 14.761465554246925 and b is -70.03648227355981\n",
      "Iteration 1854, the loss is 5.16157513993165, parameters k is 14.761599763733091 and b is -70.03650203640566\n",
      "Iteration 1855, the loss is 5.161576220134277, parameters k is 14.761733973219258 and b is -70.03652179925152\n",
      "Iteration 1856, the loss is 5.161581441189982, parameters k is 14.7612583012825 and b is -70.03654156209737\n",
      "Iteration 1857, the loss is 5.161580499225535, parameters k is 14.761392510768665 and b is -70.03656132494322\n",
      "Iteration 1858, the loss is 5.1615795572610805, parameters k is 14.761526720254832 and b is -70.03658108778907\n",
      "Iteration 1859, the loss is 5.161578615296633, parameters k is 14.761660929740998 and b is -70.03660085063493\n",
      "Iteration 1860, the loss is 5.161582644760301, parameters k is 14.761795139227164 and b is -70.03662061348078\n",
      "Iteration 1861, the loss is 5.161584916554974, parameters k is 14.761319467290406 and b is -70.03664037632663\n",
      "Iteration 1862, the loss is 5.16158397459052, parameters k is 14.761453676776572 and b is -70.03666013917248\n",
      "Iteration 1863, the loss is 5.161583032626074, parameters k is 14.761587886262738 and b is -70.03667990201834\n",
      "Iteration 1864, the loss is 5.161582112336666, parameters k is 14.761722095748905 and b is -70.03669966486419\n",
      "Iteration 1865, the loss is 5.161585185591203, parameters k is 14.761555514721236 and b is -70.03671942771004\n",
      "Iteration 1866, the loss is 5.1615842436267485, parameters k is 14.761689724207402 and b is -70.03673919055589\n",
      "Iteration 1867, the loss is 5.16158893561522, parameters k is 14.761823933693568 and b is -70.03675895340174\n",
      "Iteration 1868, the loss is 5.161590544885089, parameters k is 14.76134826175681 and b is -70.0367787162476\n",
      "Iteration 1869, the loss is 5.161589602920637, parameters k is 14.761482471242976 and b is -70.03679847909345\n",
      "Iteration 1870, the loss is 5.161588660956189, parameters k is 14.761616680729142 and b is -70.0368182419393\n",
      "Iteration 1871, the loss is 5.161588273246495, parameters k is 14.761750890215309 and b is -70.03683800478515\n",
      "Iteration 1872, the loss is 5.161594962214536, parameters k is 14.76127521827855 and b is -70.036857767631\n",
      "Iteration 1873, the loss is 5.161594020250078, parameters k is 14.761409427764717 and b is -70.03687753047686\n",
      "Iteration 1874, the loss is 5.161593078285628, parameters k is 14.761543637250883 and b is -70.03689729332271\n",
      "Iteration 1875, the loss is 5.161592136321182, parameters k is 14.761677846737049 and b is -70.03691705616856\n",
      "Iteration 1876, the loss is 5.16159469787252, parameters k is 14.761812056223215 and b is -70.03693681901441\n",
      "Iteration 1877, the loss is 5.161598437579518, parameters k is 14.761336384286457 and b is -70.03695658186027\n",
      "Iteration 1878, the loss is 5.1615974956150685, parameters k is 14.761470593772623 and b is -70.03697634470612\n",
      "Iteration 1879, the loss is 5.1615965536506225, parameters k is 14.76160480325879 and b is -70.03699610755197\n",
      "Iteration 1880, the loss is 5.16159561168617, parameters k is 14.761739012744956 and b is -70.03701587039782\n",
      "Iteration 1881, the loss is 5.161601122498556, parameters k is 14.761873222231122 and b is -70.03703563324368\n",
      "Iteration 1882, the loss is 5.161601912944513, parameters k is 14.761397550294364 and b is -70.03705539608953\n",
      "Iteration 1883, the loss is 5.161600970980053, parameters k is 14.76153175978053 and b is -70.03707515893538\n",
      "Iteration 1884, the loss is 5.161600029015605, parameters k is 14.761665969266696 and b is -70.03709492178123\n",
      "Iteration 1885, the loss is 5.161600460129823, parameters k is 14.761800178752862 and b is -70.03711468462708\n",
      "Iteration 1886, the loss is 5.161606330273939, parameters k is 14.761324506816104 and b is -70.03713444747294\n",
      "Iteration 1887, the loss is 5.161605388309498, parameters k is 14.76145871630227 and b is -70.03715421031879\n",
      "Iteration 1888, the loss is 5.161604446345053, parameters k is 14.761592925788436 and b is -70.03717397316464\n",
      "Iteration 1889, the loss is 5.161603504380596, parameters k is 14.761727135274603 and b is -70.0371937360105\n",
      "Iteration 1890, the loss is 5.161606884755854, parameters k is 14.761861344760769 and b is -70.03721349885635\n",
      "Iteration 1891, the loss is 5.161609805638931, parameters k is 14.76138567282401 and b is -70.0372332617022\n",
      "Iteration 1892, the loss is 5.161608863674483, parameters k is 14.761519882310177 and b is -70.03725302454805\n",
      "Iteration 1893, the loss is 5.161607921710037, parameters k is 14.761654091796343 and b is -70.0372727873939\n",
      "Iteration 1894, the loss is 5.161606979745579, parameters k is 14.76178830128251 and b is -70.03729255023975\n",
      "Iteration 1895, the loss is 5.161613309381883, parameters k is 14.761922510768676 and b is -70.03731231308561\n",
      "Iteration 1896, the loss is 5.161613281003925, parameters k is 14.761446838831917 and b is -70.03733207593146\n",
      "Iteration 1897, the loss is 5.161612339039471, parameters k is 14.761581048318083 and b is -70.03735183877731\n",
      "Iteration 1898, the loss is 5.1616113970750215, parameters k is 14.76171525780425 and b is -70.03737160162316\n",
      "Iteration 1899, the loss is 5.161612647013153, parameters k is 14.761849467290416 and b is -70.03739136446902\n",
      "Iteration 1900, the loss is 5.161617698333356, parameters k is 14.761373795353657 and b is -70.03741112731487\n",
      "Iteration 1901, the loss is 5.1616167563689075, parameters k is 14.761508004839824 and b is -70.03743089016072\n",
      "Iteration 1902, the loss is 5.161615814404461, parameters k is 14.76164221432599 and b is -70.03745065300657\n",
      "Iteration 1903, the loss is 5.161614872440004, parameters k is 14.761776423812156 and b is -70.03747041585243\n",
      "Iteration 1904, the loss is 5.1616190716391825, parameters k is 14.761910633298323 and b is -70.03749017869828\n",
      "Iteration 1905, the loss is 5.161621173698349, parameters k is 14.761434961361564 and b is -70.03750994154413\n",
      "Iteration 1906, the loss is 5.1616202317339015, parameters k is 14.76156917084773 and b is -70.03752970438998\n",
      "Iteration 1907, the loss is 5.161619289769446, parameters k is 14.761703380333897 and b is -70.03754946723583\n",
      "Iteration 1908, the loss is 5.161618409270452, parameters k is 14.761837589820063 and b is -70.03756923008169\n",
      "Iteration 1909, the loss is 5.161625591027784, parameters k is 14.761361917883304 and b is -70.03758899292754\n",
      "Iteration 1910, the loss is 5.161624649063337, parameters k is 14.76149612736947 and b is -70.03760875577339\n",
      "Iteration 1911, the loss is 5.161623707098885, parameters k is 14.761630336855637 and b is -70.03762851861924\n",
      "Iteration 1912, the loss is 5.161622765134436, parameters k is 14.761764546341803 and b is -70.0376482814651\n",
      "Iteration 1913, the loss is 5.1616248338964885, parameters k is 14.76189875582797 and b is -70.03766804431095\n",
      "Iteration 1914, the loss is 5.161629066392778, parameters k is 14.761423083891211 and b is -70.0376878071568\n",
      "Iteration 1915, the loss is 5.161628124428322, parameters k is 14.761557293377377 and b is -70.03770757000265\n",
      "Iteration 1916, the loss is 5.161627182463874, parameters k is 14.761691502863544 and b is -70.0377273328485\n",
      "Iteration 1917, the loss is 5.161626240499425, parameters k is 14.76182571234971 and b is -70.03774709569436\n",
      "Iteration 1918, the loss is 5.16163125852251, parameters k is 14.761959921835876 and b is -70.03776685854021\n",
      "Iteration 1919, the loss is 5.161632541757766, parameters k is 14.761484249899118 and b is -70.03778662138606\n",
      "Iteration 1920, the loss is 5.1616315997933135, parameters k is 14.761618459385284 and b is -70.03780638423191\n",
      "Iteration 1921, the loss is 5.161630657828863, parameters k is 14.76175266887145 and b is -70.03782614707777\n",
      "Iteration 1922, the loss is 5.161630596153786, parameters k is 14.761886878357616 and b is -70.03784590992362\n",
      "Iteration 1923, the loss is 5.1616369590872, parameters k is 14.761411206420858 and b is -70.03786567276947\n",
      "Iteration 1924, the loss is 5.161636017122755, parameters k is 14.761545415907024 and b is -70.03788543561532\n",
      "Iteration 1925, the loss is 5.161635075158302, parameters k is 14.76167962539319 and b is -70.03790519846117\n",
      "Iteration 1926, the loss is 5.161634133193854, parameters k is 14.761813834879357 and b is -70.03792496130703\n",
      "Iteration 1927, the loss is 5.161637020779813, parameters k is 14.761948044365523 and b is -70.03794472415288\n",
      "Iteration 1928, the loss is 5.161640434452187, parameters k is 14.761472372428765 and b is -70.03796448699873\n",
      "Iteration 1929, the loss is 5.161639492487742, parameters k is 14.761606581914931 and b is -70.03798424984458\n",
      "Iteration 1930, the loss is 5.161638550523289, parameters k is 14.761740791401097 and b is -70.03800401269044\n",
      "Iteration 1931, the loss is 5.161637608558841, parameters k is 14.761875000887263 and b is -70.03802377553629\n",
      "Iteration 1932, the loss is 5.161643445405845, parameters k is 14.76200921037343 and b is -70.03804353838214\n",
      "Iteration 1933, the loss is 5.16164390981718, parameters k is 14.761533538436671 and b is -70.038063301228\n",
      "Iteration 1934, the loss is 5.161642967852733, parameters k is 14.761667747922838 and b is -70.03808306407385\n",
      "Iteration 1935, the loss is 5.1616420258882805, parameters k is 14.761801957409004 and b is -70.0381028269197\n",
      "Iteration 1936, the loss is 5.161642783037116, parameters k is 14.76193616689517 and b is -70.03812258976555\n",
      "Iteration 1937, the loss is 5.161648327146614, parameters k is 14.761460494958412 and b is -70.0381423526114\n",
      "Iteration 1938, the loss is 5.161647385182166, parameters k is 14.761594704444578 and b is -70.03816211545725\n",
      "Iteration 1939, the loss is 5.161646443217718, parameters k is 14.761728913930744 and b is -70.0381818783031\n",
      "Iteration 1940, the loss is 5.16164550125326, parameters k is 14.76186312341691 and b is -70.03820164114896\n",
      "Iteration 1941, the loss is 5.161649207663141, parameters k is 14.761997332903077 and b is -70.03822140399481\n",
      "Iteration 1942, the loss is 5.161651802511602, parameters k is 14.761521660966318 and b is -70.03824116684066\n",
      "Iteration 1943, the loss is 5.161650860547157, parameters k is 14.761655870452485 and b is -70.03826092968652\n",
      "Iteration 1944, the loss is 5.161649918582705, parameters k is 14.76179007993865 and b is -70.03828069253237\n",
      "Iteration 1945, the loss is 5.161648976618257, parameters k is 14.761924289424817 and b is -70.03830045537822\n",
      "Iteration 1946, the loss is 5.161655632289172, parameters k is 14.762058498910983 and b is -70.03832021822407\n",
      "Iteration 1947, the loss is 5.161655277876591, parameters k is 14.761582826974225 and b is -70.03833998106992\n",
      "Iteration 1948, the loss is 5.161654335912142, parameters k is 14.761717036460391 and b is -70.03835974391578\n",
      "Iteration 1949, the loss is 5.161653393947691, parameters k is 14.761851245946557 and b is -70.03837950676163\n",
      "Iteration 1950, the loss is 5.1616549699204475, parameters k is 14.761985455432724 and b is -70.03839926960748\n",
      "Iteration 1951, the loss is 5.161659695206028, parameters k is 14.761509783495965 and b is -70.03841903245333\n",
      "Iteration 1952, the loss is 5.1616587532415785, parameters k is 14.761643992982131 and b is -70.03843879529919\n",
      "Iteration 1953, the loss is 5.161657811277133, parameters k is 14.761778202468298 and b is -70.03845855814504\n",
      "Iteration 1954, the loss is 5.16165686931268, parameters k is 14.761912411954464 and b is -70.03847832099089\n",
      "Iteration 1955, the loss is 5.161661394546475, parameters k is 14.76204662144063 and b is -70.03849808383674\n",
      "Iteration 1956, the loss is 5.16166317057102, parameters k is 14.761570949503872 and b is -70.0385178466826\n",
      "Iteration 1957, the loss is 5.16166222860657, parameters k is 14.761705158990038 and b is -70.03853760952845\n",
      "Iteration 1958, the loss is 5.161661286642124, parameters k is 14.761839368476204 and b is -70.0385573723743\n",
      "Iteration 1959, the loss is 5.161660732177748, parameters k is 14.76197357796237 and b is -70.03857713522015\n",
      "Iteration 1960, the loss is 5.161667587900457, parameters k is 14.761497906025612 and b is -70.038596898066\n",
      "Iteration 1961, the loss is 5.161666645936005, parameters k is 14.761632115511778 and b is -70.03861666091186\n",
      "Iteration 1962, the loss is 5.161665703971565, parameters k is 14.761766324997945 and b is -70.03863642375771\n",
      "Iteration 1963, the loss is 5.161664762007112, parameters k is 14.761900534484111 and b is -70.03865618660356\n",
      "Iteration 1964, the loss is 5.1616671568037695, parameters k is 14.762034743970277 and b is -70.03867594944941\n",
      "Iteration 1965, the loss is 5.161671063265442, parameters k is 14.761559072033519 and b is -70.03869571229527\n",
      "Iteration 1966, the loss is 5.161670121300992, parameters k is 14.761693281519685 and b is -70.03871547514112\n",
      "Iteration 1967, the loss is 5.161669179336549, parameters k is 14.761827491005851 and b is -70.03873523798697\n",
      "Iteration 1968, the loss is 5.161668237372098, parameters k is 14.761961700492018 and b is -70.03875500083282\n",
      "Iteration 1969, the loss is 5.1616735814298025, parameters k is 14.762095909978184 and b is -70.03877476367867\n",
      "Iteration 1970, the loss is 5.161674538630438, parameters k is 14.761620238041425 and b is -70.03879452652453\n",
      "Iteration 1971, the loss is 5.161673596665989, parameters k is 14.761754447527592 and b is -70.03881428937038\n",
      "Iteration 1972, the loss is 5.161672654701538, parameters k is 14.761888657013758 and b is -70.03883405221623\n",
      "Iteration 1973, the loss is 5.161672919061078, parameters k is 14.762022866499924 and b is -70.03885381506208\n",
      "Iteration 1974, the loss is 5.161678955959871, parameters k is 14.761547194563166 and b is -70.03887357790794\n",
      "Iteration 1975, the loss is 5.161678013995425, parameters k is 14.761681404049332 and b is -70.03889334075379\n",
      "Iteration 1976, the loss is 5.161677072030972, parameters k is 14.761815613535498 and b is -70.03891310359964\n",
      "Iteration 1977, the loss is 5.161676130066525, parameters k is 14.761949823021665 and b is -70.03893286644549\n",
      "Iteration 1978, the loss is 5.161679343687102, parameters k is 14.76208403250783 and b is -70.03895262929134\n",
      "Iteration 1979, the loss is 5.161682431324859, parameters k is 14.761608360571072 and b is -70.0389723921372\n",
      "Iteration 1980, the loss is 5.161681489360412, parameters k is 14.761742570057239 and b is -70.03899215498305\n",
      "Iteration 1981, the loss is 5.161680547395961, parameters k is 14.761876779543405 and b is -70.0390119178289\n",
      "Iteration 1982, the loss is 5.1616796054315115, parameters k is 14.762010989029571 and b is -70.03903168067475\n",
      "Iteration 1983, the loss is 5.161685768313135, parameters k is 14.762145198515737 and b is -70.0390514435206\n",
      "Iteration 1984, the loss is 5.161685906689848, parameters k is 14.761669526578979 and b is -70.03907120636646\n",
      "Iteration 1985, the loss is 5.1616849647254055, parameters k is 14.761803736065145 and b is -70.03909096921231\n",
      "Iteration 1986, the loss is 5.161684022760949, parameters k is 14.761937945551312 and b is -70.03911073205816\n",
      "Iteration 1987, the loss is 5.161685105944405, parameters k is 14.762072155037478 and b is -70.03913049490401\n",
      "Iteration 1988, the loss is 5.16169032401929, parameters k is 14.76159648310072 and b is -70.03915025774987\n",
      "Iteration 1989, the loss is 5.1616893820548375, parameters k is 14.761730692586886 and b is -70.03917002059572\n",
      "Iteration 1990, the loss is 5.161688440090391, parameters k is 14.761864902073052 and b is -70.03918978344157\n",
      "Iteration 1991, the loss is 5.161687498125938, parameters k is 14.761999111559218 and b is -70.03920954628742\n",
      "Iteration 1992, the loss is 5.161691530570433, parameters k is 14.762133321045384 and b is -70.03922930913328\n",
      "Iteration 1993, the loss is 5.161693799384281, parameters k is 14.761657649108626 and b is -70.03924907197913\n",
      "Iteration 1994, the loss is 5.161692857419828, parameters k is 14.761791858594792 and b is -70.03926883482498\n",
      "Iteration 1995, the loss is 5.161691915455376, parameters k is 14.761926068080959 and b is -70.03928859767083\n",
      "Iteration 1996, the loss is 5.161690985388391, parameters k is 14.762060277567125 and b is -70.03930836051669\n",
      "Iteration 1997, the loss is 5.1616941798196825, parameters k is 14.7618853961442 and b is -70.03932812336254\n",
      "Iteration 1998, the loss is 5.161693237855237, parameters k is 14.762019605630366 and b is -70.03934788620839\n",
      "Iteration 1999, the loss is 5.1616974265988445, parameters k is 14.762153815116532 and b is -70.03936764905424\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "k = random.random() * 200 - 100\n",
    "b = random.random() * 200 - 100\n",
    "num_iter = 2000\n",
    "losses = []\n",
    "lr = 0.01\n",
    "min_loss = float('inf')\n",
    "\n",
    "for i in range(num_iter):\n",
    "  current_price = [price(rm, k, b) for rm in x_rm]\n",
    "  current_loss = loss(y, current_price)\n",
    "  losses.append(current_loss)\n",
    "  print(\"Iteration {}, the loss is {}, parameters k is {} and b is {}\".format(i,current_loss,k,b))\n",
    "  k = k - lr * derivative_k(x_rm, y, current_price)\n",
    "  b = b - lr * derivative_b(x_rm, y, current_price)\n",
    "best_k = k\n",
    "best_b = b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "05eE0HgEpaWY",
    "outputId": "2e7bf76a-beda-4234-c3dd-e8c95c09070a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcfca10a358>]"
      ]
     },
     "execution_count": 73,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(list(range(len(losses))),losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "T04gCyL6q-AB",
    "outputId": "d129e36f-e069-4559-e1c3-60ae005e44f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcfca1172b0>]"
      ]
     },
     "execution_count": 74,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD4CAYAAAAJmJb0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deXyU1dXA8d9JGCDgElCKGk3BarFS\nFzSutNYdrSipWtSqVd9W2rdaxfpGsNYK1lqU1qW1WnGptEVBUZDFglXQCtYFDKiItCigBBUQIgop\nZDnvH8/MZGbyzP48s+V8Px8+5M723JlJztw5z73niqpijDGmNJXluwPGGGP8Y0HeGGNKmAV5Y4wp\nYRbkjTGmhFmQN8aYEtYl3x2ItPvuu2u/fv3y3Q1jjCkqixcv3qiqfdyuK6gg369fPxYtWpTvbhhj\nTFERkTXxrrN0jTHGlDAL8sYYU8IsyBtjTAmzIG+MMSXMkyAvIpUiMlVE3hWR5SJyjIj0FpF/iMh/\ngv/38uJYxhhjUufVSP5uYI6qHgAcAiwHRgPPq+r+wPPBtjHGmBzKOsiLyK7AccBDAKq6Q1UbgWHA\nxODNJgK12R7LGGNMerwYyfcHNgB/FpF6EXlQRHoCfVX1o+BtPgb6ut1ZREaIyCIRWbRhwwYPumOM\nMUXkySdh2jTfHt6LxVBdgMOAn6rqqyJyNzGpGVVVEXEtXK+qE4AJADU1NVbc3hjTOXz+OeyyS3u7\nrQ1EPD+MFyP5tcBaVX012J6KE/Q/EZE9AYL/r/fgWMYYU/x+//voAP/vf/sS4MGDIK+qHwMfisiA\n4EUnAe8AM4BLgpddAjyd7bGMMaaobdzoBPOrr3baV14JqrD//r4d0qvaNT8FJolIV+B94DKcD5DH\nReQHwBpguEfHMsaY4jNmDIwd295euxaqqnw/rCdBXlWXADUuV53kxeMbY0zR+vBDqK5ub998M9x4\nY84OX1BVKI0xpqT85Cdw333t7Y0bYbfdctoFK2tgjDFeW7HCyb2HAvw99zi59xwHeLCRvDHGeEcV\nzj0Xnnqq/bLPP4eddspbl2wkb4wxXli8GMrK2gP8o486QT+PAR5sJG+MMdlpa4NvfQsWLHDae+wB\nq1dDt2557VaIjeSNMSZTL7wA5eXtAf6ZZ+CjjwomwION5I0xJn0tLfC1r8HKlU774IPhjTecgF9g\nbCRvjDHpmD4dAoH2AL9gASxdWpABHmwkb4wxqWlqgr59ndkyAKeeCnPm+FZzxis2kjfGmGQeeQR6\n9GgP8EuXwty5BR/gwUbyxhgT32efQWVle/uii+Cvf81ffzJgI3ljjHFzxx3RAf6994ouwION5I0x\nJtr69U7uPeSaa5yAX6RsJG+MMSHXXx8d4NetK+oADxbkjTEG1qxxTqKOG+e0b73VKUmw55757ZcH\nLF1jjOncLr8cHnywvb1pE/Tqlb/+eMxG8saYzumdd5zReyjA33+/M3ovoQAPHo3kRWQ18DnQCrSo\nao2I9AamAP2A1cBwVd3sxfGMP6bXNzB+7grWNTaxV2UFdUMGUDvI/+3J/OL188nl62N9b3+chsYm\nykVoVaWyIoAIbN7WTJlAmzq3rawIMOasgdQOqkp+fFUYNgxmznTaXbs6o/eePV3vC3j22k2vb2Ds\nzGVs3tbcod9+EVXN/kGcIF+jqhsjLrsd2KSq40RkNNBLVUclepyamhpdtGhR1v0x6Zte38D1T71F\nU3Nr+LKKQDm/Ofugogz0Xj+fXL4+1nf3x0kmUCacd+Q+PLm4If7xX3sNjjqq/U5TpsDw4XGPGSgX\nUGhuU/fHS8P0+gbqpi6luTU65gbKhPHfPSSr90JEFquq2xasvqZrhgETgz9PBGp9PJbJ0vi5Kzr8\nQTU1tzJ+7oo89Sg7Xj+fXL4+1nf3x0mmuU157NUPXY//278vd4J7KMBXV8P27eEAH++Yza0aFeBD\nj5fJazd+7ooOAT7Ubz//zrwK8go8KyKLRWRE8LK+qvpR8OePgb5udxSRESKySEQWbdiwwaPumHSt\na2xK6/JC5/XzyeXrY33PvG+tLpmJwauXsOCGU5xRPMCzzzqzabp2zfiYDRn0L9Hj+/l35lWQ/4aq\nHgacDlwhIsdFXqlOTsg1L6SqE1S1RlVr+vTp41F3TLr2qqxI6/JC5/XzyeXrY33PvG/lEbVkurS2\nsOC+y5g05RfOBUccAa2tcMopUfeZXt/A4HHz3ANUHBK8XzoSPSc//848CfKq2hD8fz0wDTgS+ERE\n9gQI/r/ei2MZf9QNGUBFILpUakWgPHziqdh4/Xxy+fpY390fJ5lAmXDBUftQESjntBULWfnbWvbe\n4mQHXpw4wxnJl0WHvFAePt7IPFDuXoBMIe0US92QAa6PFygTX//Osp5dIyI9gTJV/Tz486nAzcAM\n4BJgXPD/p7M9lvFP6KRPqcyu8fr55PL1sb5HP04qqZHwLJUBvRh7/hGUb98OwMIBR7Hh0anUHra3\n6/0S5f6rgn0fOWWJ6/XpplhCz6noZteIyL44o3dwPjQeVdVfi8huwONANbAGZwrlpkSPZbNrjDGx\n+o+e7ZpKEWDVuDPaL3jwQWdhU8jbb8PAgVk/9uBx81w/aKoqK1g4+sSk/c+FRLNrsh7Jq+r7wCEu\nl38KnJTt4xtjSluyee17VVa4BtlwHruxMXoB02WXwcMPp3TspI+Nk2ZxmxZaLKlMW/FqjMmbyJy4\n4sxauf6pt6JOaibM8992W3SAX7Uq5QCf9LGDagdV8ZuzD6KqsgLBGcEX0/oRq11jjMm5yNWssULz\n0ENB1C3P/4vDe3F6ZJ591Kj24mJpSPUcQu2gqqIJ6rEsyBtjciqV1ayxJzWjgmxdHZz62/YrP/44\nujxwmoo5gKfCgrwxJqdSWc0ayolH5utr2jbzxPiLIx5oPPzf//nZ1ZJgQd4Yk1PJph6GcuKRI/7f\nzb6Dc96e136jzZujt+bLUqkV54tkQd4Yk1PxZrRA+9z02kFVDB43j+p17zH34SvD19edfhUvHzeM\nhR4H+Mj0UejkL1ASgd6CvDHGE6mOhuNNSYyasaLKrRPq+NaqNwD4omsFh1/5N7YHuiEe13lJVFTN\ngrwxxuA+Gr5myhIWrdnELbUHRd026YyWl1+GwYP5VvD2I75zA89+9Zjw/b2u81JqxfliWZA3xmTN\nbTSswN9e+YC/vfJBVBoG4sxoaW2FmhpY4pQR2LhnNUdf+HtayqPD1AkHeFvIMJUFUcXMFkMZY7KW\nbNTrtsgpypw50KVLOMDz/PMMu+rPHQI8wPx3vS1JXjdkAIGy6MJhfhcNyyUL8saYrKUy6nXdbGPH\nDthrLzj9dKd97LHOiP7EE3ObRoktDulefLIoWZA3xmStbsiAlOJiQ2MTg8fNo//o2fzyezdCt27w\nUXBvoddfh4ULw+WAc1UH323HpuZWf3dryiXLyRtjslY7qIpFazYx6ZUPEm6+IcCm9ZtZeddwyrUN\ngHUnns5ez80Gif6YyFVhsHjfDEIfSMU+d96CvDElJl8Le26pPYiaL/cO16QRoreDE+DC+me45dl7\nw5ed9MP7+O9XvspC6fg9IFd18OOdeBXat/kr5rnzWdeT95LVkzcmO251YTrMQc9hX0IBekDXZub8\n6jvh6yYdeho3DHEWOXWoC59jbq9Z7AdUSCHVkI/kaz15Y0zhKKSFPeFpkrfcAjfeGL782P99mHW7\nfCnczvdURbdvDPFW5Bbj3HnPgryIlAOLgAZVHSoi/YHJwG7AYuBiVd3h1fGMMR0V1MKedeugqv2D\nZcUPr6Z2j9MKcvON2Hn78XaDyvcHUia8nF1zNbA8on0bcKeq7gdsBn7g4bGMMS5yNSMlqauvjgrw\nrF/PgAfuSmnzjen1DeEZOIPHzYs/t95HpbSxvScjeRHZGzgD+DXwMxER4ETge8GbTATGAPd5cTxj\njLu8b1W3ciXsv397+847YeTIcDNZ7fZCKRZWShvbe5WuuQu4Dtg52N4NaFTVlmB7LeD66ojICGAE\nQHV1tUfdMaZzymtwuuACmDy5vb1lC+y8c/zbuyjIcwpFLusgLyJDgfWqulhEjk/3/qo6AZgAzuya\nbPtjTGeX8+C0ZAkMGtTe/stf4OKL498+AS/PKZRyjfh0eDGSHwycJSLfBroDuwB3A5Ui0iU4mt8b\nyH1izRjjH1U46SSYP99p9+4NDQ3QvXvGD+lVsbBCSfsUgqxPvKrq9aq6t6r2A84H5qnqhcB84Nzg\nzS4Bns72WMYYf6R9svOll5zyA6EAP2MGfPppVgEevDvhmSjt09n4OU9+FDBZRG4B6oGHfDyWMSZD\nv5j+VlQ5goSj3pYWOOQQeOcdp/21r8GbbzoVJD3gxTmF6fUNJTXPPVueBnlVfQF4Ifjz+8CRXj6+\nMSY1oXx0Q2MT5SK0qnao6R66nVu9GdeTnbNnw9Ch7e0XX4TjjvO879mcUwilaeIpxnnu2bIqlMaU\nmFCgC41mW4OlS9xquo+fuyJuQbFQga4Zr74Pffq0B/jjj4e2Nl8CfLbc0jQhxTrPPVsW5I2JoxAW\n5WQiUaCLzUsnS18csfAZzjr6K7Bxo3PBG284eXiXgmKFINHzyUf9nkJgtWuMcVHMszOSBe7I6yt7\nBNi8rbnDbXpu38ayu4aH2zMP+CbXnXM956zpwvy5hVt+N97snKrKioLqZy7ZSN4YF8U8OyNZ3jny\nercitJcsnhkV4I+//H5+OmwUTS1tTHrlAxoam1BS2NIvD0qpHIFXbCRvjIuCKvTlItFCn7ohA6h7\nYinNbR0jeOzepZ81tY/ie237jPo/XBhuP3LYUMac8uOo+6d0gtbD55KuUipH4BUL8sa48GpRjh+S\npZJqB1UxduYy1zRMc1v7tna1g6rCz/OalyZx9cuPhW931E8e4ZOdd0+pP9l88PmRFiuVcgResXSN\nMS4K+Wt/KqkktwAfEplm+eUhO7P6tqHhAP+7b1xIv1GzUg7wkN0HXzGnxYqFjeSNcVHIX/tTSSWF\n5sbH09TcStv//oQhr84IX3boVY/SWLFLWn3J9oOv0NNipcCCvDFxFOrX/lRSSYkC/L6frmXegxG5\n9nvugSuuoHH07LT6EVpcBWS84XUhp8VKhaVrjCkybqkkAU44oE+4XeUWJFW5d9qtUQH+5DGz6P9h\nPwaPm5fW1PfIvU7rpi6NmnFTN3VpyjNuCjktViosyBtTZGoHVXHO4VVExmQFnlzcEA6udUMGEChr\nv8XXP17J6tvP5Nv/fhmAq86sY/+fz2ZlE+HgnGDw30HoA2XszGU0t0bfsblVGTtzWcrPJZXdokzm\nLF1jTJ6kO3Uw8vZlIgmnM4Zm2DRu3c6UR0dz5FqnoNj6nr34xo8fZkeXALRmvn3D/Hc3APFP8CY6\n8RurUNNipcKCvDF5kO7Uwdjbx8u5R+a3B7y7mMmP/TzcvvTcMbzwlRpP+m8nRouHpWuMyYN0pw4m\nqkcTSYCnX18DX/1qOMC/86X+7Fv3tGcBHmDXigCDx82Le31lRcCzY5ns2EjemDyIV+882zroJ//n\nFYYd2V4O+HuX/JaX9zgg3BY6rlpNVxmwdUcLjU3xUzJDD9kzy6MYr9hI3pg8KE8wlcVtZkqyKYXd\nmrfz1p3f5YGnbgHgk6O/BW1tDL/6/PBMm/JgHj/RsSO53SpQ5lwRe7I1Vihnb/LPgrwxeZBoHrtb\n0a/I6ZGxzn3rOVbccQ4773BG+6dd9gfOHvZLEKF2UFV4mmLomImOHUkhatbLRUdX06W8HJeSOB1Y\nzr5wZJ2uEZHuwD+BbsHHm6qqN4lIf2AysBuwGLhYVXdkezxjSkFVnEVA4OTmr318KeCchA1tzxdr\n5+1beeuu88LtpwaewM+GXguARDx2qvl8tz6G5sKDs+Ap1cexxUyFw4uc/HbgRFX9QkQCwAIR+Tvw\nM+BOVZ0sIn8CfgDc58HxjMkZLyskRqobMoC6qUvjpj1aVbn+qbdYtGaT6/Z8P3htGjfOb982+Zs/\nepAPK/cIt3t0LWfwuHlxP0iScVuQlOro3BYzFZasg7yqKvBFsBkI/lPgROB7wcsnAmOwIG+KiNcV\nEiM/MHp0LU+a125qbuWxVz+MCvC7bW1k8T0XhdsPHFHLr0/8YYf7bt3RytYdmQV4t71gIX4JgjKB\nXboH+KypuaBq/BiHJ7NrRKQcJyWzH/BH4D2gUVVbgjdZC7i+6yIyAhgBUF1d7UV3jPFEommOqQax\nyA21I2e2bN2RWtojMn9e9+JErnjliXD7iCv+woadeqf0OKmKTdFEqhsyIOpDD5xRu61QLWyeBHlV\nbQUOFZFKYBpwQJK7RN53AjABoKamJtvZXcZ4JtsKibHfBDL95a76bD0L//Q/4fbtx32fe48ZTq8e\nAUhjZWkyydIshVyZ08Tn6Tx5VW0UkfnAMUCliHQJjub3BgpnjzBTNPzKiacikwqJsaUHUp3JEs+4\nv/+e8998Ntw++OrJbOm+Ez0CZdx05kBGTlmS1eOHlIukNCK3EgTFJ+splCLSJziCR0QqgFOA5cB8\n4NzgzS4Bns72WKZzCY2E87WnaLoVEmP7m02A32/jB6y+bWg4wF8/5Er6jZrFlu47AbCtuS3h/dMo\nKAlAm6oF7xLlxUh+T2BiMC9fBjyuqrNE5B1gsojcAtQDDyV6kGzkc7Rn/ONFTjwb6aYnMp2qGEWV\nB576FaesfA2A7eVdOPSqyTR17R51s3KRhLsnhea4NzQ2Jd1ABGzKYynzYnbNm8Agl8vfB47M9vGT\n8WOPSFMYCmHXoNhAP37uChat2cT8dzd0CPyp9Ct08rVXjwD/bW6lKWJEfsi6FTz912vD7Z8MG80z\nB3zD9XFaVRNOjxSI+kCK/TuJZFMeS1vR167J92jP+KcQdg1yG0T8LWJhUuSgIl5/y0VoU2XXigAi\n0Bg8WdoSnEJZ1tbKCxNGUP3ZJwCs3aUPJ4yYQHN55kW+FBg5ZQkjpyyhsiLAmLMG8puzDwrP9AmN\n7uNNlzSlo+iDfCGM9ow/TjigT1RAjbw8V1JJwYQGFYmmGAJR14XqrV+wZA6/mXtP+PYXDf8VC/p3\n+GKclcamZuqeWMr47x4Sd3qkKV1FH+QLYbRn/BGvyFUui1+lOlhY19iUMIcfWxKgW8sOVvzu7KjH\n2LfuadrKok/0eqW5Te3bbSdV9EE+3ujJcozFrxC+pcUbRLjdDuJPMYzs808XPsa1CyaF2z+uvZ45\nAwZ70NvE7Ntt51T0Qd4WaJSufH1Liy0/kIqGxiYGj5sX/t2bXt/A2JnLorbBiy0oBtDvupmktYO2\nCxFS2p91r8oKm4nWCYlmuVjDSzU1Nbpo0aJ8d8MUCLcZIZkuo48MbpU9AqgSVWsFcC0/kIly6bh9\n6q+evZeL658Jt8+/4FZeqT44i6M4BOhSLknr4ATKhPOO3IcnFzdYWYISJCKLVdV166+iH8mb0uXV\nt7TYD4vI0XVDYxMjpyyhTAjXSc922BMZb/t8sZnX/3hxuL25+84MuvqxLI8QIc4GHpHPJzS7xmai\ndU4W5E3OpZMyyHYZ/fT6Bq59fGnSxUCpbISRrgeevDm8qAng25f+nnf67uvpMeI9LVW467xDo+b3\nZ7u1oClOFuRNTuVy8dr0+gbqnkge4L325c3reHHCiHD7zT3246xL7vLlWPFWs+5aEejwOsdjM9FK\nmwX5IDshlRuppgzivR/pvE9jZiyj2Y8hegLP/PmnHLh+Vbgdu5mHlwS44Cj3PLsIKZVYsJlopc+C\nPFYaIZdSmRYZ2u4uFJ5D78eiNZuiAlrk+wTRuft+u1XQ2ORdGd5kBn68ktkTR4bbc756DD/+zg2+\nHlOBW2oPoubLvTt88F2TQnVKW+3aOViQx0oj5FKyaZHT6xtct7sL7ZIUm5poam5l7Mxl/Le5LSr4\nZ7rtXSbevHM4u+zYFm7XXPlXNvbs5ftxqxLMzU+UgwfnW4Ctfu0csi41XAoKYdFNvk2vb2DwuHn0\nHz2bwePm+VbON1n53vFzV8Sd3RIvt755W3P21R8zcOzqJay+bWg4wP/58DPpN2pWTgK80D43P/Re\nRb6HW7e3ECiPP//e8vCFIRd/dzaSx0oj5DJdlWxaZKIP1lRK5uaEKqtvPzPqoq+PfJwvuvXIXReC\n/8dLZTU2NRMoE3p2Le+w1aDl4QtDrv7ubCRP+ptDlJpE6So/1A6qYuHoE1k17gwWjj4x6hc63gdr\n6CRj7PuUa2csfykqwN9+3PfpN2pWTgN8rFAqK/Y9bG5TKnt05a7zDqWqsgLBSfHY4qfCkKu/OxvJ\nY6URcpGuSnVWjFstIgGO/Upv5r+7IS9pGYDytlbeGz8s6rKvXjuNHV0yLwfspXjfcEKF0zrL73Ix\nyVWa2IJ8UGf+Q/A7XRXva2m8zTcg+gP3hAP6dJgmmEsXvzGLX/3jT+F23elX8cTBp2b1mGUR9WZS\nTUCVi7BLRZeoFbuR17kF+s6ScixGuUoTZx3kRWQf4C9AX5zf1wmqereI9AamAP2A1cBwVd2c7fGM\n9/yu5Dl25jLXr6Vu0ySh4wdubJneXOnWvJ0Vd5wTdZlX5YAzmb7fpspNZw50fa/OObzKdb58Z0k5\nFqNcVdD1IiffAlyrqgcCRwNXiMiBwGjgeVXdH3g+2DYFqHZQFb85+yBf8rbT6xtcR57QcQTrlo+c\nXt+Q0+mQIde89LeoAP/Ds2+k36hZvtV7LxcJv/aVFe4poL0qK+K+V7fUHuTbe2j84effXSTPq1CK\nyNPAPcF/x6vqRyKyJ/CCqib8iLIqlKVn8Lh5aQVpAVaNOwNIvC+pX3b57xe8eff5UZdlWg74oqOr\nmf/uhpSef7LnbdUiTSI5q0IpIv1wNvV+Feirqh8Fr/oYJ53jdp8RwAiA6upqL7tjCkC6J5Ei85Gp\nbL3npdueuZvz3vpHuH3uhbexaO+BGT+e2+KteCKfd2efCGC85VmQF5GdgCeBkaq6RSJGPqqqIuL6\n266qE4AJ4IzkveqPKQyp7qwEHfORuVqM1vfzjbx676Xh9kc77cYxV0zM+nFTDfBuedjOPBHAeMuT\nIC8iAZwAP0lVnwpe/ImI7BmRrlnvxbFMu3wXVUvl+G4nl9y41VHZtSLge/2ZK16eQt1Lfw23T/2f\ne/h3n36+HjNSrx4BbjpzYNEG9Hz/DprkvJhdI8BDwHJVvSPiqhnAJcC44P9PZ3ss0y7fRdVSPX7o\n5zEzlsUN2L16BDrUUZle3+BrgP/S55/y2r2XhNsv9D+cS4eP9e148fTo2qVog2K+fwdNaryYXTMY\nuBg4UUSWBP99Gye4nyIi/wFODraNR3K9SjWb49cOqmLJTady0dEdz7kEyoWbzozOe0+vb+DaJ5Z6\n2+EIv3xuQlSAP+ynk/IS4ME9JZWrOkLZyvfvoElN1iN5VV2AMznAzUnZPr5xl++iapkcP15ZXLcK\niq0+1IGP3czj5hMv5+EjhiW4h/9iF74U0+g437+DJjW24rVI5buoWrx8ebLjp3JC0Y8g8fsZt3PW\n8n+G214WFMtm4+9QJcnQh1280fG1jzvfbAop0Of7d9CkxgqUFSk/iqqlmiaYXt/A1h0tHS4PlIkn\nq/W8DBIHfvI+q28bGg7wPzvjGk8LilUEyujRNbsFUqHR+vT6hrgfcK2q4dsUis5e2K9YeL4YKhu2\nGCo9Xs5sSGcBTrwFTr16BKj/ZXo1XdyeA8C1TyzNLmWjyqQpNzB4zZsAbO6+M0dfMZHtXbpm/pg4\nz7FxW7MvNXVCm4AkmnJaVVlRUJt92OyawpBoMZQFeQPED9zlIvxu+CFRf7j9R892TU9ErtpMRew2\nf9D+wQLws8eXZFTjpWbtMqZOGhVu/+CcG3l+v6NSvr8IdO9S7hq8Iz/44r1mPQJlbGtuS7vfAtx5\n3qHUPbE07t606b7GpnNIFOQtXWOA+HlwtzRBvHRKOmmWRNv8hWZnpDv+KG9rZe5DPwkH+P/stg9f\nqXs6rQAPcOy+vTnncPfRaGT/4r1mmQR4iHj9ElRQKBPJW8qmWGb9mGgW5Du50B9uonga2kc1JJtc\nbOh4I6csiXvMdY1NCbcBdHPCe6/z3vhhDNj4AQDDvzeOU394Hxcc2z9RzHS18L1NTHr1g7jXh0bv\nu8YpJJaJ0Os3fu4KmlvjP/N85eZD6byGxiaU6PMIprDZ7JpOLJ0CYJu3NTO9viFqdky6udhUj7dX\nZUXKM2y6tjTzr3svYbemLQD8q/ogLjj/1nBBsScXN1DZIxC3EmY8yb5FTK9vyKRmmavKigBjznJW\nvY6csiTp7SO/TeQqH26b3RcvC/IeKcYTUGNmdKzznkjkH3QmtVVSKTgmON8Ubpj2Voe9SWMNWzaf\nu2f9Ltw+49K7Wdb3K1G3aWpupVuXMioC7jn2TI168k12tGSWlom1Pfg40+sbUp6OGRpJ52o+vc2J\nL14W5D1QTAtYQjIpG5DKH3SiD7tk9xfgwuCq2EQBvuf2bSy7a3i4PeuAb3LlWdfFLQfc2NTM4K/0\n5uX3NmU8nz3W9pY2emXwDcFNaES8dXtLWv3L5cja5sQXL8vJe6AYl3cn6lu8LESyP+hkedtEOexy\nEe4871BuqT0oYd8uWTwzKsAff/n9XDlsVNJ67ws9DPAhqni2sXhDY5MntXr8GlnbnPjiZUHeA8X4\nVTZR3y48ujqjP+hkH3aJ4nCbasIRf69tn7H6tqGMfe5+AB45bCj9Rs1ide/oUatHafKUNDY1061L\nGb16ZH8CtjzBi5POc/JrZJ2rXYyM9yxd44Fi/Cobr8+9egRSrjETK94HR0Njk5MeSpDa2KuyIpzq\niR1xj1wwiZELHwu3j/rJI3yy8+4dHiNUrjiVk5deaWxqpiJQzl3nHer0NYNjJztfkOo3EL9H1lbj\nvjjZSN4DdUMGECiLHm95tcTfL3VDBhAoj+lzREXI2kFVLBx9IqvGncHC0Sem9Med6EPt+qfeipuu\nEeCEA/qEUz0he2zZyOrbhoYD/O++cSH9Rs1yDfDgfMjUDqoKrxxNV7cumf05RObC0z12aEQc7369\negRSesxyERtZG1cW5L0S+506l3mDTMUOEbNMWrvlbUOamltpbm1zfZkuDO6FGjmavfnZ+3jlvkvD\n7UOvepQ/DL4g4fFDHzKJ+jisab0AABHcSURBVJHI9pY2yssye+NC32JS/WAPlAl3nXdo+AM0Xs77\npjMHJn0+FYHyDquSjQmxIO8BtwUsza1a8CdeY5fON7dl1+dQ3jaerTtaoz5HQgH+ltqDwkFy30/X\nsvq2oXy/fjYAN57yY/qNmkVjxS7h+7kF4shURe2gKs45vMr1HECyGN7apuHRs+DMYU8l5x76gEk1\n0HbtUsb4uSvCq0eBuDnv2Hx4rx4BKisClhs3KbGcvAfyeeI10/n5fvU5VC43lX1dFZj/7gYA9tq1\nOz+feBNnrFgYvv7Aa55gW9f2VIVAVBEzt+c9vb4h7i5UoUVHofvG62PjtuaoQmu/mP4Wf3sl/grY\n2Fx4KlMrt+5oZesO5/ihWUi/OfuguMXHMsmHF+PaDeM9r/Z4fRgYCqxX1a8HL+sNTAH6AauB4aq6\n2YvjFZp8nXjNZn5+pvXgU3HCAX0SBsVI6xqbYPFiFv785PBlV51Zx4wDvxV1O7fqi7HPMdmK2p7d\n2rfaS1RgLPY1eOzVD+P2v0zoMJK+6cyB1E1dmrA8QSyv57gX49oN4w+v0jWPAKfFXDYaeF5V9wee\nD7ZLUr7mEGc6Pz/VevCZFqQKjc6TEW1j+pTRUOMUz9uwU2++eu20DgE+tAo2mWQramO/paT6vrUm\nqHHgNvWxdlAV4889JO2TsF5+8yvGtRvGH54EeVX9J7Ap5uJhwMTgzxOBWi+OVYjyNYc405RLvCJY\nO3VvH+lmU5AqlWB19Advsur2szhk9dvOBc88w8J/vkl5Rfeo24Xy9tmkoEJiR+ipvm+J5rC7nceI\nTJNUVgSSngeI179sFOPaDeMPP3PyfVX1o+DPHwN93W4kIiOAEQDV1R03ei4W+ZhDnGmaKN4feuQ8\n9mwKUiWag79zufDIHZex76bgh8XBB8Mbb0B5eXgUkGkeOd5xQ9y+DaTyvl1w1D4J00+Rr2dsmiTe\nKtbYGjVef/MrxrUbxh85mV2jzs4krt95VXWCqtaoak2fPn1y0Z2Ckk2N7kzTRKnUg89mJBivX/ft\n0sA/f3Fqe4BfsACWLoVy57aZnCiMfP22bu+YggqprAiET8ym83pPr29Imn6KfN1SKcIGzh+Dn9/8\nrAyBCfFzJP+JiOypqh+JyJ7Aeh+PVZSyPTmWacnfuiEDXLf6iwwA8UaCClEbT6fSr76BNv4xfjg7\nb98GwCdHf4u+L8+PqnOQyWvhNmouA2JrQ4YWffUbPTvq8mTHSKU0cuzrlmo6xItt/BJ9KGb6u2FK\nj2fb/4lIP2BWxOya8cCnqjpOREYDvVX1ukSP0dm2/4s3uyMX+3gmGzUnC3Dx9n+Ntfjmuzj8pmvC\n7dMu+wNrqvbrcN9MXot496msCNCzWxcnJ94jwBf/bYm7nV6iYyTaErFN1fV1i3efSKm+domksyev\nKX2Jtv/zagrlY8DxwO4isha4CRgHPC4iPwDWAMPjP0LnlM+TY4ly0aEPgKbmVspFXGeXJM3Pb9kC\nu+7K4cHmUwNP4GdDr3UaLvfN5LWId91nTc0sucmZ5z543Lykc9bTPXabatx9Vt2+JQXKhJ26dwlv\nAJ7thuvx5vjbJh7GjSdBXlXjrTc/yYvHL1WpnhzL5aKW2BFioumD8YLgW9eO4aA7xobbx414gA96\n7Rl1m4bGJvqPnh1+PpmcKEzlPql8YCY6R5FOn9w+HHv1CKCKZwE+WfrIZs+YWFbWII9SOTmW6701\nUz1xCC7Bbv16EAkH+AdrhtFv1KwOAT4k8vmccECftE8UpvL6JZtNkmgOfjonLyPfJ3A+HANlwhfb\nW2hsavbkvUvlvbHZMyaWBfk8SmWedq4XtaQ6EuwQ7G64Afq2z5I94oq/cMtJl6f0WE3Nrcx/d0Pa\naw0SvX6hWTQNjU1xa8Ulm4OfzvoHt/epuU07rEfI5r1L9t7Y7BnjxmrX5Fmyedq5ztvHS1FEnsyM\nSjusWQP9+oVvd/tx3+feY9xPvyTavzRUJjjdVIbbfWLTGhpx7FAapSrF1EmqfUrn/cj0vUu0DiDV\n52M6HwvyBS7Xi1riTa8cc9bAjgHk8svhwQfb25s28fT99ZBglkyq9WKy4TaqDs1LD82iCeXPr5my\nxJPzHMkWYsXeNhPx3hubUWMSsXRNgcv1opaUUhTvvOPMcQ8F+PvvdzY87dUraX/j1Ubfur0lpVx1\nKouZkn378eM8h9vzCpRJh41ZsnnvbAs+kwkbyRe4fCxqiZuiUIXaWpgxw2l37QqbNkHPnin3N/T/\n2JnLoqY2NjY1p734Kd5ipmTffrIp2RBPvOftdlk2751twWfS5dliKC90tsVQReX11+HII9vbU6bA\n8MyXPni5+Cn2PskWCvUfPdv13IBA3PnvxhQy3xdDmRLW1gbHHAOvvea0q6vhP/9xRvFZ8HLxU+zl\nyb5NWPEu05lYkDfxPfccnHJKe/vZZ6PbWfBr8VNIorRGKrV7jCkVduLVdNTc7EyLDAX0I46A1lbP\nAjxkdkLZq5PQdgLTdCY2kjfRnnoKzjmnvf3KK3DUUZ4fJpMTyl6ehLYTmKazsBOvxrFtG+y+OzQF\n0yFnnAEzZ0aVAzbGFKZEJ14tXWOc+e49e7YH+LffhlmzLMAbUwIsXdOZNTZCr17t7csug4cfzl9/\njDGes5F8Z3XbbdEBftUqC/DGlCAbyXc2H38Me0aU/h01CsaNy19/jDG+siDfmVx3HYwf397++OOo\n8sDGmNLje7pGRE4TkRUisjK416vJtVWrnJOooQB/++1OHRoL8MaUPF9H8iJSDvwROAVYC7wuIjNU\n9R0/j2siXHopTJzY3t68GSor89YdY0xu+T2SPxJYqarvq+oOYDIwzOdjGoC33nJG76EA/9BDzujd\nArwxnYrfOfkq4MOI9logavmkiIwARgBUV1f73J1OQBW+/W2YM8dp77STs/dqhRXfMqYzyvsUSlWd\noKo1qlrTp0+ffHenuP3rX1BW1h7gp02Dzz+3AG9MJ+b3SL4B2CeivXfwMuOl1laniFh9vdPebz9n\n96ZAIL/9Msbknd8j+deB/UWkv4h0Bc4HZvh8zM5l7lzo0qU9wD//vFPv3QK8MQafR/Kq2iIiVwJz\ngXLgYVVd5ucxO40dO6B/f1i3zmkfeyy89JKTrjHGmCDfF0Op6jPAM34fp1OZMgXOP7+9/frrUONa\ngM4Y08nZitdisnUr7LKLsyUfwNlnw9SpVi3SGBOXfbcvFn/6kzMdMhTgly+HJ5+0AG+MSchG8oVu\n0ybYbbf29o9+5AR8Y4xJgY3kC9mvfx0d4NessQBvjEmLjeQL0bp1UBWx/+gvfgG/+lX++mOMKVoW\n5AvNyJFw993t7fXrwVYCG2MyZOmaQrFypXMSNRTg77zTqUNjAd4YkwUbyReCCy+ERx9tb2/ZAjvv\nnL/+GGNKho3k82npUmf0Hgrwf/mLM3q3AG+M8YiN5PNBFU4+GebNc9q9e0NDA3Tvnt9+GWNKjo3k\nc23BAqe+TCjAz5gBn35qAd4Y4wsbyedKSwsceigsC9Zn+9rX4M03nQqSxhjjExvJ58Ls2U7p31CA\nf/FFp967BXhjjM8syvhp+3bYZx/YsMFpH3+8k6axejPGmByxkbxfJk1y8uyhAP/GGzB/vgV4Y0xO\n2Ujea59/7pQDDhk+HCZPtuBujMkLG8l76Q9/iA7w//63s8GHBXhjTJ5kFeRF5LsiskxE2kSkJua6\n60VkpYisEJEh2XWzwG3c6ATyq65y2lde6cyF33///PbLGNPpZTuSfxs4G/hn5IUiciDOpt0DgdOA\ne0WkPMtjFaYxY6Lry6xd64zojTGmAGQV5FV1uaqucLlqGDBZVber6ipgJXBkNscqOGvXOqP3sWOd\n9s03O6P3yBLBxhiTZ36deK0CXolorw1e1oGIjABGAFRXV/vUHY9deSX88Y/t7Y0bozf3MMaYApF0\nJC8iz4nI2y7/hnnRAVWdoKo1qlrTp9DL6q5Y4YzeQwH+nnuc0bsFeGNMgUo6klfVkzN43AZgn4j2\n3sHLipOqMxVy6tT2yz7/3NlY2xhjCphfUyhnAOeLSDcR6Q/sD7zm07H89cYbTkGxUIB/9FEn6FuA\nN8YUgaxy8iLyHeAPQB9gtogsUdUhqrpMRB4H3gFagCtUtTX77uZQW5tThuCll5z2HnvA6tXQrVs+\ne2WMMWnJKsir6jRgWpzrfg38OpvHz5sXXoATTmhvP/MMnH563rpjjDGZsrIGkVpaYOBAZ6UqwMEH\nO+ma8tKc4m+MKX1W1iBkxgynHHAowC9Y4GzPZwHeGFPEbCTf1OTk27dscdqnngpz5li9GWNMSejc\nI/mJE6FHj/YAv3QpzJ1rAd4YUzI650h+yxbYddf29kUXwV//mr/+GGOMTzrfSP7OO6MD/HvvWYA3\nxpSszjOSX78e+vZtb19zDdxxR/76Y4wxOdA5RvI33BAd4NetswBvjOkUSjvIf/CBcxL11lud9q23\nOiUJ9twzv/0yxpgcKd10zYgR8MAD7e1Nm6BXr/z1xxhj8qD0RvLLlzuj91CAv/9+Z/RuAd4Y0wmV\nzkheFWprnZWrAF27OqP3nj3z2y9jjMmj0gnyZRFfSqZMceq/G2NMJ1c6QX7YMFi1Cl5/3RnFG2OM\nKaEgP316vntgjDEFp/ROvBpjjAnLKsiLyHgReVdE3hSRaSJSGXHd9SKyUkRWiMiQ7LtqjDEmXdmO\n5P8BfF1VDwb+DVwPICIHAucDA4HTgHtFxAqzG2NMjmUV5FX1WVVtCTZfAfYO/jwMmKyq21V1FbAS\nODKbYxljjEmflzn5/wH+Hvy5Cvgw4rq1wcs6EJERIrJIRBZt2LDBw+4YY4xJOrtGRJ4D9nC56gZV\nfTp4mxuAFmBSuh1Q1QnABICamhpN9/7GGGPiSxrkVfXkRNeLyKXAUOAkVQ0F6QZgn4ib7R28zBhj\nTA5lO7vmNOA64CxV3RZx1QzgfBHpJiL9gf2B17I5ljHGmPRJ++A7gzuLrAS6AZ8GL3pFVX8cvO4G\nnDx9CzBSVf/u/ihRj7cBWJNxh3Jjd2BjvjuRI53ludrzLC2d8Xl+WVX7uN0oqyDfGYnIIlWtyXc/\ncqGzPFd7nqXFnmc0W/FqjDElzIK8McaUMAvy6ZuQ7w7kUGd5rvY8S4s9zwiWkzfGmBJmI3ljjClh\nFuSNMaaEWZBPk4iUi0i9iMzKd1/8IiKrReQtEVkiIovy3R+/iEiliEwNlsteLiLH5LtPXhORAcH3\nMfRvi4iMzHe//CAi14jIMhF5W0QeE5Hu+e6TH0Tk6uBzXJbKe1k6O0PlztXAcmCXfHfEZyeoaqkv\nKLkbmKOq54pIV6BHvjvkNVVdARwKzgAFp7zItLx2ygciUgVcBRyoqk0i8jhOufNH8toxj4nI14HL\ncar67gDmiMgsVV0Z7z42kk+DiOwNnAE8mO++mOyIyK7AccBDAKq6Q1Ub89sr350EvKeqhb6qPFNd\ngAoR6YLzgb0uz/3xw9eAV1V1W7DM+4vA2YnuYEE+PXfh1Oppy3dHfKbAsyKyWERG5LszPukPbAD+\nHEy/PSgiPfPdKZ+dDzyW7074QVUbgN8CHwAfAZ+p6rP57ZUv3ga+KSK7iUgP4NtEF4PswIJ8ikRk\nKLBeVRfnuy858A1VPQw4HbhCRI7Ld4d80AU4DLhPVQcBW4HR+e2Sf4LpqLOAJ/LdFz+ISC+czYr6\nA3sBPUXkovz2ynuquhy4DXgWmAMsAVoT3ceCfOoGA2eJyGpgMnCiiPwtv13yR3BUhKqux8nfluKu\nXmuBtar6arA9FSfol6rTgTdU9ZN8d8QnJwOrVHWDqjYDTwHH5rlPvlDVh1T1cFU9DtiMs/VqXBbk\nU6Sq16vq3qraD+dr7zxVLbmRgoj0FJGdQz8Dp+J8RSwpqvox8KGIDAhedBLwTh675LcLKNFUTdAH\nwNEi0kNEBOf9XJ7nPvlCRL4U/L8aJx//aKLb2+waE6svMM35O6EL8Kiqzslvl3zzU2BSMJXxPnBZ\nnvvji+CH9SnAj/LdF7+o6qsiMhV4A6e8eT2lW97gSRHZDWgGrkg2YcDKGhhjTAmzdI0xxpQwC/LG\nGFPCLMgbY0wJsyBvjDElzIK8McaUMAvyxhhTwizIG2NMCft/mudv850aqu8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x_rm,y)\n",
    "plt.plot(x_rm, best_k * x_rm + best_b, color = 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p89RdxCdjixN"
   },
   "source": [
    "<评阅点>\n",
    "+ 是否将Loss改成了“绝对值”(3')\n",
    "+ 是否完成了偏导的重新定义(5')\n",
    "+ 新的模型Loss是否能够收敛 (11’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dnVlOBANjixN"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment-03(1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
