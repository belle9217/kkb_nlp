{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习课上内容， 阅读相应论文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答以下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  What is autoencoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autoencoder 是通过一个编码和一个解码的过程，可以用于数据的降维、特征的提取或者用于生成模型中。Encoder过程利用神经网络对数据进行压缩，Decoder对数据进行解压、还原，使得输出与原始数据尽量相近。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What are the differences between greedy search and beam search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greedy Search会根据语言模型，每次挑选出在当前步骤计算出来的最优的选择。但是，在机器翻译等模型中，使用Greedy Search在一般情况下并不能得到整体最优的选择。\n",
    "\n",
    "Beam Search在当前步骤选择出B（B为超参数）个最优的选择。在考虑下一个最优选择时，会考虑前一个步骤中的B个最优选择，在所有B×n（n为单个步骤所有可能的选择个数）个选择中，再从中挑选出最优的B个选择。以此类推，得到最终的模型。\n",
    "\n",
    "Greedy Search可以看做B=1的Beam Search。因为使用Beam Search，每次会关注到更多的选择，因此通常会找到比Greedy Search更好的输出结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the intuition of attention mechanism?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention 可以帮助模型对输入的每个部分赋予不同的权重，从而使得模型关注到当前最关键的部分，使得模型的输出结果更好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the disadvantage of word embeding introduced in previous lectures ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前的Word Embedding无法处理遇到的一词多义问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What is the architecture of ELMo model. (A brief description is enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELMo模型使用了多层双向LSTM模型，损失函数是前向和后向的联合似然函数。前向过程包含了该词之前的语言信息，后向过程包含了该词之后的语言信息。因此，使用ELMo模型可以根据上下文关系，学习到词汇更为复杂的语义信息，解决一词多义的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Compared to RNN,  what is the advantage of Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer的优势：\n",
    "- 用注意力机制可以学习到上下文关系，解决一词多义问题。\n",
    "- Multi-Head可以并行加速计算。\n",
    "- 使用Positional Encoding来处理时序问题，解决长序列信息缺失的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Why we use layer normalizaiton instead of batch normalization in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在RNN网络中，当前时间步输出的改变和下一个时间的输入高度相关，因此不适合采用BN。而是采用LN分别对单个样本进行计算，对每个样本的所有特征进行标准化处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Why we need position embedding in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Transformer中使用Positional Encoding来获得每个单词的位置编码，从而使得Transformer模型可以处理时序问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Briefly describe what is self-attention and what is multi-head attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- self attention: 在attention计算过程中，Q=K=V。也就是说，对于输入句子中的每个词，都和该句子中的所有词进行attention计算，这样可以学习到句子内部的复杂结构。\n",
    "- multi-head attention: 岁Query, Key, Value进行线性变换后的结果，进行Attention计算，即$Attention(Q,K,V)=softmax(\\frac{Q \\dot K^{T}}{\\sqrt{d_k}}) V$。此过程计算h次（即多头），每次进行线性变换的参数W是不一样的。随后对attention计算的结果进行拼接，再进行一次线性变换得到多头attention的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. What is the basic unit of GPT model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT mode的基本单元是：Masked Multi Self Attention + Layer Norm + Feed Forward + Layer Norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Briefly descibe how to use GPT in other NLP tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT模型用于NLP Tasks时，需要按照不同的任务对输入数据进行修改。\n",
    "\n",
    "- Classification. 对于分类问题，不需要作修改。\n",
    "- Entailment. 对于推理问题，需要使用分隔符将先验与假设分开。\n",
    "- Similarity. 对于相似度问题，需要将句子顺序颠倒前后两次输入的结果相加后，进行推测。\n",
    "- Multiple Choice. 对于多问答问题，需要将上下文和问题放在一起，与答案分隔开再进行预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. What is masked language model in BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "masked language model: 在BERT模型中，由于采用了双向的transformer，导致模型在训练中能够看到整个句子，从而影响预测结果。因此，BERT在预测的句子中，随机将一些词替换成<'MASK'>标签，再通过上下文的语义分析，最终使得相应位置输出来预测被替换的词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. What are the inputs of BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT的输入包括：Word Vector（词向量）, Positional Encoding（位置编码，学习单词的位置关系）, Segment Embedding（段向量，学习句子的前后关系）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Briely descibe how to use BERT in other NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 单文本分类任务：对于文本分类任务，BERT模型在文本前插入一个[CLS]符号，并将该符号对应的输出向量作为整篇文本的语义表示，用于文本分类。\n",
    "- 语句对分类任务：除了添加[CLS]符号并将对应的输出作为文本的语义表示，还对输入的两句话用一个[SEP]符号作分割。\n",
    "- 序列标注任务：利用文本中每个字对应的输出向量对该字进行标注（Begin，Intermediate，End）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. What are the differences between these three models: GPT, BERT, GPT2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPT采取Pre Training + FineTuning两个阶段，预训练阶段采用“单向语言模型”。\n",
    "- BERT在预训练阶段采用“双向语言模型”，通过masked language mode解决预测问题。\n",
    "- GPT2也采用了“单向语言模型”，但相比GPT使用了更大的数据和更多的参数（更复杂的Transformer模型）。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
