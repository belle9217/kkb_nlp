{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 复习上课内容以及复现课程代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples = 506\n",
      "Epoch: 1, Loss: 182.347\n",
      "Epoch: 101, Loss: 6.605\n",
      "Epoch: 201, Loss: 6.580\n",
      "Epoch: 301, Loss: 4.393\n",
      "Epoch: 401, Loss: 4.884\n",
      "Epoch: 501, Loss: 3.808\n",
      "Epoch: 601, Loss: 4.406\n",
      "Epoch: 701, Loss: 3.892\n",
      "Epoch: 801, Loss: 3.918\n",
      "Epoch: 901, Loss: 4.252\n",
      "Epoch: 1001, Loss: 3.538\n",
      "Epoch: 1101, Loss: 3.287\n",
      "Epoch: 1201, Loss: 3.446\n",
      "Epoch: 1301, Loss: 3.901\n",
      "Epoch: 1401, Loss: 4.098\n",
      "Epoch: 1501, Loss: 3.930\n",
      "Epoch: 1601, Loss: 3.481\n",
      "Epoch: 1701, Loss: 4.042\n",
      "Epoch: 1801, Loss: 3.068\n",
      "Epoch: 1901, Loss: 3.661\n",
      "Epoch: 2001, Loss: 3.736\n",
      "Epoch: 2101, Loss: 3.547\n",
      "Epoch: 2201, Loss: 3.311\n",
      "Epoch: 2301, Loss: 4.002\n",
      "Epoch: 2401, Loss: 3.728\n",
      "Epoch: 2501, Loss: 3.767\n",
      "Epoch: 2601, Loss: 3.724\n",
      "Epoch: 2701, Loss: 3.941\n",
      "Epoch: 2801, Loss: 3.552\n",
      "Epoch: 2901, Loss: 3.164\n",
      "Epoch: 3001, Loss: 3.934\n",
      "Epoch: 3101, Loss: 3.147\n",
      "Epoch: 3201, Loss: 3.736\n",
      "Epoch: 3301, Loss: 4.088\n",
      "Epoch: 3401, Loss: 3.221\n",
      "Epoch: 3501, Loss: 3.597\n",
      "Epoch: 3601, Loss: 2.896\n",
      "Epoch: 3701, Loss: 3.482\n",
      "Epoch: 3801, Loss: 3.325\n",
      "Epoch: 3901, Loss: 3.266\n",
      "Epoch: 4001, Loss: 3.559\n",
      "Epoch: 4101, Loss: 3.786\n",
      "Epoch: 4201, Loss: 3.328\n",
      "Epoch: 4301, Loss: 3.773\n",
      "Epoch: 4401, Loss: 3.789\n",
      "Epoch: 4501, Loss: 2.810\n",
      "Epoch: 4601, Loss: 3.307\n",
      "Epoch: 4701, Loss: 3.511\n",
      "Epoch: 4801, Loss: 3.542\n",
      "Epoch: 4901, Loss: 3.497\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, inputs=[]):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = []\n",
    "        for n in self.inputs:\n",
    "            n.outputs.append(self)\n",
    "\n",
    "        self.value = None\n",
    "        \n",
    "        self.gradients = {}\n",
    "        \n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplemented\n",
    "    \n",
    "\n",
    "    def backward(self):\n",
    "        raise NotImplemented\n",
    "        \n",
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self, value=None):\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "        \n",
    "    def backward(self):\n",
    "        self.gradients = {self:0} # initialization \n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self] = grad_cost * 1\n",
    "            \n",
    "class Add(Node):\n",
    "    def __init__(self, *nodes):\n",
    "        Node.__init__(self, nodes)\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        self.value = sum(map(lambda n: n.value, self.inputs))\n",
    "\n",
    "class Linear(Node):\n",
    "    def __init__(self, nodes, weights, bias):\n",
    "        Node.__init__(self, [nodes, weights, bias])\n",
    "\n",
    "    def forward(self):\n",
    "        inputs = self.inputs[0].value\n",
    "        weights = self.inputs[1].value\n",
    "        bias = self.inputs[2].value\n",
    "\n",
    "        self.value = np.dot(inputs, weights) + bias\n",
    "        \n",
    "    def backward(self):\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "\n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self.inputs[0]] = np.dot(grad_cost, self.inputs[1].value.T)\n",
    "            self.gradients[self.inputs[1]] = np.dot(self.inputs[0].value.T, grad_cost)\n",
    "            self.gradients[self.inputs[2]] = np.sum(grad_cost, axis=0, keepdims=False)\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    def __init__(self, node):\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1./(1 + np.exp(-1 * x))\n",
    "\n",
    "    def forward(self):\n",
    "        self.x = self.inputs[0].value   # [0] input is a list\n",
    "        self.value = self._sigmoid(self.x)\n",
    "\n",
    "    def backward(self):\n",
    "        self.partial = self._sigmoid(self.x) * (1 - self._sigmoid(self.x))\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "\n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]  # Get the partial of the cost with respect to this node.\n",
    "\n",
    "            self.gradients[self.inputs[0]] = grad_cost * self.partial\n",
    "            # use * to keep all the dimension same!.\n",
    "\n",
    "\n",
    "\n",
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        y = self.inputs[0].value.reshape(-1, 1)\n",
    "        a = self.inputs[1].value.reshape(-1, 1)\n",
    "        assert(y.shape == a.shape)\n",
    "\n",
    "        self.m = self.inputs[0].value.shape[0]\n",
    "        self.diff = y - a\n",
    "\n",
    "        self.value = np.mean(self.diff**2)\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "        self.gradients[self.inputs[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inputs[1]] = (-2 / self.m) * self.diff\n",
    "\n",
    "\n",
    "def forward_and_backward(outputnode, graph):\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "\n",
    "    for n in  graph[::-1]:\n",
    "        n.backward()\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outputs:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outputs:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def sgd_update(trainables, learning_rate=1e-2):\n",
    "    for t in trainables:\n",
    "        t.value += -1 * learning_rate * t.gradients[t]\n",
    "\n",
    "\n",
    "losses = []\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "n_features = X_.shape[1]\n",
    "n_hidden = 10\n",
    "W1_ = np.random.randn(n_features, n_hidden)\n",
    "b1_ = np.zeros(n_hidden)\n",
    "W2_ = np.random.randn(n_hidden, 1)\n",
    "b2_ = np.zeros(1)\n",
    "\n",
    "X, y = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "cost = MSE(y, l2)\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}\n",
    "\n",
    "epochs = 5000\n",
    "m = X_.shape[0]\n",
    "batch_size = 16\n",
    "steps_per_epoch = m // batch_size\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "trainables = [W1, b1, W2, b2]\n",
    "\n",
    "print(\"Total number of examples = {}\".format(m))\n",
    "\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "\n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "\n",
    "        _ = None\n",
    "        forward_and_backward(_, graph) # set output node not important.\n",
    "\n",
    "        rate = 1e-2\n",
    "    \n",
    "        sgd_update(trainables, rate)\n",
    "\n",
    "        loss += graph[-1].value\n",
    "    \n",
    "    if i % 100 == 0: \n",
    "        print(\"Epoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(64, activation='sigmoid', input_dim=13))\n",
    "model.add(tf.keras.layers.Dense(32, activation='sigmoid', input_dim=64))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.compile(loss='mse', optimizer='sgd', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 506 samples\n",
      "Epoch 1/500\n",
      "506/506 [==============================] - 1s 2ms/sample - loss: 142.2903 - mse: 142.2903\n",
      "Epoch 2/500\n",
      "506/506 [==============================] - 0s 79us/sample - loss: 63.1810 - mse: 63.1810\n",
      "Epoch 3/500\n",
      "506/506 [==============================] - 0s 109us/sample - loss: 46.5026 - mse: 46.5026\n",
      "Epoch 4/500\n",
      "506/506 [==============================] - 0s 101us/sample - loss: 37.7618 - mse: 37.7617\n",
      "Epoch 5/500\n",
      "506/506 [==============================] - 0s 80us/sample - loss: 30.1853 - mse: 30.1853\n",
      "Epoch 6/500\n",
      "506/506 [==============================] - 0s 74us/sample - loss: 26.4742 - mse: 26.4742\n",
      "Epoch 7/500\n",
      "506/506 [==============================] - 0s 95us/sample - loss: 24.2501 - mse: 24.2501\n",
      "Epoch 8/500\n",
      "506/506 [==============================] - 0s 81us/sample - loss: 23.9674 - mse: 23.9674\n",
      "Epoch 9/500\n",
      "506/506 [==============================] - 0s 101us/sample - loss: 22.0256 - mse: 22.0256\n",
      "Epoch 10/500\n",
      "506/506 [==============================] - 0s 85us/sample - loss: 20.4568 - mse: 20.4568\n",
      "Epoch 11/500\n",
      "506/506 [==============================] - 0s 94us/sample - loss: 20.1672 - mse: 20.1672\n",
      "Epoch 12/500\n",
      "506/506 [==============================] - 0s 88us/sample - loss: 20.0406 - mse: 20.0406\n",
      "Epoch 13/500\n",
      "506/506 [==============================] - 0s 91us/sample - loss: 18.3512 - mse: 18.3512\n",
      "Epoch 14/500\n",
      "506/506 [==============================] - 0s 76us/sample - loss: 18.9996 - mse: 18.9996\n",
      "Epoch 15/500\n",
      "506/506 [==============================] - 0s 96us/sample - loss: 19.2353 - mse: 19.2353\n",
      "Epoch 16/500\n",
      "506/506 [==============================] - 0s 77us/sample - loss: 17.0217 - mse: 17.0217\n",
      "Epoch 17/500\n",
      "506/506 [==============================] - 0s 71us/sample - loss: 17.8839 - mse: 17.8839\n",
      "Epoch 18/500\n",
      "506/506 [==============================] - 0s 74us/sample - loss: 16.5313 - mse: 16.5313\n",
      "Epoch 19/500\n",
      "506/506 [==============================] - 0s 79us/sample - loss: 16.0444 - mse: 16.0444\n",
      "Epoch 20/500\n",
      "506/506 [==============================] - 0s 94us/sample - loss: 16.1609 - mse: 16.1609\n",
      "Epoch 21/500\n",
      "506/506 [==============================] - 0s 92us/sample - loss: 16.0646 - mse: 16.0646\n",
      "Epoch 22/500\n",
      "506/506 [==============================] - 0s 95us/sample - loss: 14.9890 - mse: 14.9890\n",
      "Epoch 23/500\n",
      "506/506 [==============================] - 0s 96us/sample - loss: 14.6669 - mse: 14.6669\n",
      "Epoch 24/500\n",
      "506/506 [==============================] - 0s 98us/sample - loss: 14.7411 - mse: 14.7411\n",
      "Epoch 25/500\n",
      "506/506 [==============================] - 0s 96us/sample - loss: 14.8563 - mse: 14.8563\n",
      "Epoch 26/500\n",
      "506/506 [==============================] - 0s 96us/sample - loss: 15.0193 - mse: 15.0193\n",
      "Epoch 27/500\n",
      "506/506 [==============================] - 0s 91us/sample - loss: 14.1540 - mse: 14.1540\n",
      "Epoch 28/500\n",
      "506/506 [==============================] - 0s 102us/sample - loss: 14.2102 - mse: 14.2102\n",
      "Epoch 29/500\n",
      "506/506 [==============================] - 0s 84us/sample - loss: 14.9902 - mse: 14.9902\n",
      "Epoch 30/500\n",
      "506/506 [==============================] - 0s 75us/sample - loss: 13.3817 - mse: 13.3817\n",
      "Epoch 31/500\n",
      "506/506 [==============================] - 0s 106us/sample - loss: 13.9355 - mse: 13.9355\n",
      "Epoch 32/500\n",
      "506/506 [==============================] - 0s 94us/sample - loss: 13.5203 - mse: 13.5203\n",
      "Epoch 33/500\n",
      "506/506 [==============================] - 0s 88us/sample - loss: 13.7879 - mse: 13.7879\n",
      "Epoch 34/500\n",
      "506/506 [==============================] - 0s 86us/sample - loss: 13.4319 - mse: 13.4319\n",
      "Epoch 35/500\n",
      "506/506 [==============================] - 0s 87us/sample - loss: 13.3208 - mse: 13.3208\n",
      "Epoch 36/500\n",
      "506/506 [==============================] - 0s 89us/sample - loss: 13.3798 - mse: 13.3798\n",
      "Epoch 37/500\n",
      "506/506 [==============================] - 0s 86us/sample - loss: 13.1637 - mse: 13.1637\n",
      "Epoch 38/500\n",
      "506/506 [==============================] - 0s 96us/sample - loss: 12.9236 - mse: 12.9236\n",
      "Epoch 39/500\n",
      "506/506 [==============================] - 0s 82us/sample - loss: 12.4913 - mse: 12.4913\n",
      "Epoch 40/500\n",
      "506/506 [==============================] - 0s 91us/sample - loss: 12.5344 - mse: 12.5344\n",
      "Epoch 41/500\n",
      "506/506 [==============================] - 0s 82us/sample - loss: 12.5634 - mse: 12.5634\n",
      "Epoch 42/500\n",
      "506/506 [==============================] - 0s 89us/sample - loss: 12.8415 - mse: 12.8415\n",
      "Epoch 43/500\n",
      "506/506 [==============================] - 0s 89us/sample - loss: 12.6177 - mse: 12.6177\n",
      "Epoch 44/500\n",
      "506/506 [==============================] - 0s 88us/sample - loss: 12.1381 - mse: 12.1381\n",
      "Epoch 45/500\n",
      "506/506 [==============================] - 0s 83us/sample - loss: 12.9547 - mse: 12.9547\n",
      "Epoch 46/500\n",
      "506/506 [==============================] - 0s 73us/sample - loss: 12.0057 - mse: 12.0057\n",
      "Epoch 47/500\n",
      "506/506 [==============================] - 0s 88us/sample - loss: 12.1261 - mse: 12.1261\n",
      "Epoch 48/500\n",
      "506/506 [==============================] - 0s 98us/sample - loss: 12.1691 - mse: 12.1691\n",
      "Epoch 49/500\n",
      "506/506 [==============================] - 0s 89us/sample - loss: 11.8879 - mse: 11.8879\n",
      "Epoch 50/500\n",
      "506/506 [==============================] - 0s 83us/sample - loss: 11.6838 - mse: 11.6838\n",
      "Epoch 51/500\n",
      "506/506 [==============================] - 0s 75us/sample - loss: 12.3528 - mse: 12.3528\n",
      "Epoch 52/500\n",
      "506/506 [==============================] - 0s 80us/sample - loss: 11.8205 - mse: 11.8205\n",
      "Epoch 53/500\n",
      "506/506 [==============================] - 0s 85us/sample - loss: 11.7143 - mse: 11.7143\n",
      "Epoch 54/500\n",
      "506/506 [==============================] - 0s 92us/sample - loss: 12.1837 - mse: 12.1837\n",
      "Epoch 55/500\n",
      "506/506 [==============================] - 0s 88us/sample - loss: 11.3572 - mse: 11.3572\n",
      "Epoch 56/500\n",
      "506/506 [==============================] - 0s 89us/sample - loss: 11.4342 - mse: 11.4342\n",
      "Epoch 57/500\n",
      "506/506 [==============================] - 0s 126us/sample - loss: 11.2042 - mse: 11.2042\n",
      "Epoch 58/500\n",
      "506/506 [==============================] - 0s 91us/sample - loss: 11.2182 - mse: 11.2182\n",
      "Epoch 59/500\n",
      "506/506 [==============================] - 0s 88us/sample - loss: 11.5611 - mse: 11.5611\n",
      "Epoch 60/500\n",
      "506/506 [==============================] - 0s 99us/sample - loss: 11.4716 - mse: 11.4716\n",
      "Epoch 61/500\n",
      "506/506 [==============================] - 0s 94us/sample - loss: 11.6859 - mse: 11.6859\n",
      "Epoch 62/500\n",
      "506/506 [==============================] - 0s 97us/sample - loss: 11.2215 - mse: 11.2215\n",
      "Epoch 63/500\n",
      "506/506 [==============================] - 0s 69us/sample - loss: 10.9566 - mse: 10.9566\n",
      "Epoch 64/500\n",
      "506/506 [==============================] - 0s 87us/sample - loss: 11.2530 - mse: 11.2530\n",
      "Epoch 65/500\n",
      "506/506 [==============================] - 0s 64us/sample - loss: 11.3330 - mse: 11.3330\n",
      "Epoch 66/500\n",
      "506/506 [==============================] - 0s 78us/sample - loss: 10.4734 - mse: 10.4734\n",
      "Epoch 67/500\n",
      "506/506 [==============================] - 0s 72us/sample - loss: 11.1001 - mse: 11.1001\n",
      "Epoch 68/500\n",
      "506/506 [==============================] - 0s 60us/sample - loss: 11.1623 - mse: 11.1623\n",
      "Epoch 69/500\n",
      "506/506 [==============================] - 0s 80us/sample - loss: 10.8437 - mse: 10.8437\n",
      "Epoch 70/500\n",
      "506/506 [==============================] - 0s 79us/sample - loss: 11.2325 - mse: 11.2325\n",
      "Epoch 71/500\n",
      "506/506 [==============================] - 0s 92us/sample - loss: 10.5991 - mse: 10.5991\n",
      "Epoch 72/500\n",
      "506/506 [==============================] - 0s 84us/sample - loss: 10.6992 - mse: 10.6992\n",
      "Epoch 73/500\n",
      "506/506 [==============================] - 0s 105us/sample - loss: 10.4435 - mse: 10.4435\n",
      "Epoch 74/500\n",
      "506/506 [==============================] - 0s 117us/sample - loss: 10.9595 - mse: 10.9595\n",
      "Epoch 75/500\n",
      "506/506 [==============================] - 0s 92us/sample - loss: 10.2663 - mse: 10.2663\n",
      "Epoch 76/500\n",
      "506/506 [==============================] - 0s 98us/sample - loss: 10.3298 - mse: 10.3298\n",
      "Epoch 77/500\n",
      "506/506 [==============================] - 0s 83us/sample - loss: 10.2683 - mse: 10.2683\n",
      "Epoch 78/500\n",
      "506/506 [==============================] - 0s 86us/sample - loss: 10.2662 - mse: 10.2662\n",
      "Epoch 79/500\n",
      "506/506 [==============================] - 0s 92us/sample - loss: 10.1747 - mse: 10.1747\n",
      "Epoch 80/500\n",
      "506/506 [==============================] - 0s 76us/sample - loss: 10.1246 - mse: 10.1246\n",
      "Epoch 81/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 89us/sample - loss: 10.0236 - mse: 10.0236\n",
      "Epoch 82/500\n",
      "506/506 [==============================] - 0s 73us/sample - loss: 9.9575 - mse: 9.9575\n",
      "Epoch 83/500\n",
      "506/506 [==============================] - 0s 73us/sample - loss: 9.8743 - mse: 9.8743\n",
      "Epoch 84/500\n",
      "506/506 [==============================] - 0s 87us/sample - loss: 10.2362 - mse: 10.2362\n",
      "Epoch 85/500\n",
      "506/506 [==============================] - 0s 76us/sample - loss: 10.4782 - mse: 10.4782\n",
      "Epoch 86/500\n",
      "506/506 [==============================] - 0s 90us/sample - loss: 10.0906 - mse: 10.0906\n",
      "Epoch 87/500\n",
      "506/506 [==============================] - 0s 96us/sample - loss: 9.9287 - mse: 9.9287\n",
      "Epoch 88/500\n",
      "506/506 [==============================] - 0s 93us/sample - loss: 10.3261 - mse: 10.3261\n",
      "Epoch 89/500\n",
      "506/506 [==============================] - 0s 95us/sample - loss: 10.0293 - mse: 10.0293\n",
      "Epoch 90/500\n",
      "506/506 [==============================] - 0s 106us/sample - loss: 9.6444 - mse: 9.6444\n",
      "Epoch 91/500\n",
      "506/506 [==============================] - 0s 65us/sample - loss: 9.6817 - mse: 9.6817\n",
      "Epoch 92/500\n",
      "506/506 [==============================] - 0s 74us/sample - loss: 9.8086 - mse: 9.8086\n",
      "Epoch 93/500\n",
      "506/506 [==============================] - 0s 69us/sample - loss: 9.4070 - mse: 9.4070\n",
      "Epoch 94/500\n",
      "506/506 [==============================] - 0s 52us/sample - loss: 9.3378 - mse: 9.3378\n",
      "Epoch 95/500\n",
      "506/506 [==============================] - 0s 79us/sample - loss: 9.7122 - mse: 9.7122\n",
      "Epoch 96/500\n",
      "506/506 [==============================] - 0s 67us/sample - loss: 9.6529 - mse: 9.6529\n",
      "Epoch 97/500\n",
      "506/506 [==============================] - 0s 72us/sample - loss: 9.4305 - mse: 9.4305\n",
      "Epoch 98/500\n",
      "506/506 [==============================] - 0s 75us/sample - loss: 9.6484 - mse: 9.6484\n",
      "Epoch 99/500\n",
      "506/506 [==============================] - 0s 73us/sample - loss: 9.4750 - mse: 9.4749\n",
      "Epoch 100/500\n",
      "506/506 [==============================] - 0s 63us/sample - loss: 9.3162 - mse: 9.3162\n",
      "Epoch 101/500\n",
      "506/506 [==============================] - 0s 75us/sample - loss: 9.0881 - mse: 9.0881\n",
      "Epoch 102/500\n",
      "506/506 [==============================] - 0s 83us/sample - loss: 9.4028 - mse: 9.4028\n",
      "Epoch 103/500\n",
      "506/506 [==============================] - 0s 55us/sample - loss: 9.1261 - mse: 9.1261\n",
      "Epoch 104/500\n",
      "506/506 [==============================] - 0s 71us/sample - loss: 8.9804 - mse: 8.9804\n",
      "Epoch 105/500\n",
      "506/506 [==============================] - 0s 61us/sample - loss: 8.9112 - mse: 8.9112\n",
      "Epoch 106/500\n",
      "506/506 [==============================] - 0s 78us/sample - loss: 8.8886 - mse: 8.8886\n",
      "Epoch 107/500\n",
      "506/506 [==============================] - 0s 71us/sample - loss: 8.9943 - mse: 8.9943\n",
      "Epoch 108/500\n",
      "506/506 [==============================] - 0s 78us/sample - loss: 8.8986 - mse: 8.8986\n",
      "Epoch 109/500\n",
      "506/506 [==============================] - 0s 55us/sample - loss: 8.8572 - mse: 8.8572\n",
      "Epoch 110/500\n",
      "506/506 [==============================] - 0s 59us/sample - loss: 9.1564 - mse: 9.1564\n",
      "Epoch 111/500\n",
      "506/506 [==============================] - 0s 63us/sample - loss: 8.8937 - mse: 8.8937\n",
      "Epoch 112/500\n",
      "506/506 [==============================] - 0s 84us/sample - loss: 8.8100 - mse: 8.8100\n",
      "Epoch 113/500\n",
      "506/506 [==============================] - 0s 107us/sample - loss: 8.6112 - mse: 8.6112\n",
      "Epoch 114/500\n",
      "506/506 [==============================] - 0s 146us/sample - loss: 8.5759 - mse: 8.5759\n",
      "Epoch 115/500\n",
      "506/506 [==============================] - 0s 119us/sample - loss: 8.7852 - mse: 8.7852\n",
      "Epoch 116/500\n",
      "506/506 [==============================] - 0s 103us/sample - loss: 8.8322 - mse: 8.8322\n",
      "Epoch 117/500\n",
      "506/506 [==============================] - 0s 127us/sample - loss: 8.7715 - mse: 8.7715\n",
      "Epoch 118/500\n",
      "506/506 [==============================] - 0s 93us/sample - loss: 8.3304 - mse: 8.3304\n",
      "Epoch 119/500\n",
      "506/506 [==============================] - 0s 100us/sample - loss: 8.4667 - mse: 8.4667\n",
      "Epoch 120/500\n",
      "506/506 [==============================] - 0s 63us/sample - loss: 8.4880 - mse: 8.4880\n",
      "Epoch 121/500\n",
      "506/506 [==============================] - 0s 75us/sample - loss: 8.5631 - mse: 8.5631\n",
      "Epoch 122/500\n",
      "506/506 [==============================] - 0s 61us/sample - loss: 8.5822 - mse: 8.5822\n",
      "Epoch 123/500\n",
      "506/506 [==============================] - 0s 63us/sample - loss: 8.2063 - mse: 8.2063\n",
      "Epoch 124/500\n",
      "506/506 [==============================] - 0s 65us/sample - loss: 8.1002 - mse: 8.1002\n",
      "Epoch 125/500\n",
      "506/506 [==============================] - 0s 64us/sample - loss: 8.3552 - mse: 8.3552\n",
      "Epoch 126/500\n",
      "506/506 [==============================] - 0s 62us/sample - loss: 8.2485 - mse: 8.2485\n",
      "Epoch 127/500\n",
      "506/506 [==============================] - 0s 67us/sample - loss: 8.0236 - mse: 8.0236\n",
      "Epoch 128/500\n",
      "506/506 [==============================] - 0s 76us/sample - loss: 8.0208 - mse: 8.0208\n",
      "Epoch 129/500\n",
      "506/506 [==============================] - 0s 82us/sample - loss: 8.2253 - mse: 8.2253\n",
      "Epoch 130/500\n",
      "506/506 [==============================] - 0s 68us/sample - loss: 7.9770 - mse: 7.9770\n",
      "Epoch 131/500\n",
      "506/506 [==============================] - 0s 62us/sample - loss: 7.8472 - mse: 7.8472\n",
      "Epoch 132/500\n",
      "506/506 [==============================] - 0s 66us/sample - loss: 8.3073 - mse: 8.3073\n",
      "Epoch 133/500\n",
      "506/506 [==============================] - 0s 63us/sample - loss: 8.0759 - mse: 8.0759\n",
      "Epoch 134/500\n",
      "506/506 [==============================] - 0s 61us/sample - loss: 8.1335 - mse: 8.1335\n",
      "Epoch 135/500\n",
      "506/506 [==============================] - 0s 54us/sample - loss: 7.6833 - mse: 7.6833\n",
      "Epoch 136/500\n",
      "506/506 [==============================] - 0s 57us/sample - loss: 7.9396 - mse: 7.9396\n",
      "Epoch 137/500\n",
      "506/506 [==============================] - 0s 74us/sample - loss: 8.0877 - mse: 8.0877\n",
      "Epoch 138/500\n",
      "506/506 [==============================] - 0s 76us/sample - loss: 7.5305 - mse: 7.5305\n",
      "Epoch 139/500\n",
      "506/506 [==============================] - 0s 108us/sample - loss: 7.5637 - mse: 7.5637\n",
      "Epoch 140/500\n",
      "506/506 [==============================] - 0s 89us/sample - loss: 7.7404 - mse: 7.7404\n",
      "Epoch 141/500\n",
      "506/506 [==============================] - 0s 83us/sample - loss: 7.4466 - mse: 7.4466\n",
      "Epoch 142/500\n",
      "506/506 [==============================] - 0s 92us/sample - loss: 7.4389 - mse: 7.4389\n",
      "Epoch 143/500\n",
      "506/506 [==============================] - 0s 88us/sample - loss: 7.6590 - mse: 7.6590\n",
      "Epoch 144/500\n",
      "506/506 [==============================] - 0s 92us/sample - loss: 7.4383 - mse: 7.4383\n",
      "Epoch 145/500\n",
      "506/506 [==============================] - 0s 103us/sample - loss: 7.4361 - mse: 7.4361\n",
      "Epoch 146/500\n",
      "506/506 [==============================] - 0s 101us/sample - loss: 7.3090 - mse: 7.3090\n",
      "Epoch 147/500\n",
      "506/506 [==============================] - 0s 95us/sample - loss: 7.6062 - mse: 7.6062\n",
      "Epoch 148/500\n",
      "506/506 [==============================] - 0s 82us/sample - loss: 7.4584 - mse: 7.4584\n",
      "Epoch 149/500\n",
      "506/506 [==============================] - 0s 77us/sample - loss: 7.1421 - mse: 7.1421\n",
      "Epoch 150/500\n",
      "506/506 [==============================] - 0s 75us/sample - loss: 7.0950 - mse: 7.0950\n",
      "Epoch 151/500\n",
      "506/506 [==============================] - 0s 78us/sample - loss: 7.3910 - mse: 7.3910\n",
      "Epoch 152/500\n",
      "506/506 [==============================] - 0s 71us/sample - loss: 7.1584 - mse: 7.1584\n",
      "Epoch 153/500\n",
      "506/506 [==============================] - 0s 82us/sample - loss: 7.2360 - mse: 7.2360\n",
      "Epoch 154/500\n",
      "506/506 [==============================] - 0s 80us/sample - loss: 7.0885 - mse: 7.0885\n",
      "Epoch 155/500\n",
      "506/506 [==============================] - 0s 87us/sample - loss: 7.0162 - mse: 7.0162\n",
      "Epoch 156/500\n",
      "506/506 [==============================] - 0s 70us/sample - loss: 7.2069 - mse: 7.2069\n",
      "Epoch 157/500\n",
      "506/506 [==============================] - 0s 68us/sample - loss: 6.9725 - mse: 6.9725\n",
      "Epoch 158/500\n",
      "506/506 [==============================] - 0s 86us/sample - loss: 7.0729 - mse: 7.0729\n",
      "Epoch 159/500\n",
      "506/506 [==============================] - 0s 82us/sample - loss: 6.8939 - mse: 6.8939\n",
      "Epoch 160/500\n",
      "506/506 [==============================] - 0s 74us/sample - loss: 6.7053 - mse: 6.7053\n",
      "Epoch 161/500\n",
      "506/506 [==============================] - 0s 60us/sample - loss: 6.8967 - mse: 6.8967\n",
      "Epoch 162/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 64us/sample - loss: 6.6101 - mse: 6.6101\n",
      "Epoch 163/500\n",
      "506/506 [==============================] - 0s 60us/sample - loss: 6.6171 - mse: 6.6171\n",
      "Epoch 164/500\n",
      "506/506 [==============================] - 0s 58us/sample - loss: 6.5034 - mse: 6.5034\n",
      "Epoch 165/500\n",
      "506/506 [==============================] - 0s 82us/sample - loss: 6.8488 - mse: 6.8488\n",
      "Epoch 166/500\n",
      "506/506 [==============================] - 0s 52us/sample - loss: 6.6965 - mse: 6.6965\n",
      "Epoch 167/500\n",
      "506/506 [==============================] - 0s 86us/sample - loss: 6.5972 - mse: 6.5972\n",
      "Epoch 168/500\n",
      "506/506 [==============================] - 0s 77us/sample - loss: 6.6373 - mse: 6.6373\n",
      "Epoch 169/500\n",
      "506/506 [==============================] - 0s 53us/sample - loss: 6.3791 - mse: 6.3791\n",
      "Epoch 170/500\n",
      "506/506 [==============================] - 0s 89us/sample - loss: 6.4731 - mse: 6.4731\n",
      "Epoch 171/500\n",
      "506/506 [==============================] - 0s 110us/sample - loss: 6.5655 - mse: 6.5655\n",
      "Epoch 172/500\n",
      "506/506 [==============================] - 0s 87us/sample - loss: 6.5524 - mse: 6.5524\n",
      "Epoch 173/500\n",
      "506/506 [==============================] - 0s 97us/sample - loss: 6.3556 - mse: 6.3556\n",
      "Epoch 174/500\n",
      "506/506 [==============================] - 0s 105us/sample - loss: 6.3650 - mse: 6.3650\n",
      "Epoch 175/500\n",
      "506/506 [==============================] - 0s 105us/sample - loss: 6.1357 - mse: 6.1357\n",
      "Epoch 176/500\n",
      "506/506 [==============================] - 0s 139us/sample - loss: 6.4328 - mse: 6.4328\n",
      "Epoch 177/500\n",
      "506/506 [==============================] - 0s 110us/sample - loss: 6.4326 - mse: 6.4326\n",
      "Epoch 178/500\n",
      "506/506 [==============================] - 0s 105us/sample - loss: 6.2818 - mse: 6.2818\n",
      "Epoch 179/500\n",
      "506/506 [==============================] - 0s 89us/sample - loss: 6.1403 - mse: 6.1403\n",
      "Epoch 180/500\n",
      "506/506 [==============================] - 0s 96us/sample - loss: 6.1425 - mse: 6.1425\n",
      "Epoch 181/500\n",
      "506/506 [==============================] - 0s 99us/sample - loss: 6.2730 - mse: 6.2730\n",
      "Epoch 182/500\n",
      "506/506 [==============================] - 0s 92us/sample - loss: 6.2560 - mse: 6.2560\n",
      "Epoch 183/500\n",
      "506/506 [==============================] - 0s 87us/sample - loss: 5.9896 - mse: 5.9896\n",
      "Epoch 184/500\n",
      "506/506 [==============================] - 0s 90us/sample - loss: 6.0997 - mse: 6.0997\n",
      "Epoch 185/500\n",
      "506/506 [==============================] - 0s 113us/sample - loss: 6.0649 - mse: 6.0648\n",
      "Epoch 186/500\n",
      "506/506 [==============================] - 0s 101us/sample - loss: 6.0371 - mse: 6.0371\n",
      "Epoch 187/500\n",
      "506/506 [==============================] - 0s 118us/sample - loss: 5.8412 - mse: 5.8412\n",
      "Epoch 188/500\n",
      "506/506 [==============================] - 0s 100us/sample - loss: 6.2135 - mse: 6.2135\n",
      "Epoch 189/500\n",
      "506/506 [==============================] - 0s 112us/sample - loss: 5.8961 - mse: 5.8961\n",
      "Epoch 190/500\n",
      "506/506 [==============================] - 0s 91us/sample - loss: 5.9463 - mse: 5.9463\n",
      "Epoch 191/500\n",
      "506/506 [==============================] - 0s 91us/sample - loss: 6.0311 - mse: 6.0311\n",
      "Epoch 192/500\n",
      "506/506 [==============================] - 0s 104us/sample - loss: 5.9622 - mse: 5.9622\n",
      "Epoch 193/500\n",
      "506/506 [==============================] - 0s 96us/sample - loss: 5.9151 - mse: 5.9151\n",
      "Epoch 194/500\n",
      "506/506 [==============================] - 0s 96us/sample - loss: 5.8916 - mse: 5.8916\n",
      "Epoch 195/500\n",
      "506/506 [==============================] - 0s 101us/sample - loss: 5.8843 - mse: 5.8843\n",
      "Epoch 196/500\n",
      "506/506 [==============================] - 0s 100us/sample - loss: 5.8409 - mse: 5.8409\n",
      "Epoch 197/500\n",
      "506/506 [==============================] - 0s 105us/sample - loss: 5.9586 - mse: 5.9586\n",
      "Epoch 198/500\n",
      "506/506 [==============================] - 0s 86us/sample - loss: 5.5552 - mse: 5.5552\n",
      "Epoch 199/500\n",
      "506/506 [==============================] - 0s 93us/sample - loss: 5.7460 - mse: 5.7460\n",
      "Epoch 200/500\n",
      "506/506 [==============================] - 0s 136us/sample - loss: 5.4988 - mse: 5.4988\n",
      "Epoch 201/500\n",
      "506/506 [==============================] - 0s 101us/sample - loss: 5.7839 - mse: 5.7839\n",
      "Epoch 202/500\n",
      "506/506 [==============================] - 0s 84us/sample - loss: 5.6745 - mse: 5.6745\n",
      "Epoch 203/500\n",
      "506/506 [==============================] - 0s 101us/sample - loss: 5.5654 - mse: 5.5654\n",
      "Epoch 204/500\n",
      "506/506 [==============================] - 0s 109us/sample - loss: 5.7078 - mse: 5.7078\n",
      "Epoch 205/500\n",
      "506/506 [==============================] - 0s 108us/sample - loss: 5.5698 - mse: 5.5698\n",
      "Epoch 206/500\n",
      "506/506 [==============================] - 0s 98us/sample - loss: 5.5582 - mse: 5.5582\n",
      "Epoch 207/500\n",
      "506/506 [==============================] - 0s 85us/sample - loss: 5.4130 - mse: 5.4130\n",
      "Epoch 208/500\n",
      "506/506 [==============================] - 0s 94us/sample - loss: 5.4572 - mse: 5.4572\n",
      "Epoch 209/500\n",
      "506/506 [==============================] - 0s 122us/sample - loss: 5.4361 - mse: 5.4361\n",
      "Epoch 210/500\n",
      "506/506 [==============================] - 0s 102us/sample - loss: 5.6916 - mse: 5.6916\n",
      "Epoch 211/500\n",
      "506/506 [==============================] - 0s 108us/sample - loss: 5.5667 - mse: 5.5667\n",
      "Epoch 212/500\n",
      "506/506 [==============================] - 0s 117us/sample - loss: 5.4050 - mse: 5.4050\n",
      "Epoch 213/500\n",
      "506/506 [==============================] - 0s 93us/sample - loss: 5.4020 - mse: 5.4020\n",
      "Epoch 214/500\n",
      "506/506 [==============================] - 0s 68us/sample - loss: 5.4023 - mse: 5.4023\n",
      "Epoch 215/500\n",
      "506/506 [==============================] - 0s 53us/sample - loss: 5.1890 - mse: 5.1890\n",
      "Epoch 216/500\n",
      "506/506 [==============================] - 0s 79us/sample - loss: 5.2603 - mse: 5.2603\n",
      "Epoch 217/500\n",
      "506/506 [==============================] - 0s 57us/sample - loss: 5.4718 - mse: 5.4718\n",
      "Epoch 218/500\n",
      "506/506 [==============================] - 0s 70us/sample - loss: 5.4144 - mse: 5.4144\n",
      "Epoch 219/500\n",
      "506/506 [==============================] - 0s 52us/sample - loss: 5.2582 - mse: 5.2582\n",
      "Epoch 220/500\n",
      "506/506 [==============================] - 0s 67us/sample - loss: 5.2837 - mse: 5.2837\n",
      "Epoch 221/500\n",
      "506/506 [==============================] - 0s 63us/sample - loss: 5.2051 - mse: 5.2051\n",
      "Epoch 222/500\n",
      "506/506 [==============================] - 0s 77us/sample - loss: 5.2614 - mse: 5.2614\n",
      "Epoch 223/500\n",
      "506/506 [==============================] - 0s 65us/sample - loss: 5.1784 - mse: 5.1784\n",
      "Epoch 224/500\n",
      "506/506 [==============================] - 0s 59us/sample - loss: 5.2869 - mse: 5.2869\n",
      "Epoch 225/500\n",
      "506/506 [==============================] - 0s 65us/sample - loss: 5.1789 - mse: 5.1789\n",
      "Epoch 226/500\n",
      "506/506 [==============================] - 0s 68us/sample - loss: 5.0576 - mse: 5.0576\n",
      "Epoch 227/500\n",
      "506/506 [==============================] - 0s 78us/sample - loss: 5.2842 - mse: 5.2842\n",
      "Epoch 228/500\n",
      "506/506 [==============================] - 0s 66us/sample - loss: 5.2817 - mse: 5.2817\n",
      "Epoch 229/500\n",
      "506/506 [==============================] - 0s 58us/sample - loss: 4.9757 - mse: 4.9757\n",
      "Epoch 230/500\n",
      "506/506 [==============================] - 0s 77us/sample - loss: 5.2050 - mse: 5.2050\n",
      "Epoch 231/500\n",
      "506/506 [==============================] - 0s 71us/sample - loss: 5.2508 - mse: 5.2508\n",
      "Epoch 232/500\n",
      "506/506 [==============================] - 0s 92us/sample - loss: 5.0417 - mse: 5.0417\n",
      "Epoch 233/500\n",
      "506/506 [==============================] - 0s 129us/sample - loss: 4.9435 - mse: 4.9435\n",
      "Epoch 234/500\n",
      "506/506 [==============================] - 0s 100us/sample - loss: 5.0105 - mse: 5.0105\n",
      "Epoch 235/500\n",
      "506/506 [==============================] - 0s 102us/sample - loss: 4.9860 - mse: 4.9860\n",
      "Epoch 236/500\n",
      "506/506 [==============================] - 0s 121us/sample - loss: 4.9789 - mse: 4.9789\n",
      "Epoch 237/500\n",
      "506/506 [==============================] - 0s 111us/sample - loss: 5.0873 - mse: 5.0873\n",
      "Epoch 238/500\n",
      "506/506 [==============================] - 0s 121us/sample - loss: 5.0507 - mse: 5.0507\n",
      "Epoch 239/500\n",
      "506/506 [==============================] - 0s 99us/sample - loss: 4.8358 - mse: 4.8358\n",
      "Epoch 240/500\n",
      "506/506 [==============================] - 0s 103us/sample - loss: 4.9280 - mse: 4.9280\n",
      "Epoch 241/500\n",
      "506/506 [==============================] - 0s 92us/sample - loss: 4.8936 - mse: 4.8936\n",
      "Epoch 242/500\n",
      "506/506 [==============================] - 0s 64us/sample - loss: 4.8846 - mse: 4.8846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 243/500\n",
      "506/506 [==============================] - 0s 105us/sample - loss: 4.8528 - mse: 4.8528\n",
      "Epoch 244/500\n",
      "506/506 [==============================] - 0s 101us/sample - loss: 5.0374 - mse: 5.0374\n",
      "Epoch 245/500\n",
      "506/506 [==============================] - 0s 87us/sample - loss: 4.7967 - mse: 4.7967\n",
      "Epoch 246/500\n",
      "506/506 [==============================] - 0s 88us/sample - loss: 4.8856 - mse: 4.8856\n",
      "Epoch 247/500\n",
      "506/506 [==============================] - 0s 93us/sample - loss: 4.6510 - mse: 4.6510\n",
      "Epoch 248/500\n",
      "506/506 [==============================] - 0s 107us/sample - loss: 4.7621 - mse: 4.7621\n",
      "Epoch 249/500\n",
      "506/506 [==============================] - 0s 118us/sample - loss: 4.6738 - mse: 4.6738\n",
      "Epoch 250/500\n",
      "506/506 [==============================] - 0s 104us/sample - loss: 4.7618 - mse: 4.7618\n",
      "Epoch 251/500\n",
      "506/506 [==============================] - 0s 93us/sample - loss: 5.0145 - mse: 5.0145\n",
      "Epoch 252/500\n",
      "506/506 [==============================] - 0s 120us/sample - loss: 4.8775 - mse: 4.8775\n",
      "Epoch 253/500\n",
      "506/506 [==============================] - 0s 96us/sample - loss: 4.6868 - mse: 4.6868\n",
      "Epoch 254/500\n",
      "506/506 [==============================] - 0s 103us/sample - loss: 4.7607 - mse: 4.7607\n",
      "Epoch 255/500\n",
      "506/506 [==============================] - 0s 98us/sample - loss: 4.6131 - mse: 4.6131\n",
      "Epoch 256/500\n",
      "506/506 [==============================] - 0s 102us/sample - loss: 4.8308 - mse: 4.8308\n",
      "Epoch 257/500\n",
      "506/506 [==============================] - 0s 101us/sample - loss: 4.6323 - mse: 4.6323\n",
      "Epoch 258/500\n",
      "506/506 [==============================] - 0s 95us/sample - loss: 4.7866 - mse: 4.7866\n",
      "Epoch 259/500\n",
      "506/506 [==============================] - 0s 149us/sample - loss: 4.5669 - mse: 4.5669\n",
      "Epoch 260/500\n",
      "506/506 [==============================] - 0s 86us/sample - loss: 4.7206 - mse: 4.7206\n",
      "Epoch 261/500\n",
      "506/506 [==============================] - 0s 104us/sample - loss: 4.7318 - mse: 4.7318\n",
      "Epoch 262/500\n",
      "506/506 [==============================] - 0s 126us/sample - loss: 4.6734 - mse: 4.6734\n",
      "Epoch 263/500\n",
      "506/506 [==============================] - 0s 103us/sample - loss: 4.5795 - mse: 4.5795\n",
      "Epoch 264/500\n",
      "506/506 [==============================] - 0s 111us/sample - loss: 4.4962 - mse: 4.4962\n",
      "Epoch 265/500\n",
      "506/506 [==============================] - 0s 125us/sample - loss: 4.8432 - mse: 4.8432\n",
      "Epoch 266/500\n",
      "506/506 [==============================] - 0s 112us/sample - loss: 4.5888 - mse: 4.5888\n",
      "Epoch 267/500\n",
      "506/506 [==============================] - 0s 113us/sample - loss: 4.6461 - mse: 4.6461\n",
      "Epoch 268/500\n",
      "506/506 [==============================] - 0s 102us/sample - loss: 4.4240 - mse: 4.4240\n",
      "Epoch 269/500\n",
      "506/506 [==============================] - 0s 95us/sample - loss: 4.5044 - mse: 4.5044\n",
      "Epoch 270/500\n",
      "506/506 [==============================] - 0s 98us/sample - loss: 4.4926 - mse: 4.4926\n",
      "Epoch 271/500\n",
      "506/506 [==============================] - 0s 105us/sample - loss: 4.4778 - mse: 4.4778\n",
      "Epoch 272/500\n",
      "506/506 [==============================] - 0s 90us/sample - loss: 4.5362 - mse: 4.5362\n",
      "Epoch 273/500\n",
      "506/506 [==============================] - 0s 112us/sample - loss: 4.4918 - mse: 4.4918\n",
      "Epoch 274/500\n",
      "506/506 [==============================] - 0s 106us/sample - loss: 4.5637 - mse: 4.5637\n",
      "Epoch 275/500\n",
      "506/506 [==============================] - 0s 129us/sample - loss: 4.5343 - mse: 4.5343\n",
      "Epoch 276/500\n",
      "506/506 [==============================] - 0s 89us/sample - loss: 4.5635 - mse: 4.5635\n",
      "Epoch 277/500\n",
      "506/506 [==============================] - 0s 91us/sample - loss: 4.5002 - mse: 4.5002\n",
      "Epoch 278/500\n",
      "506/506 [==============================] - 0s 117us/sample - loss: 4.4302 - mse: 4.4302\n",
      "Epoch 279/500\n",
      "506/506 [==============================] - 0s 111us/sample - loss: 4.4292 - mse: 4.4292\n",
      "Epoch 280/500\n",
      "506/506 [==============================] - 0s 103us/sample - loss: 4.4792 - mse: 4.4792\n",
      "Epoch 281/500\n",
      "506/506 [==============================] - 0s 97us/sample - loss: 4.3476 - mse: 4.3476\n",
      "Epoch 282/500\n",
      "506/506 [==============================] - 0s 131us/sample - loss: 4.3921 - mse: 4.3921\n",
      "Epoch 283/500\n",
      "506/506 [==============================] - 0s 131us/sample - loss: 4.4853 - mse: 4.4853\n",
      "Epoch 284/500\n",
      "506/506 [==============================] - 0s 97us/sample - loss: 4.2572 - mse: 4.2572\n",
      "Epoch 285/500\n",
      "506/506 [==============================] - 0s 109us/sample - loss: 4.4243 - mse: 4.4243\n",
      "Epoch 286/500\n",
      "506/506 [==============================] - 0s 100us/sample - loss: 4.2157 - mse: 4.2157\n",
      "Epoch 287/500\n",
      "506/506 [==============================] - 0s 84us/sample - loss: 4.4315 - mse: 4.4315\n",
      "Epoch 288/500\n",
      "506/506 [==============================] - 0s 82us/sample - loss: 4.3650 - mse: 4.3650\n",
      "Epoch 289/500\n",
      "506/506 [==============================] - 0s 61us/sample - loss: 4.2896 - mse: 4.2896\n",
      "Epoch 290/500\n",
      "506/506 [==============================] - 0s 75us/sample - loss: 4.3403 - mse: 4.3403\n",
      "Epoch 291/500\n",
      "506/506 [==============================] - 0s 101us/sample - loss: 4.2450 - mse: 4.2450\n",
      "Epoch 292/500\n",
      "506/506 [==============================] - 0s 104us/sample - loss: 4.5957 - mse: 4.5957\n",
      "Epoch 293/500\n",
      "506/506 [==============================] - 0s 104us/sample - loss: 4.1257 - mse: 4.1257\n",
      "Epoch 294/500\n",
      "506/506 [==============================] - 0s 87us/sample - loss: 4.3276 - mse: 4.3276\n",
      "Epoch 295/500\n",
      "506/506 [==============================] - 0s 82us/sample - loss: 4.2324 - mse: 4.2324\n",
      "Epoch 296/500\n",
      "506/506 [==============================] - 0s 90us/sample - loss: 4.2236 - mse: 4.2236\n",
      "Epoch 297/500\n",
      "506/506 [==============================] - 0s 91us/sample - loss: 4.3747 - mse: 4.3747\n",
      "Epoch 298/500\n",
      "506/506 [==============================] - 0s 68us/sample - loss: 4.1851 - mse: 4.1851\n",
      "Epoch 299/500\n",
      "506/506 [==============================] - 0s 70us/sample - loss: 4.1241 - mse: 4.1241\n",
      "Epoch 300/500\n",
      "506/506 [==============================] - 0s 88us/sample - loss: 4.2563 - mse: 4.2563\n",
      "Epoch 301/500\n",
      "506/506 [==============================] - 0s 60us/sample - loss: 4.3208 - mse: 4.3208\n",
      "Epoch 302/500\n",
      "506/506 [==============================] - 0s 84us/sample - loss: 4.2951 - mse: 4.2951\n",
      "Epoch 303/500\n",
      "506/506 [==============================] - 0s 88us/sample - loss: 4.2447 - mse: 4.2447\n",
      "Epoch 304/500\n",
      "506/506 [==============================] - 0s 95us/sample - loss: 4.2389 - mse: 4.2389\n",
      "Epoch 305/500\n",
      "506/506 [==============================] - 0s 95us/sample - loss: 4.1607 - mse: 4.1607\n",
      "Epoch 306/500\n",
      "506/506 [==============================] - 0s 92us/sample - loss: 4.1476 - mse: 4.1476\n",
      "Epoch 307/500\n",
      "506/506 [==============================] - 0s 89us/sample - loss: 4.1381 - mse: 4.1381\n",
      "Epoch 308/500\n",
      "506/506 [==============================] - 0s 96us/sample - loss: 4.0659 - mse: 4.0659\n",
      "Epoch 309/500\n",
      "506/506 [==============================] - 0s 101us/sample - loss: 4.0385 - mse: 4.0385\n",
      "Epoch 310/500\n",
      "506/506 [==============================] - 0s 98us/sample - loss: 4.0815 - mse: 4.0815\n",
      "Epoch 311/500\n",
      "506/506 [==============================] - 0s 102us/sample - loss: 3.8799 - mse: 3.8799\n",
      "Epoch 312/500\n",
      "506/506 [==============================] - 0s 97us/sample - loss: 4.1251 - mse: 4.1251\n",
      "Epoch 313/500\n",
      "506/506 [==============================] - 0s 89us/sample - loss: 4.0376 - mse: 4.0376\n",
      "Epoch 314/500\n",
      "506/506 [==============================] - 0s 103us/sample - loss: 4.0783 - mse: 4.0783\n",
      "Epoch 315/500\n",
      "506/506 [==============================] - 0s 95us/sample - loss: 4.2310 - mse: 4.2310\n",
      "Epoch 316/500\n",
      "506/506 [==============================] - 0s 101us/sample - loss: 3.9580 - mse: 3.9580\n",
      "Epoch 317/500\n",
      "506/506 [==============================] - 0s 89us/sample - loss: 3.9695 - mse: 3.9695\n",
      "Epoch 318/500\n",
      "506/506 [==============================] - 0s 122us/sample - loss: 3.9661 - mse: 3.9661\n",
      "Epoch 319/500\n",
      "506/506 [==============================] - 0s 116us/sample - loss: 3.9590 - mse: 3.9590\n",
      "Epoch 320/500\n",
      "506/506 [==============================] - 0s 101us/sample - loss: 4.0066 - mse: 4.0066\n",
      "Epoch 321/500\n",
      "506/506 [==============================] - 0s 103us/sample - loss: 4.0700 - mse: 4.0700\n",
      "Epoch 322/500\n",
      "506/506 [==============================] - 0s 88us/sample - loss: 4.0123 - mse: 4.0123\n",
      "Epoch 323/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 119us/sample - loss: 3.9802 - mse: 3.9802\n",
      "Epoch 324/500\n",
      "506/506 [==============================] - 0s 88us/sample - loss: 3.9014 - mse: 3.9014\n",
      "Epoch 325/500\n",
      "506/506 [==============================] - 0s 90us/sample - loss: 4.1160 - mse: 4.1160\n",
      "Epoch 326/500\n",
      "506/506 [==============================] - 0s 85us/sample - loss: 4.0834 - mse: 4.0834\n",
      "Epoch 327/500\n",
      "506/506 [==============================] - 0s 85us/sample - loss: 3.9887 - mse: 3.9887\n",
      "Epoch 328/500\n",
      "506/506 [==============================] - 0s 81us/sample - loss: 4.0160 - mse: 4.0160\n",
      "Epoch 329/500\n",
      "506/506 [==============================] - 0s 94us/sample - loss: 3.8797 - mse: 3.8797\n",
      "Epoch 330/500\n",
      "506/506 [==============================] - 0s 98us/sample - loss: 4.1530 - mse: 4.1530\n",
      "Epoch 331/500\n",
      "506/506 [==============================] - 0s 111us/sample - loss: 3.9232 - mse: 3.9232\n",
      "Epoch 332/500\n",
      "506/506 [==============================] - 0s 125us/sample - loss: 3.8089 - mse: 3.8089\n",
      "Epoch 333/500\n",
      "506/506 [==============================] - 0s 117us/sample - loss: 3.9701 - mse: 3.9701\n",
      "Epoch 334/500\n",
      "506/506 [==============================] - 0s 104us/sample - loss: 4.1416 - mse: 4.1416\n",
      "Epoch 335/500\n",
      "506/506 [==============================] - 0s 105us/sample - loss: 3.8461 - mse: 3.8461\n",
      "Epoch 336/500\n",
      "506/506 [==============================] - 0s 125us/sample - loss: 3.7540 - mse: 3.7540\n",
      "Epoch 337/500\n",
      "506/506 [==============================] - 0s 94us/sample - loss: 3.8683 - mse: 3.8683\n",
      "Epoch 338/500\n",
      "506/506 [==============================] - 0s 114us/sample - loss: 3.8280 - mse: 3.8280\n",
      "Epoch 339/500\n",
      "506/506 [==============================] - 0s 105us/sample - loss: 3.7266 - mse: 3.7266\n",
      "Epoch 340/500\n",
      "506/506 [==============================] - 0s 116us/sample - loss: 3.7685 - mse: 3.7685\n",
      "Epoch 341/500\n",
      "506/506 [==============================] - 0s 100us/sample - loss: 3.9031 - mse: 3.9031\n",
      "Epoch 342/500\n",
      "506/506 [==============================] - 0s 106us/sample - loss: 3.7491 - mse: 3.7491\n",
      "Epoch 343/500\n",
      "506/506 [==============================] - 0s 131us/sample - loss: 3.8115 - mse: 3.8115\n",
      "Epoch 344/500\n",
      "506/506 [==============================] - 0s 74us/sample - loss: 3.8448 - mse: 3.8448\n",
      "Epoch 345/500\n",
      "506/506 [==============================] - 0s 72us/sample - loss: 3.6649 - mse: 3.6649\n",
      "Epoch 346/500\n",
      "506/506 [==============================] - 0s 65us/sample - loss: 3.6865 - mse: 3.6865\n",
      "Epoch 347/500\n",
      "506/506 [==============================] - 0s 73us/sample - loss: 3.8678 - mse: 3.8678\n",
      "Epoch 348/500\n",
      "506/506 [==============================] - 0s 65us/sample - loss: 3.8006 - mse: 3.8006\n",
      "Epoch 349/500\n",
      "506/506 [==============================] - 0s 83us/sample - loss: 3.8341 - mse: 3.8341\n",
      "Epoch 350/500\n",
      "506/506 [==============================] - 0s 72us/sample - loss: 3.8281 - mse: 3.8281\n",
      "Epoch 351/500\n",
      "506/506 [==============================] - 0s 71us/sample - loss: 3.7222 - mse: 3.7222\n",
      "Epoch 352/500\n",
      "506/506 [==============================] - 0s 118us/sample - loss: 3.5522 - mse: 3.5522\n",
      "Epoch 353/500\n",
      "506/506 [==============================] - 0s 103us/sample - loss: 3.7492 - mse: 3.7492\n",
      "Epoch 354/500\n",
      "506/506 [==============================] - 0s 137us/sample - loss: 3.5840 - mse: 3.5840\n",
      "Epoch 355/500\n",
      "506/506 [==============================] - 0s 87us/sample - loss: 3.8468 - mse: 3.8468\n",
      "Epoch 356/500\n",
      "506/506 [==============================] - 0s 130us/sample - loss: 3.6321 - mse: 3.6321\n",
      "Epoch 357/500\n",
      "506/506 [==============================] - 0s 128us/sample - loss: 3.7088 - mse: 3.7088\n",
      "Epoch 358/500\n",
      "506/506 [==============================] - 0s 101us/sample - loss: 3.7145 - mse: 3.7145\n",
      "Epoch 359/500\n",
      "506/506 [==============================] - 0s 87us/sample - loss: 3.5904 - mse: 3.5904\n",
      "Epoch 360/500\n",
      "506/506 [==============================] - 0s 116us/sample - loss: 3.6652 - mse: 3.6652\n",
      "Epoch 361/500\n",
      "506/506 [==============================] - 0s 81us/sample - loss: 3.7200 - mse: 3.7200\n",
      "Epoch 362/500\n",
      "506/506 [==============================] - 0s 111us/sample - loss: 3.5991 - mse: 3.5991\n",
      "Epoch 363/500\n",
      "506/506 [==============================] - 0s 135us/sample - loss: 3.6896 - mse: 3.6896\n",
      "Epoch 364/500\n",
      "506/506 [==============================] - 0s 133us/sample - loss: 3.5583 - mse: 3.5583\n",
      "Epoch 365/500\n",
      "506/506 [==============================] - 0s 96us/sample - loss: 3.8410 - mse: 3.8410\n",
      "Epoch 366/500\n",
      "506/506 [==============================] - 0s 122us/sample - loss: 3.7204 - mse: 3.7204\n",
      "Epoch 367/500\n",
      "506/506 [==============================] - 0s 121us/sample - loss: 3.5595 - mse: 3.5595\n",
      "Epoch 368/500\n",
      "506/506 [==============================] - 0s 110us/sample - loss: 3.8592 - mse: 3.8592\n",
      "Epoch 369/500\n",
      "506/506 [==============================] - 0s 132us/sample - loss: 3.4748 - mse: 3.4748\n",
      "Epoch 370/500\n",
      "506/506 [==============================] - 0s 109us/sample - loss: 3.6066 - mse: 3.6066\n",
      "Epoch 371/500\n",
      "506/506 [==============================] - 0s 91us/sample - loss: 3.6233 - mse: 3.6233\n",
      "Epoch 372/500\n",
      "506/506 [==============================] - 0s 119us/sample - loss: 3.5447 - mse: 3.5447\n",
      "Epoch 373/500\n",
      "506/506 [==============================] - 0s 132us/sample - loss: 3.5415 - mse: 3.5415\n",
      "Epoch 374/500\n",
      "506/506 [==============================] - 0s 101us/sample - loss: 3.5068 - mse: 3.5068\n",
      "Epoch 375/500\n",
      "506/506 [==============================] - 0s 110us/sample - loss: 3.5940 - mse: 3.5940\n",
      "Epoch 376/500\n",
      "506/506 [==============================] - 0s 131us/sample - loss: 3.5774 - mse: 3.5774\n",
      "Epoch 377/500\n",
      "506/506 [==============================] - 0s 92us/sample - loss: 3.5513 - mse: 3.5513\n",
      "Epoch 378/500\n",
      "506/506 [==============================] - 0s 123us/sample - loss: 3.5793 - mse: 3.5793\n",
      "Epoch 379/500\n",
      "506/506 [==============================] - 0s 111us/sample - loss: 3.5508 - mse: 3.5508\n",
      "Epoch 380/500\n",
      "506/506 [==============================] - 0s 99us/sample - loss: 3.4415 - mse: 3.4415\n",
      "Epoch 381/500\n",
      "506/506 [==============================] - 0s 86us/sample - loss: 3.4867 - mse: 3.4867\n",
      "Epoch 382/500\n",
      "506/506 [==============================] - 0s 97us/sample - loss: 3.4779 - mse: 3.4779\n",
      "Epoch 383/500\n",
      "506/506 [==============================] - 0s 97us/sample - loss: 3.5320 - mse: 3.5320\n",
      "Epoch 384/500\n",
      "506/506 [==============================] - 0s 108us/sample - loss: 3.4657 - mse: 3.4657\n",
      "Epoch 385/500\n",
      "506/506 [==============================] - 0s 109us/sample - loss: 3.5334 - mse: 3.5334\n",
      "Epoch 386/500\n",
      "506/506 [==============================] - 0s 109us/sample - loss: 3.6054 - mse: 3.6054\n",
      "Epoch 387/500\n",
      "506/506 [==============================] - 0s 108us/sample - loss: 3.4273 - mse: 3.4273\n",
      "Epoch 388/500\n",
      "506/506 [==============================] - 0s 110us/sample - loss: 3.4006 - mse: 3.4006\n",
      "Epoch 389/500\n",
      "506/506 [==============================] - 0s 105us/sample - loss: 3.4564 - mse: 3.4564\n",
      "Epoch 390/500\n",
      "506/506 [==============================] - 0s 90us/sample - loss: 3.3557 - mse: 3.3557\n",
      "Epoch 391/500\n",
      "506/506 [==============================] - 0s 100us/sample - loss: 3.5800 - mse: 3.5800\n",
      "Epoch 392/500\n",
      "506/506 [==============================] - 0s 87us/sample - loss: 3.3260 - mse: 3.3260\n",
      "Epoch 393/500\n",
      "506/506 [==============================] - 0s 112us/sample - loss: 3.3212 - mse: 3.3212\n",
      "Epoch 394/500\n",
      "506/506 [==============================] - 0s 100us/sample - loss: 3.2987 - mse: 3.2987\n",
      "Epoch 395/500\n",
      "506/506 [==============================] - 0s 85us/sample - loss: 3.3507 - mse: 3.3507\n",
      "Epoch 396/500\n",
      "506/506 [==============================] - 0s 74us/sample - loss: 3.3809 - mse: 3.3809\n",
      "Epoch 397/500\n",
      "506/506 [==============================] - 0s 91us/sample - loss: 3.3973 - mse: 3.3973\n",
      "Epoch 398/500\n",
      "506/506 [==============================] - 0s 74us/sample - loss: 3.3380 - mse: 3.3380\n",
      "Epoch 399/500\n",
      "506/506 [==============================] - 0s 102us/sample - loss: 3.3211 - mse: 3.3211\n",
      "Epoch 400/500\n",
      "506/506 [==============================] - 0s 78us/sample - loss: 3.2983 - mse: 3.2983\n",
      "Epoch 401/500\n",
      "506/506 [==============================] - 0s 85us/sample - loss: 3.2671 - mse: 3.2671\n",
      "Epoch 402/500\n",
      "506/506 [==============================] - 0s 86us/sample - loss: 3.3160 - mse: 3.3160\n",
      "Epoch 403/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 94us/sample - loss: 3.3736 - mse: 3.3736\n",
      "Epoch 404/500\n",
      "506/506 [==============================] - 0s 74us/sample - loss: 3.4599 - mse: 3.4599\n",
      "Epoch 405/500\n",
      "506/506 [==============================] - 0s 73us/sample - loss: 3.3253 - mse: 3.3253\n",
      "Epoch 406/500\n",
      "506/506 [==============================] - 0s 78us/sample - loss: 3.2729 - mse: 3.2729\n",
      "Epoch 407/500\n",
      "506/506 [==============================] - 0s 62us/sample - loss: 3.3047 - mse: 3.3047\n",
      "Epoch 408/500\n",
      "506/506 [==============================] - 0s 71us/sample - loss: 3.2390 - mse: 3.2390\n",
      "Epoch 409/500\n",
      "506/506 [==============================] - 0s 65us/sample - loss: 3.2541 - mse: 3.2541\n",
      "Epoch 410/500\n",
      "506/506 [==============================] - 0s 65us/sample - loss: 3.3193 - mse: 3.3193\n",
      "Epoch 411/500\n",
      "506/506 [==============================] - 0s 69us/sample - loss: 3.2651 - mse: 3.2651\n",
      "Epoch 412/500\n",
      "506/506 [==============================] - 0s 70us/sample - loss: 3.3914 - mse: 3.3914\n",
      "Epoch 413/500\n",
      "506/506 [==============================] - 0s 92us/sample - loss: 3.3799 - mse: 3.3799\n",
      "Epoch 414/500\n",
      "506/506 [==============================] - 0s 104us/sample - loss: 3.3099 - mse: 3.3099\n",
      "Epoch 415/500\n",
      "506/506 [==============================] - 0s 122us/sample - loss: 3.2017 - mse: 3.2017\n",
      "Epoch 416/500\n",
      "506/506 [==============================] - 0s 128us/sample - loss: 3.4411 - mse: 3.4411\n",
      "Epoch 417/500\n",
      "506/506 [==============================] - 0s 99us/sample - loss: 3.1742 - mse: 3.1742\n",
      "Epoch 418/500\n",
      "506/506 [==============================] - 0s 103us/sample - loss: 3.2175 - mse: 3.2175\n",
      "Epoch 419/500\n",
      "506/506 [==============================] - 0s 129us/sample - loss: 3.4437 - mse: 3.4437\n",
      "Epoch 420/500\n",
      "506/506 [==============================] - 0s 106us/sample - loss: 3.3569 - mse: 3.3569\n",
      "Epoch 421/500\n",
      "506/506 [==============================] - 0s 123us/sample - loss: 3.1395 - mse: 3.1395\n",
      "Epoch 422/500\n",
      "506/506 [==============================] - 0s 134us/sample - loss: 3.1168 - mse: 3.1168\n",
      "Epoch 423/500\n",
      "506/506 [==============================] - 0s 118us/sample - loss: 3.1420 - mse: 3.1420\n",
      "Epoch 424/500\n",
      "506/506 [==============================] - 0s 128us/sample - loss: 3.0743 - mse: 3.0743\n",
      "Epoch 425/500\n",
      "506/506 [==============================] - 0s 140us/sample - loss: 3.1314 - mse: 3.1314\n",
      "Epoch 426/500\n",
      "506/506 [==============================] - 0s 111us/sample - loss: 3.2938 - mse: 3.2938\n",
      "Epoch 427/500\n",
      "506/506 [==============================] - 0s 112us/sample - loss: 3.1418 - mse: 3.1418\n",
      "Epoch 428/500\n",
      "506/506 [==============================] - 0s 112us/sample - loss: 3.1592 - mse: 3.1592\n",
      "Epoch 429/500\n",
      "506/506 [==============================] - 0s 124us/sample - loss: 3.1624 - mse: 3.1624\n",
      "Epoch 430/500\n",
      "506/506 [==============================] - 0s 126us/sample - loss: 3.2788 - mse: 3.2788\n",
      "Epoch 431/500\n",
      "506/506 [==============================] - 0s 101us/sample - loss: 3.0577 - mse: 3.0577\n",
      "Epoch 432/500\n",
      "506/506 [==============================] - 0s 123us/sample - loss: 3.1054 - mse: 3.1054\n",
      "Epoch 433/500\n",
      "506/506 [==============================] - 0s 117us/sample - loss: 3.1084 - mse: 3.1084\n",
      "Epoch 434/500\n",
      "506/506 [==============================] - 0s 110us/sample - loss: 3.0742 - mse: 3.0742\n",
      "Epoch 435/500\n",
      "506/506 [==============================] - 0s 111us/sample - loss: 3.1810 - mse: 3.1810\n",
      "Epoch 436/500\n",
      "506/506 [==============================] - 0s 93us/sample - loss: 3.1312 - mse: 3.1312\n",
      "Epoch 437/500\n",
      "506/506 [==============================] - 0s 104us/sample - loss: 3.0577 - mse: 3.0577\n",
      "Epoch 438/500\n",
      "506/506 [==============================] - 0s 87us/sample - loss: 3.0171 - mse: 3.0171\n",
      "Epoch 439/500\n",
      "506/506 [==============================] - 0s 86us/sample - loss: 3.0896 - mse: 3.0896\n",
      "Epoch 440/500\n",
      "506/506 [==============================] - 0s 109us/sample - loss: 3.0542 - mse: 3.0542\n",
      "Epoch 441/500\n",
      "506/506 [==============================] - 0s 89us/sample - loss: 3.0055 - mse: 3.0055\n",
      "Epoch 442/500\n",
      "506/506 [==============================] - 0s 110us/sample - loss: 3.0467 - mse: 3.0467\n",
      "Epoch 443/500\n",
      "506/506 [==============================] - 0s 90us/sample - loss: 3.0057 - mse: 3.0057\n",
      "Epoch 444/500\n",
      "506/506 [==============================] - 0s 96us/sample - loss: 2.9379 - mse: 2.9379\n",
      "Epoch 445/500\n",
      "506/506 [==============================] - 0s 93us/sample - loss: 3.1408 - mse: 3.1408\n",
      "Epoch 446/500\n",
      "506/506 [==============================] - 0s 100us/sample - loss: 3.0556 - mse: 3.0556\n",
      "Epoch 447/500\n",
      "506/506 [==============================] - 0s 123us/sample - loss: 3.1082 - mse: 3.1082\n",
      "Epoch 448/500\n",
      "506/506 [==============================] - 0s 121us/sample - loss: 3.0569 - mse: 3.0569\n",
      "Epoch 449/500\n",
      "506/506 [==============================] - 0s 99us/sample - loss: 3.0665 - mse: 3.0665\n",
      "Epoch 450/500\n",
      "506/506 [==============================] - 0s 100us/sample - loss: 2.9807 - mse: 2.9807\n",
      "Epoch 451/500\n",
      "506/506 [==============================] - 0s 102us/sample - loss: 2.9618 - mse: 2.9618\n",
      "Epoch 452/500\n",
      "506/506 [==============================] - 0s 99us/sample - loss: 2.9979 - mse: 2.9979\n",
      "Epoch 453/500\n",
      "506/506 [==============================] - 0s 95us/sample - loss: 2.8794 - mse: 2.8794\n",
      "Epoch 454/500\n",
      "506/506 [==============================] - 0s 87us/sample - loss: 3.0427 - mse: 3.0427\n",
      "Epoch 455/500\n",
      "506/506 [==============================] - 0s 92us/sample - loss: 2.9460 - mse: 2.9460\n",
      "Epoch 456/500\n",
      "506/506 [==============================] - 0s 93us/sample - loss: 2.9248 - mse: 2.9248\n",
      "Epoch 457/500\n",
      "506/506 [==============================] - 0s 101us/sample - loss: 3.0675 - mse: 3.0675\n",
      "Epoch 458/500\n",
      "506/506 [==============================] - 0s 73us/sample - loss: 2.9208 - mse: 2.9208\n",
      "Epoch 459/500\n",
      "506/506 [==============================] - 0s 89us/sample - loss: 2.9052 - mse: 2.9052\n",
      "Epoch 460/500\n",
      "506/506 [==============================] - 0s 69us/sample - loss: 2.9306 - mse: 2.9306\n",
      "Epoch 461/500\n",
      "506/506 [==============================] - 0s 83us/sample - loss: 2.8785 - mse: 2.8785\n",
      "Epoch 462/500\n",
      "506/506 [==============================] - 0s 74us/sample - loss: 2.9893 - mse: 2.9893\n",
      "Epoch 463/500\n",
      "506/506 [==============================] - 0s 93us/sample - loss: 3.0897 - mse: 3.0897\n",
      "Epoch 464/500\n",
      "506/506 [==============================] - 0s 122us/sample - loss: 2.9990 - mse: 2.9990\n",
      "Epoch 465/500\n",
      "506/506 [==============================] - 0s 129us/sample - loss: 2.9834 - mse: 2.9834\n",
      "Epoch 466/500\n",
      "506/506 [==============================] - 0s 117us/sample - loss: 2.8273 - mse: 2.8273\n",
      "Epoch 467/500\n",
      "506/506 [==============================] - 0s 109us/sample - loss: 2.9274 - mse: 2.9274\n",
      "Epoch 468/500\n",
      "506/506 [==============================] - 0s 117us/sample - loss: 2.9709 - mse: 2.9709\n",
      "Epoch 469/500\n",
      "506/506 [==============================] - 0s 115us/sample - loss: 2.8939 - mse: 2.8939\n",
      "Epoch 470/500\n",
      "506/506 [==============================] - 0s 108us/sample - loss: 2.8752 - mse: 2.8752\n",
      "Epoch 471/500\n",
      "506/506 [==============================] - 0s 105us/sample - loss: 2.8952 - mse: 2.8952\n",
      "Epoch 472/500\n",
      "506/506 [==============================] - 0s 105us/sample - loss: 3.0095 - mse: 3.0095\n",
      "Epoch 473/500\n",
      "506/506 [==============================] - 0s 110us/sample - loss: 3.0692 - mse: 3.0692\n",
      "Epoch 474/500\n",
      "506/506 [==============================] - 0s 137us/sample - loss: 2.9093 - mse: 2.9093\n",
      "Epoch 475/500\n",
      "506/506 [==============================] - 0s 101us/sample - loss: 2.8172 - mse: 2.8172\n",
      "Epoch 476/500\n",
      "506/506 [==============================] - 0s 95us/sample - loss: 2.9259 - mse: 2.9259\n",
      "Epoch 477/500\n",
      "506/506 [==============================] - 0s 106us/sample - loss: 2.9098 - mse: 2.9098\n",
      "Epoch 478/500\n",
      "506/506 [==============================] - 0s 92us/sample - loss: 2.9122 - mse: 2.9122\n",
      "Epoch 479/500\n",
      "506/506 [==============================] - 0s 104us/sample - loss: 2.9180 - mse: 2.9180\n",
      "Epoch 480/500\n",
      "506/506 [==============================] - 0s 94us/sample - loss: 2.8255 - mse: 2.8255\n",
      "Epoch 481/500\n",
      "506/506 [==============================] - 0s 133us/sample - loss: 2.8643 - mse: 2.8643\n",
      "Epoch 482/500\n",
      "506/506 [==============================] - 0s 122us/sample - loss: 2.8137 - mse: 2.8137\n",
      "Epoch 483/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 112us/sample - loss: 2.9569 - mse: 2.9569\n",
      "Epoch 484/500\n",
      "506/506 [==============================] - 0s 129us/sample - loss: 2.9096 - mse: 2.9096\n",
      "Epoch 485/500\n",
      "506/506 [==============================] - 0s 92us/sample - loss: 2.8752 - mse: 2.8752\n",
      "Epoch 486/500\n",
      "506/506 [==============================] - 0s 64us/sample - loss: 2.8252 - mse: 2.8252\n",
      "Epoch 487/500\n",
      "506/506 [==============================] - 0s 90us/sample - loss: 2.8929 - mse: 2.8929\n",
      "Epoch 488/500\n",
      "506/506 [==============================] - 0s 78us/sample - loss: 2.8240 - mse: 2.8240\n",
      "Epoch 489/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 2.9094 - mse: 2.9094\n",
      "Epoch 490/500\n",
      "506/506 [==============================] - 0s 70us/sample - loss: 2.8011 - mse: 2.8011\n",
      "Epoch 491/500\n",
      "506/506 [==============================] - 0s 61us/sample - loss: 2.9275 - mse: 2.9275\n",
      "Epoch 492/500\n",
      "506/506 [==============================] - 0s 73us/sample - loss: 2.7314 - mse: 2.7314\n",
      "Epoch 493/500\n",
      "506/506 [==============================] - 0s 69us/sample - loss: 2.7562 - mse: 2.7562\n",
      "Epoch 494/500\n",
      "506/506 [==============================] - 0s 70us/sample - loss: 2.9339 - mse: 2.9339\n",
      "Epoch 495/500\n",
      "506/506 [==============================] - 0s 79us/sample - loss: 2.8156 - mse: 2.8156\n",
      "Epoch 496/500\n",
      "506/506 [==============================] - 0s 55us/sample - loss: 2.7503 - mse: 2.7503\n",
      "Epoch 497/500\n",
      "506/506 [==============================] - 0s 72us/sample - loss: 2.9835 - mse: 2.9835\n",
      "Epoch 498/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 2.7156 - mse: 2.7156\n",
      "Epoch 499/500\n",
      "506/506 [==============================] - 0s 75us/sample - loss: 2.8857 - mse: 2.8857\n",
      "Epoch 500/500\n",
      "506/506 [==============================] - 0s 69us/sample - loss: 2.8133 - mse: 2.8133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f92b4143748>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_, y_, epochs=500, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答一下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What does a neuron compute?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经元使用非线性激活函数（Sigmoid、ReLu等）对线性函数（wx+b）进行计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2. Why we use non-linear activation funcitons in neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为如果使用线性激活函数的话，多层神经网络与单层的效果一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the 'Logistic Loss' ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Loss = -(y*\\log \\hat{y} + (1-y)\\log(1-\\hat{y}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Assume that you are building a binary classifier for detecting if an image containing cats, which activation functions would you recommen using for the output layer ?\n",
    "\n",
    "A. ReLU    \n",
    "B. Leaky ReLU    \n",
    "C. sigmoid    \n",
    "D. tanh  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C.Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Why we don't use zero initialization for all parameters ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果所有参数的初始值都是0，则后面的隐藏层和输出层的结果都为零，导致梯度消失，神经网络将学不到任何东西。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you implement the softmax function using python ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def softmax(a:list):\n",
    "    total = sum(math.exp(i) for i in a)\n",
    "    return [math.exp(i) / total for i in a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.实践题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this practical part, you will build a simple digits recognizer to check if the digit in the image is larger than 5. This assignmnet will guide you step by step to finish your first small project in this course ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Packages  \n",
    "sklearn is a famous package for machine learning.   \n",
    "matplotlib is a common package for vasualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Overvie of the dataset  \n",
    "    - a training set has m_train images labeled as 0 if the digit < 5 or 1 if the digit >= 5\n",
    "    - a test set contains m_test images labels as if the digit < 5 or 1 if the digit >= 5\n",
    "    - eah image if of shape (num_px, num_px ). Thus, each image is square(height=num_px and  width = num_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data \n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vilizating the data\n",
    "for i in range(1,11):\n",
    "    plt.subplot(2,5,i)\n",
    "    plt.imshow(digits.data[i-1].reshape([8,8]),cmap=plt.cm.gray_r)\n",
    "    plt.text(3,10,str(digits.target[i-1]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training set and test set \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformulate the label. \n",
    "# If the digit is smaller than 5, the label is 0.\n",
    "# If the digit is larger than 5, the label is 1.\n",
    "\n",
    "y_train[y_train < 5 ] = 0\n",
    "y_train[y_train >= 5] = 1\n",
    "y_test[y_test < 5] = 0\n",
    "y_test[y_test >= 5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1347, 64)\n",
      "(450, 64)\n",
      "(1347, 1)\n",
      "(450, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3- Architecture of the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mathematical expression of the algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one example $x^{(i)}$:   \n",
    " $$ z^{(i)} = w^T * x^{(i)} +b $$   \n",
    " $$ y^{(i)} = a^{(i)} = sigmoid(z^{(i)})$$   \n",
    " $$L(a^{(i)},y^{(i)}) = -y^{(i)} log(a^{(i)})-(1-y^{(i)})log(1-a^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total cost over all training examples:\n",
    "$$ J = \\frac{1}{m}\\sum_{i=1}^{m}L(a^{(i)},y^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - Building the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1- Activation function    \n",
    "###### Exercise:\n",
    "Finish the sigmoid funciton "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(z):\n",
    "    '''\n",
    "    Compute the sigmoid of z\n",
    "    Arguments: z -- a scalar or numpy array of any size.\n",
    "    \n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    '''\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([-10,0,100]) = [4.53978687e-05 5.00000000e-01 1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# Test your code \n",
    "# The result should be [0.5 0.88079708]\n",
    "print(\"sigmoid([-10,0,100]) = \" + str(sigmoid(np.array([-10,0,100]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1-Initializaing parameters\n",
    "###### Exercise:\n",
    "Finishe the initialize_parameters function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random innitialize the parameters\n",
    "\n",
    "def initialize_parameters(dim):\n",
    "    '''\n",
    "    Argument: dim -- size of the w vector\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim,1)\n",
    "    b -- initializaed scalar\n",
    "    '''\n",
    "    \n",
    "    w = np.random.random((dim,1))\n",
    "    b = np.random.random()\n",
    "    \n",
    "    assert(w.shape == (dim,1))\n",
    "    assert(isinstance(b,float) or isinstance(b,int))\n",
    "    \n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.70510704],\n",
       "        [0.10440858],\n",
       "        [0.51509255],\n",
       "        [0.64678648],\n",
       "        [0.34501852],\n",
       "        [0.14580832],\n",
       "        [0.10768875],\n",
       "        [0.07441426],\n",
       "        [0.20032451],\n",
       "        [0.97285245]]), 0.2604728989795829)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize_parameters(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3-Forward and backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Some mathematical expressions\n",
    "Forward Propagation:   \n",
    ". X    \n",
    ". A = $\\sigma(w^T*X+b) = (a^{(1)},a^{(2)},...,a^{(m)}$   \n",
    ". J = $-\\frac{1}{m} \\sum_{i=1}^{m}y^{(i)}log(a^{(i)}+(1-y^{(i)})log(1-a^{(i)})$       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some derivative: \n",
    "$$\\frac{\\partial{J}}{\\partial{w}} = \\frac{1}{m}X*(A-Y)^T$$   \n",
    "$$\\frac{\\partial{J}}{\\partial{b}} = \\frac{1}{m}\\sum_{i=1}^m(a^{(i)}-y^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise:\n",
    "Finish the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w,b,X,Y):\n",
    "    '''\n",
    "    Implement the cost function and its gradient for the propagation\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    A = sigmoid(X.dot(w) + b)\n",
    "    cost = sum(-y*np.log(a)-(1-y)*np.log(1-a) for y,a in zip(Y,A)) / m\n",
    "    \n",
    "    dw = np.dot(X.T,(A-Y)) / m\n",
    "    db = np.sum(A-Y) / m    \n",
    "    \n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {'dw':dw,\n",
    "             'db':db}\n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = initialize_parameters(10)\n",
    "X = np.random.random((100,10))\n",
    "Y = np.array([np.random.choice([0,1]) for _ in range(100)]).reshape(100,1)\n",
    "grads, cost = propagate(w, b, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1.71432883)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4 -Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise:\n",
    "Minimizing the cost function using gradient descent.   \n",
    "$$\\theta = \\theta - \\alpha*d\\theta$$ where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, lr, print_cost=False):\n",
    "    '''\n",
    "    This function optimize w and b by running a gradient descen algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params - dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        \n",
    "        dw = grads['dw']\n",
    "        db = grads['db']\n",
    "        \n",
    "        w = w - lr * dw\n",
    "        b = b - lr * db\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\":w,\n",
    "              \"b\":b}\n",
    "    \n",
    "    grads = {\"dw\":dw,\n",
    "             \"db\":db}\n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise\n",
    "The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the predict() function.    \n",
    "Two steps to finish this task:   \n",
    "1. Calculate $\\hat{Y} = A = \\sigma(w^T*X+b)$   \n",
    "2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector Y_prediction. If you wish, you can use an if/else statement in a for loop (though there is also a way to vectorize this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments: \n",
    "    w -- weights\n",
    "    b -- bias \n",
    "    X -- data \n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    m = X.shape[0]\n",
    "    Y_prediction = np.zeros((m,1))\n",
    "    A = sigmoid(X.dot(w) + b)\n",
    "    for i in range(A.shape[0]):\n",
    "        if A[i,0] < 0.5:\n",
    "            Y_prediction[i,0] = 0\n",
    "        else:\n",
    "            Y_prediction[i,0] = 1\n",
    "    \n",
    "    assert(Y_prediction.shape == (m,1))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5- Merge all functions into a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations !! You have finished all the necessary components for constructing a model. Now, Let's take the challenge to merge all the implemented function into one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, y_train, X_test, y_test, num_iterations, learning_rate,print_cost):\n",
    "    \"\"\"\n",
    "    Build the logistic regression model by calling all the functions you have implemented.\n",
    "    Arguments:\n",
    "    X_train - training set\n",
    "    Y_train - training label\n",
    "    X_test - test set\n",
    "    Y_test - test label\n",
    "    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n",
    "    eg: d = {\"w\":w,\n",
    "             \"b\":b,\n",
    "             \"training_accuracy\": traing_accuracy,\n",
    "             \"test_accuracy\":test_accuracy,\n",
    "             \"cost\":cost}\n",
    "    \"\"\"\n",
    "    w, b = initialize_parameters(X_train.shape[1])\n",
    "    params, grads, costs = optimize(w, b, X_train, y_train, num_iterations, learning_rate, print_cost)\n",
    "    w = params['w']\n",
    "    b = params['b']\n",
    "    \n",
    "    y_train_pred = predict(w, b, X_train)\n",
    "    training_accuracy = sum(y_train_pred == y_train) / y_train.shape[0]\n",
    "    \n",
    "    y_test_pred = predict(w, b, X_test)\n",
    "    test_accuracy = sum(y_test_pred == y_test) / y_test.shape[0]\n",
    "    \n",
    "    d = {\"w\":w,\n",
    "         \"b\":b,\n",
    "         \"training_accuracy\": training_accuracy,\n",
    "         \"test_accuracy\":test_accuracy,\n",
    "         \"costs\":costs}\n",
    "    return d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  \n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in multiply\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "d = model(X_train, y_train, X_test, y_test, num_iterations=1000, learning_rate=1e-2,print_cost=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.88938382]),\n",
       " array([0.87555556]),\n",
       " [array(nan),\n",
       "  array(nan),\n",
       "  array(nan),\n",
       "  array(0.43079217),\n",
       "  array(0.36875416),\n",
       "  array(0.33923772),\n",
       "  array(0.32250647),\n",
       "  array(0.31171687),\n",
       "  array(0.3041157),\n",
       "  array(0.29842249)])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['training_accuracy'],d['test_accuracy'],d['costs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.选做题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on building your first logistic regression model. It is your time to analyze it further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 Observe the effect of learning rate on the leraning process.   \n",
    "Hits: plot the learning curve with different learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  \n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in multiply\n",
      "  \n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: overflow encountered in exp\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "lrs = np.logspace(-9, 0, 10)\n",
    "test_acc, train_acc = [], []\n",
    "for lr in lrs:\n",
    "    d = model(X_train, y_train, X_test, y_test, num_iterations=1000, learning_rate=lr,print_cost=False)\n",
    "    train_acc.append(d['training_accuracy'])\n",
    "    test_acc.append(d['test_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f926f571cf8>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xscale('log')\n",
    "ax.plot(lrs, train_acc, label='train_accuracy')\n",
    "ax.plot(lrs, test_acc, label='test_accuracy')\n",
    "plt.xlabel('learning rate')\n",
    "plt.ylabel('accuracy')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，当learning rate在0.01左右时，train accuracy和test accuray最高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 Observe the effect of iteration_num on the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  \n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in multiply\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "iters = [100,500,1000,5000,10000]\n",
    "test_acc, train_acc = [], []\n",
    "for num in iters:\n",
    "    d = model(X_train, y_train, X_test, y_test, num_iterations=int(num), learning_rate=1e-2,print_cost=False)\n",
    "    train_acc.append(d['training_accuracy'])\n",
    "    test_acc.append(d['test_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f926f7c9a20>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(iters, train_acc, label='train_accuracy')\n",
    "ax.plot(iters, test_acc, label='test_accuracy')\n",
    "plt.xlabel('iteration numbers')\n",
    "plt.ylabel('accuracy')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，当iteration numbers在1000以上时，training accuracy和test accuracy都会维持在较高的数值（85%以上）。继续增加iteration number，对accuracy的提高程度有限。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge ! ! !\n",
    "\n",
    "The original data have images labeled 0,1,2,3,4,5,6,7,8,9. In our logistic model, we only detect if the digit in the image is larger or smaller than 5. Now, Let's go for a more challenging problem. Try to use softmax function to build a model to recognize which digit (0,1,2,3,4,5,6,7,8,9) is in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25)\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "#w，b参数初始化，w(64,10) b(10,)\n",
    "def initialize_parameters_10(feature_dim, target_dim=10):\n",
    "    w = np.random.random((feature_dim,target_dim))\n",
    "    b = np.random.random((target_dim, ))\n",
    "    return w,b\n",
    "\n",
    "#softmax函数的输入为矩阵\n",
    "def softmax(x):\n",
    "    x = x - x.max(axis=1, keepdims=True)\n",
    "    y = np.exp(x)\n",
    "    return y / y.sum(axis=1, keepdims=True)\n",
    "\n",
    "#计算cross entropy，A为softmax函数的预测值，A(num,target_class), y(num,1)而非onehot\n",
    "def cross_entropy(A, y):\n",
    "    m = A.shape[0]\n",
    "    loss = 0\n",
    "    for i, y_i in enumerate(y):\n",
    "        loss += -np.log(A[i,y_i])\n",
    "    return loss / m\n",
    "\n",
    "def onehot(y):\n",
    "    m = y.shape[0]\n",
    "    y_onehot = np.zeros((m,10))\n",
    "    for i in range(m):\n",
    "        y_onehot[i,y[i,0]] = 1\n",
    "    return y_onehot\n",
    "\n",
    "def propagate_10(w,b,X,Y):\n",
    "    m = X.shape[0]\n",
    "    A = softmax(X.dot(w) + b) #A[m,10]\n",
    "    cost = cross_entropy(A, Y) \n",
    "    \n",
    "    Y_onehot = onehot(Y)\n",
    "    assert Y_onehot.shape == A.shape\n",
    "    #dw, db的公式与二分类的logistic loss一样，Y取onehot后的值，与经softmax计算的A的维度保持一样\n",
    "    dw = np.dot(X.T,(A-Y_onehot)) / m\n",
    "    db = np.sum(A-Y_onehot, axis=0) / m  \n",
    "    \n",
    "    assert dw.shape == w.shape\n",
    "    assert db.shape == b.shape\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    grads = {'dw':dw,\n",
    "             'db':db}\n",
    "    return grads, cost\n",
    "\n",
    "#与二分类的optimize计算完全一样\n",
    "def optimize_10(w, b, X, Y, num_iterations, lr, print_cost):\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        grads, cost = propagate_10(w, b, X, Y)\n",
    "        \n",
    "        dw = grads['dw']\n",
    "        db = grads['db']\n",
    "        \n",
    "        w = w - lr * dw\n",
    "        b = b - lr * db\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\":w,\n",
    "              \"b\":b}\n",
    "    \n",
    "    grads = {\"dw\":dw,\n",
    "             \"db\":db}\n",
    "    return params, grads, costs\n",
    "\n",
    "#这里根据w，b计算后，经softmax函数，取argmax为预测的Y值。\n",
    "def predict_10(w, b, X):\n",
    "    A = softmax(X.dot(w) + b)\n",
    "    Y_prediction = A.argmax(axis=1)    \n",
    "    return Y_prediction.reshape(-1,1)\n",
    "\n",
    "def model_10(X_train, y_train, X_test, y_test, num_iterations, learning_rate, print_cost, n_class=10):\n",
    " \n",
    "    w, b = initialize_parameters_10(X_train.shape[1], n_class)\n",
    "    params, grads, costs = optimize_10(w, b, X_train, y_train, num_iterations, learning_rate, print_cost)\n",
    "    w = params['w']\n",
    "    b = params['b']\n",
    "    \n",
    "    y_train_pred = predict_10(w, b, X_train)\n",
    "    training_accuracy = sum(y_train_pred == y_train) / y_train.shape[0]\n",
    "    \n",
    "    y_test_pred = predict_10(w, b, X_test)\n",
    "    test_accuracy = sum(y_test_pred == y_test) / y_test.shape[0]\n",
    "    \n",
    "    d = {\"w\":w,\n",
    "         \"b\":b,\n",
    "         \"training_accuracy\": training_accuracy,\n",
    "         \"test_accuracy\":test_accuracy,\n",
    "         \"costs\":costs}\n",
    "    return d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 30.381748\n",
      "Cost after iteration 100: 10.326082\n",
      "Cost after iteration 200: 6.544870\n",
      "Cost after iteration 300: 4.435820\n",
      "Cost after iteration 400: 3.203702\n",
      "Cost after iteration 500: 2.486431\n",
      "Cost after iteration 600: 2.007459\n",
      "Cost after iteration 700: 1.658831\n",
      "Cost after iteration 800: 1.394266\n",
      "Cost after iteration 900: 1.192119\n"
     ]
    }
   ],
   "source": [
    "d = model_10(X_train, y_train, X_test, y_test, num_iterations=1000, learning_rate=1e-3,print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.79213066]), array([0.77111111]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['training_accuracy'], d['test_accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过1000次的迭代训练后，training accuracy为79.2%， test accuracy为77.2%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations ! You have completed assigment 4. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
